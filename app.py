import streamlit as st

# ========================================================================
#                           ××¢×¨×›×ª ××™××•×ª ××•×¤×¦×™×•× ×œ×™×ª - ×¨×™×©×•× ×•×”×ª×—×‘×¨×•×ª
# ========================================================================
# ××¢×¨×›×ª ×¨×™×©×•× ×™×¤×” ×•××•×¤×¦×™×•× ×œ×™×ª ×¢× email ×•×¡×™×¡××”
# ×œ×‘×™×˜×•×œ ×”××¢×¨×›×ª: ×©× ×” ××ª ENABLE_AUTH ×œ-False
ENABLE_AUTH = True

try:
    import auth  # ×™×™×‘×•× ××•×“×•×œ ×”××™××•×ª
    AUTH_AVAILABLE = True
except ImportError:
    AUTH_AVAILABLE = False
    ENABLE_AUTH = False

# ========================================================================
#                           ×‘×œ×•×§ ×™×™×‘×•× ×”×¡×¤×¨×™×•×ª ×”×¨××©×™×•×ª
# ========================================================================
# ×¤×•× ×§×¦×™×” ×‘×˜×•×—×” ×œ×–×™×”×•×™ ××›×©×™×¨×™× × ×™×™×“×™×
def is_mobile_browser():
    """
    ×–×™×”×•×™ ××›×©×™×¨ × ×™×™×“ ×‘×¦×•×¨×” ×‘×˜×•×—×” ×œ×œ× ×‘×¢×™×•×ª ×ª×¦×•×’×”
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - ×œ×–×”×•×ª ×× ×”××©×ª××© ××©×ª××© ×‘××›×©×™×¨ × ×™×™×“
    - ×œ×©××•×¨ ××ª ×”×ª×•×¦××” ×‘×¡×©×Ÿ ×›×“×™ ×œ×× ×•×¢ ×‘×“×™×§×•×ª ×—×•×–×¨×•×ª
    - ×œ×”×—×–×™×¨ False ×›×‘×¨×™×¨×ª ××—×“×œ ×œ×× ×™×¢×ª ×©×’×™××•×ª
    
    ×”×—×–×¨×”: bool - True ×× ××›×©×™×¨ × ×™×™×“, False ××—×¨×ª
    """
    try:
        # ×©×™××•×© ×‘×¡×˜×˜×•×¡ ×”×¡×©×Ÿ ×›×“×™ ×œ×©××•×¨ ××ª ×ª×•×¦××ª ×”×–×™×”×•×™
        if 'is_mobile_cache' not in st.session_state:
            # ×‘×“×™×§×ª ×¨×•×—×‘ ×”××¡×š ××• ×”×’×“×¨×” ×™×“× ×™×ª ×¢×œ ×™×“×™ ×”××©×ª××©
            # ×›×‘×¨×™×¨×ª ××—×“×œ × × ×™×— ×©×–×” ×œ× ××›×©×™×¨ × ×™×™×“
            mobile_detected = False
            
            # ×× ×”××©×ª××© ×”×’×“×™×¨ ×™×“× ×™×ª ×‘××§×•× ××—×¨
            if 'mobile_device_manual' in st.session_state:
                mobile_detected = st.session_state.mobile_device_manual
                
            st.session_state.is_mobile_cache = mobile_detected
        return st.session_state.is_mobile_cache
    except (AttributeError, KeyError, Exception) as e:
        # ×¨×™×©×•× ×©×’×™××” ×œ×“×™×‘×•×’, ××š ×”×—×–×¨×ª ×¢×¨×š ×‘×˜×•×—
        return False

# ========================================================================
#                           ×™×™×‘×•× ×¡×¤×¨×™×•×ª × ×™×ª×•×— × ×ª×•× ×™×
# ========================================================================
import pandas as pd                 # ×¡×¤×¨×™×™×” ×œ×¢×‘×•×“×” ×¢× × ×ª×•× ×™× ×˜×‘×œ××™×™×
import plotly.express as px         # ×¡×¤×¨×™×™×” ×œ×™×¦×™×¨×ª ×ª×¨×©×™××™× ××™× ×˜×¨××§×˜×™×‘×™×™×
import plotly.graph_objects as go   # ××•×‘×™×™×§×˜×™× ××ª×§×“××™× ×œ×ª×¨×©×™××™×
from plotly.subplots import make_subplots  # ×™×¦×™×¨×ª ×ª×¨×©×™××™× ××•×¨×›×‘×™× ×¢× ×ª×ª×™-×’×¨×¤×™×
import numpy as np                  # ×¡×¤×¨×™×™×” ×œ×—×™×©×•×‘×™× × ×•××¨×™×™×
from datetime import datetime, timedelta  # ×¢×‘×•×“×” ×¢× ×ª××¨×™×›×™× ×•×–×× ×™×
import pytz                         # ×˜×™×¤×•×œ ×‘××–×•×¨×™ ×–××Ÿ
import seaborn as sns              # ×¡×¤×¨×™×™×” × ×•×¡×¤×ª ×œ×•×™×–×•××œ×™×–×¦×™×”

# ========================================================================
#                           ×”×’×“×¨×•×ª ×ª×¦×•×’×” ×•×¨×™× ×“×•×¨
# ========================================================================
import time                        # ×¤×•× ×§×¦×™×•×ª ×–××Ÿ ×•×©×”×™×”
import matplotlib                  # ×¡×¤×¨×™×™×” ×‘×¡×™×¡×™×ª ×œ×ª×¨×©×™××™×
matplotlib.use('Agg')             # ×”×’×“×¨×ª ×¨×™× ×“×•×¨ ×œ×œ× ×××©×§ ×’×¨×¤×™ (backend)
import matplotlib.pyplot as plt    # ×¤×•× ×§×¦×™×•×ª ×™×¦×™×¨×ª ×ª×¨×©×™××™×

# ========================================================================
#                           ×¡×¤×¨×™×•×ª ×œ××™×“×ª ××›×•× ×”
# ========================================================================
from sklearn.cluster import KMeans              # ××œ×’×•×¨×™×ª× K-Means ×œ×§×œ××¡×˜×¨×™× ×’
from sklearn.decomposition import PCA          # × ×™×ª×•×— ×¨×›×™×‘×™× ×¨××©×™×™×
from sklearn.preprocessing import StandardScaler  # × ×¨××•×œ × ×ª×•× ×™×
from scipy import stats                        # ×—×™×©×•×‘×™× ×¡×˜×˜×™×¡×˜×™×™× ××ª×§×“××™×

# ========================================================================
#                           ×¡×¤×¨×™×•×ª × ×™×”×•×œ ×§×‘×¦×™× ×•×‘×¡×™×¡×™ × ×ª×•× ×™×
# ========================================================================
import sqlite3                     # ×¢×‘×•×“×” ×¢× ×‘×¡×™×¡ × ×ª×•× ×™× SQLite
import os                          # ×¤×•× ×§×¦×™×•×ª ××¢×¨×›×ª ×”×¤×¢×œ×”  
import io                          # ×¤×•× ×§×¦×™×•×ª ×§×œ×˜/×¤×œ×˜





# ========================================================================
#                         ×˜×¢×™× ×ª ×× ×”×œ PostgreSQL ××•×¤×¦×™×•× ×œ×™  
# ========================================================================
# ×‘×œ×•×§ ×–×” ×× ×¡×” ×œ×˜×¢×•×Ÿ ××ª ×× ×”×œ PostgreSQL ×‘×¦×•×¨×” ×‘×˜×•×—×”
try:
    import psycopg2  # noqa: F401    # ×× ×”×œ PostgreSQL ×œ×‘×¡×™×¡ × ×ª×•× ×™× ××ª×§×“×
    _HAS_PG = True                   # ×“×’×œ ×”××¦×™×™×Ÿ ×©×”×× ×”×œ ×–××™×Ÿ
except Exception:
    _HAS_PG = False                  # ×‘××§×¨×” ×©×œ ×›×™×©×œ×•×Ÿ, ×××©×™×›×™× ×œ×œ× PostgreSQL





# ========================================================================
#                           ×¡×¤×¨×™×•×ª ×“×•×—×•×ª ×•×¨×©×ª
# ========================================================================
from reportlab.pdfgen import canvas         # ×™×¦×™×¨×ª ×§×‘×¦×™ PDF
from reportlab.lib.pagesizes import letter  # ×’×“×œ×™ ×¢××•×“×™× ×œPDF
import requests                             # ×‘×§×©×•×ª HTTP
import warnings                             # × ×™×”×•×œ ××–×”×¨×•×ª
warnings.filterwarnings('ignore')          # ×”×©×ª×§×ª ××–×”×¨×•×ª ××™×•×ª×¨×•×ª

# ========================================================================
#                           ×”×’×“×¨×•×ª ×¢××•×“ Streamlit
# ========================================================================
# ×”×’×“×¨×ª ×ª×¦×•×¨×ª ×”×¢××•×“ ×¢× ××•×¤×˜×™××™×–×¦×™×•×ª ×œ××›×©×™×¨×™× × ×™×™×“×™×
st.set_page_config(
    page_title="DataBot Analytics",     # ×›×•×ª×¨×ª ×”×“×£
    page_icon="ğŸš€",                        # ××™×™×§×•×Ÿ ×”×“×£
    layout="wide",                         # ×¤×¨×™×¡×” ×¨×—×‘×”
    initial_sidebar_state="expanded"       # ×¡×¨×’×œ ×¦×“ ××•×¨×—×‘ ×›×‘×¨×™×¨×ª ××—×“×œ
)

# ========================================================================
#                           ×”×’×“×¨×•×ª ××™×•×—×“×•×ª ×œ××›×©×™×¨×™× × ×™×™×“×™×
# ========================================================================
# ×ª×¦×•×¨×” ××™×•×—×“×ª ×œ××›×©×™×¨×™× × ×™×™×“×™× (×¤×©×•×˜×”)
if 'mobile_config_set' not in st.session_state:
    # ×”×’×“×¨×ª ×“×’×œ ×œ××•×¤×˜×™××™×–×¦×™×•×ª × ×™×™×“
    # × ×˜×¤×œ ×‘××’×‘×œ×•×ª ×’×•×“×œ ×§×‘×¦×™× ×‘×¤×•× ×§×¦×™×™×ª ×”×”×¢×œ××”
    st.session_state.mobile_config_set = True

# ========================================================================
#                           ×¢×™×¦×•×‘ CSS ×¢× ×©×™×¤×•×¨×™× ×œ××›×©×™×¨×™× × ×™×™×“×™×
# ========================================================================
# ×‘×œ×•×§ ×–×” ××›×™×œ ××ª ×›×œ ×”×’×“×¨×•×ª ×”×¢×™×¦×•×‘ ×œ××¤×œ×™×§×¦×™×”
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
        background: linear-gradient(90deg, #1f77b4, #ff7f0e);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        font-weight: bold;
    }
    .stButton > button {
        background: linear-gradient(45deg, #667eea, #764ba2);
        color: white;
        border: none;
        border-radius: 20px;
        padding: 0.5rem 1rem;
        font-weight: bold;
    }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin: 0.5rem 0;
    }
    .insight-box {
        background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
        margin: 1rem 0;
    }
    .advice-box {
        background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
        margin: 1rem 0;
    }
    .desktop-float {
        position: fixed;
        bottom: 20px;
        right: 20px;
        z-index: 999;
        background: linear-gradient(45deg, #ff6b6b, #feca57);
        padding: 15px;
        border-radius: 50px;
        color: white;
        text-align: center;
        box-shadow: 0 4px 20px rgba(0,0,0,0.3);
        animation: pulse 2s infinite;
    }
    @keyframes pulse {
        0% { transform: scale(1); }
        50% { transform: scale(1.05); }
        100% { transform: scale(1); }
    }
    .mobile-banner {
        background: linear-gradient(45deg, #ff6b6b, #feca57);
        padding: 20px;
        border-radius: 15px;
        color: white;
        text-align: center;
        margin: 20px 0;
        box-shadow: 0 4px 15px rgba(0,0,0,0.2);
        animation: slideIn 1s ease-out;
    }
    @keyframes slideIn {
        from { opacity: 0; transform: translateY(-30px); }
        to { opacity: 1; transform: translateY(0); }
    }
</style>
""", unsafe_allow_html=True)

# ========================================================================
#                           ×¤×•× ×§×¦×™×” ×¨××©×™×ª ×©×œ ×”××¤×œ×™×§×¦×™×”
# ========================================================================
def main():
    """
    ×¤×•× ×§×¦×™×™×ª ×”×¨××©×™×ª ×©×œ ×™×™×©×•× DataBot Analytics Pro
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - ×”×’×“×¨×ª ×××©×§ ×”××©×ª××© ×”×¨××©×™
    - × ×™×”×•×œ ×”×’×“×¨×•×ª ××›×©×™×¨×™× × ×™×™×“×™×/×“×¡×§×˜×•×¤  
    - ×”×¦×’×ª ×ª×¤×¨×™×˜ × ×™×•×•×˜ ×•×—×œ×§×™ ×”××¤×œ×™×§×¦×™×” ×”×©×•× ×™×
    - ×˜×™×¤×•×œ ×‘××•×¤×˜×™××™×–×¦×™×•×ª ××™×•×—×“×•×ª ×œ××›×©×™×¨×™× × ×™×™×“×™×
    """
    
    # ========================================================================
    #                           ××¢×¨×›×ª ×¨×™×©×•× ×•×”×ª×—×‘×¨×•×ª ××•×¤×¦×™×•× ×œ×™×ª
    # ========================================================================
    # ×‘×“×™×§×ª ××¢×¨×›×ª ×”××™××•×ª ×‘×¦×•×¨×” ×‘×˜×•×—×” ×¢× × ×¤×™×œ×” ×—×–×¨×”
    if ENABLE_AUTH and AUTH_AVAILABLE:
        # ×‘×“×™×§×” ×× ×œ×”×¦×™×’ ×××©×§ ×”××™××•×ª
        if st.session_state.get("show_auth_ui", False):
            auth.show_auth_ui()  # ×”×¦×’×ª ×—×œ×•×Ÿ ×¨×™×©×•×/×”×ª×—×‘×¨×•×ª
            return  # ×¢×¦×™×¨×” ×›××Ÿ ×œ×”×¦×’×ª ×××©×§ ×”××™××•×ª ×‘×œ×‘×“
            
        # ×”×•×¡×¤×ª ××¤×©×¨×•×ª ×”×ª×—×‘×¨×•×ª ×‘×¡×¨×’×œ ×”×¦×“
        with st.sidebar:
            st.markdown("---")
            if not auth.check_authentication():
                # ×›×¤×ª×•×¨ ×”×ª×—×‘×¨×•×ª/×¨×™×©×•×
                if st.button("ğŸ” Login/Register", type="primary", use_container_width=True):
                    st.session_state.show_auth_ui = True
                    st.rerun()
                st.info("ğŸ“ Login for personalized experience")
            else:
                # ×”×¦×’×ª ×¤×¨×˜×™ ×”××©×ª××© ×”××—×•×‘×¨
                auth.show_user_info()
                st.success("âœ… ×”×ª×—×‘×¨×ª ×‘×”×¦×œ×—×”!")
    
    # ×”×¦×’×ª ×›×•×ª×¨×ª ×¨××©×™×ª ××¢×•×¦×‘×ª
    st.markdown('<h1 class="main-header">ğŸš€ DataBot Analytics Pro</h1>', unsafe_allow_html=True)
    
    # ========================================================================
    #                           ×”×’×“×¨×•×ª ××›×©×™×¨ - ××•×˜×•××˜×™ ×œ×¤×™ ×’×•×“×œ ××¡×š
    # ========================================================================
    # ×–×™×”×•×™ ××•×˜×•××˜×™ ×©×œ ××›×©×™×¨ × ×™×™×“ (×œ×œ× ×¦×•×¨×š ×‘×‘×—×™×¨×” ×™×“× ×™×ª)
    is_mobile_device = False  # Default to desktop for clean UI
    
    # ========================================================================
    #                           ×”×•×“×¢×ª ×‘×¨×•×›×™× ×”×‘××™× ×¢×‘×•×¨ ×“×¡×§×˜×•×¤
    # ========================================================================
    # ×”×•×“×¢×” × ×¢×™××” ×•×¤×©×•×˜×” ×œ××©×ª××©×™ ×“×¡×§×˜×•×¤
    st.success("ğŸ–¥ï¸ **Desktop Version Active** - Full functionality available!")
    
    # ×”×•×“×¢×” ×ª××™×“ ××•×¦×’×ª - ×”×‘×”×¨×” ×©×–×” ×¤×¨×•×™×§×˜ ×œ×“×•×’××”
    st.warning("ğŸ™Œ This application is presented as a pet project, so it shouldn't be taken too seriously. Thanks for giving it a try!")
    
    # ========================================================================
    #                           ×ª×¤×¨×™×˜ × ×™×•×•×˜ ×‘×¡×¨×’×œ ×”×¦×“
    # ========================================================================
    with st.sidebar:
        st.markdown("### ğŸ¯ Navigation")  # ×›×•×ª×¨×ª × ×™×•×•×˜
        st.markdown("---")              # ×§×• ×”×¤×¨×“×”
        
        # ×ª×¤×¨×™×˜ × ×™×•×•×˜ ×¢× ×ª×™××•×¨×™×
        page = st.selectbox(
            "Select section",           # ×ª×•×•×™×ª ×”×‘×—×™×¨×”
            ["ğŸ  Dashboard", "ğŸ“ Data Upload", "ğŸ“ˆ Charts", "ğŸ“Š Statistics", 
             "ğŸ¤– Machine Learning", "ğŸ§ª A/B Testing", "ğŸ’¾ Database", "ğŸ“„ Reports"]
        )
        
        # ========================================================================
        #                           ×”×¦×’×ª ××™×“×¢ ×¢×œ ×”×¡×§×¦×™×” ×”× ×•×›×—×™×ª
        # ========================================================================
        # ××™×œ×•×Ÿ ×”××›×™×œ ×ª×™××•×¨ ×œ×›×œ ×¡×§×¦×™×” ×‘××¤×œ×™×§×¦×™×”
        section_info = {
            "ğŸ  Dashboard": "Main overview with key metrics and insights",      # ×œ×•×— ×‘×§×¨×” ×¨××©×™
            "ğŸ“ Data Upload": "Upload and clean CSV, Excel, JSON files",       # ×”×¢×œ××ª ×•× ×™×§×•×™ ×§×‘×¦×™×
            "ğŸ“ˆ Charts": "Interactive visualizations including 3D plots",       # ×ª×¨×©×™××™× ××™× ×˜×¨××§×˜×™×‘×™×™×
            "ğŸ“Š Statistics": "Descriptive stats and statistical tests",        # ×¡×˜×˜×™×¡×˜×™×§×” ×•×‘×“×™×§×•×ª
            "ğŸ¤– Machine Learning": "Clustering, PCA, anomaly detection",       # ×œ××™×“×ª ××›×•× ×”
            "ğŸ§ª A/B Testing": "Statistical significance testing",             # ×‘×“×™×§×•×ª A/B
            "ğŸ’¾ Database": "SQL operations and database management",          # × ×™×”×•×œ ×‘×¡×™×¡×™ × ×ª×•× ×™×
            "ğŸ“„ Reports": "Generate comprehensive analysis reports"            # ×™×¦×™×¨×ª ×“×•×—×•×ª
        }
        
        # ×”×¦×’×ª ××™×“×¢ ×¢×œ ×”×¡×§×¦×™×” ×”× ×‘×—×¨×ª
        st.info(section_info[page])
        st.markdown("---")  # ×§×• ×”×¤×¨×“×”
        
        # ========================================================================
        #                           ×¤×¢×•×œ×•×ª ××”×™×¨×•×ª ×‘×¡×¨×’×œ ×”×¦×“
        # ========================================================================
        # ×”×¦×’×ª ×¤×¢×•×œ×•×ª ××”×™×¨×•×ª ×¨×§ ×× ×™×© × ×ª×•× ×™× ×˜×¢×•× ×™×
        if 'data' in st.session_state:
            st.markdown("### âš¡ Quick Actions")  # ×›×•×ª×¨×ª ×¤×¢×•×œ×•×ª ××”×™×¨×•×ª
            
            # ×§×‘×œ×ª ×”× ×ª×•× ×™× ×•×”×¢××•×“×•×ª ×”× ×•××¨×™×•×ª
            df = st.session_state.data
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            
            # ×›×¤×ª×•×¨ ×¡×™×›×•× × ×ª×•× ×™× ××”×™×¨
            if st.button("ğŸ¯ Data Summary", use_container_width=True):
                st.session_state.show_summary = True
            
            # ×›×¤×ª×•×¨ ××ª×× ××”×™×¨ (×–××™×Ÿ ×¨×§ ×¢× 2+ ×¢××•×“×•×ª × ×•××¨×™×•×ª)
            if len(numeric_cols) >= 2 and st.button("ğŸ”— Quick Correlation", use_container_width=True):
                st.session_state.show_correlation = True
            
            # ×›×¤×ª×•×¨ ×’×¨×£ ×ª×œ×ª ××™××“×™ (×–××™×Ÿ ×¨×§ ×¢× 3+ ×¢××•×“×•×ª × ×•××¨×™×•×ª)
            if len(numeric_cols) >= 3 and st.button("ğŸŒ 3D Quick Plot", use_container_width=True):
                st.session_state.show_3d = True
            
            st.markdown("---")  # ×§×• ×”×¤×¨×“×”
            
            # ========================================================================
            #                           ××™×“×¢ ×¢×œ ×”× ×ª×•× ×™× ×‘×¡×¨×’×œ ×”×¦×“
            # ========================================================================
            st.markdown("### ğŸ“Š Data Info")      # ×›×•×ª×¨×ª ××™×“×¢ × ×ª×•× ×™×
            st.write(f"**Rows:** {len(df):,}")           # ××¡×¤×¨ ×©×•×¨×•×ª
            st.write(f"**Columns:** {len(df.columns)}")  # ××¡×¤×¨ ×¢××•×“×•×ª
            st.write(f"**Numeric:** {len(numeric_cols)}") # ××¡×¤×¨ ×¢××•×“×•×ª × ×•××¨×™×•×ª
            
            # ×—×™×©×•×‘ ××—×•×– ×”× ×ª×•× ×™× ×”×—×¡×¨×™×
            missing_pct = (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100
            quality_color = "ğŸŸ¢" if missing_pct < 5 else "ğŸŸ¡" if missing_pct < 15 else "ğŸ”´"
            st.write(f"**Quality:** {quality_color} {100-missing_pct:.1f}%")
        
        else:
            # ========================================================================
            #                           ××“×¨×™×š ×”×ª×—×œ×” ×œ××©×ª××©×™× ×—×“×©×™×
            # ========================================================================
            st.markdown("### ğŸš€ Getting Started")  # ×›×•×ª×¨×ª ××“×¨×™×š ×”×ª×—×œ×”
            st.write("1. Upload your data files")   # ×”×¢×œ××ª ×§×‘×¦×™×
            st.write("2. Or load demo data")        # ×˜×¢×™× ×ª × ×ª×•× ×™ ×“×•×’××”
            st.write("3. Explore with charts")     # ×—×§×™×¨×” ×¢× ×ª×¨×©×™××™×
            st.write("4. Run ML analysis")         # × ×™×ª×•×— ×œ××™×“×ª ××›×•× ×”
            st.write("5. Generate reports")        # ×™×¦×™×¨×ª ×“×•×—×•×ª
            
            st.markdown("---")  # ×§×• ×”×¤×¨×“×”
            
            # ========================================================================
            #                           ×¡×•×’×™ ×§×‘×¦×™× × ×ª××›×™×
            # ========================================================================
            st.markdown("### ğŸ“‹ Supported Files")  # ×›×•×ª×¨×ª ×§×‘×¦×™× × ×ª××›×™×
            st.write("â€¢ CSV files")                # ×§×‘×¦×™ CSV
            st.write("â€¢ Excel (.xlsx, .xls)")     # ×§×‘×¦×™ ××§×¡×œ
            st.write("â€¢ JSON files")              # ×§×‘×¦×™ JSON
            st.write("â€¢ Multiple file upload")    # ×”×¢×œ××ª ×§×‘×¦×™× ××¨×•×‘×™×
        
        st.markdown("---")  # ×§×• ×”×¤×¨×“×”
        
        # ========================================================================
        #                           ××“×•×¨ ×¢×–×¨×” ×•×ª××™×›×”
        # ========================================================================
        st.markdown("### ğŸ†˜ Need Help?")  # ×›×•×ª×¨×ª ×¢×–×¨×”
        with st.expander("ğŸ“– How to use"):  # ×§×•×¤×¡×ª ×¢×–×¨×” ××ª×¨×—×‘×ª
            st.write("""
            **Quick Start:**
            1. Go to Data Upload
            2. Load demo data or upload files
            3. Explore Dashboard for insights
            4. Use Charts for visualization
            5. Try Machine Learning features
            
            **Pro Tips:**
            â€¢ Use 3D Scatter for complex relationships
            â€¢ Check Statistics for data quality
            â€¢ A/B Testing for comparisons
            â€¢ Reports for final analysis
            """)
    
    # ========================================================================
    #                           ×˜×™×¤×•×œ ×‘×¤×¢×•×œ×•×ª ××”×™×¨×•×ª
    # ========================================================================
    # ×‘×“×™×§×” ×•×”×¨×¦×” ×©×œ ×¤×¢×•×œ×•×ª ××”×™×¨×•×ª ×©×”××©×ª××© ×‘×™×§×©
    if 'show_summary' in st.session_state and st.session_state.show_summary:
        show_quick_summary()                    # ×”×¦×’×ª ×¡×™×›×•× ××”×™×¨
        st.session_state.show_summary = False   # ××™×¤×•×¡ ×”×“×’×œ
    
    if 'show_correlation' in st.session_state and st.session_state.show_correlation:
        show_quick_correlation()                  # ×”×¦×’×ª ××ª×× ××”×™×¨
        st.session_state.show_correlation = False # ××™×¤×•×¡ ×”×“×’×œ
        
    if 'show_3d' in st.session_state and st.session_state.show_3d:
        show_quick_3d()                         # ×”×¦×’×ª ×’×¨×£ ×ª×œ×ª ××™××“×™ ××”×™×¨
        st.session_state.show_3d = False        # ××™×¤×•×¡ ×”×“×’×œ
    
    # ========================================================================
    #                           × ×™×ª×•×‘ ×“×¤×™× - ×”×¤× ×™×” ×œ×¤×•× ×§×¦×™×•×ª ×”××ª××™××•×ª
    # ========================================================================
    # ×‘×“×™×§×ª ×”×“×£ ×”× ×‘×—×¨ ×•×”×¤×¢×œ×ª ×”×¤×•× ×§×¦×™×” ×”××ª××™××”
    if page == "ğŸ  Dashboard":                  # ×“×£ ×œ×•×— ×”×‘×§×¨×” ×”×¨××©×™
        show_dashboard()
    elif page == "ğŸ“ Data Upload":             # ×“×£ ×”×¢×œ××ª × ×ª×•× ×™×
        show_upload()
    elif page == "ğŸ“ˆ Charts":                  # ×“×£ ×ª×¨×©×™××™×
        show_charts()
    elif page == "ğŸ“Š Statistics":              # ×“×£ ×¡×˜×˜×™×¡×˜×™×§×•×ª
        show_stats()
    elif page == "ğŸ¤– Machine Learning":        # ×“×£ ×œ××™×“×ª ××›×•× ×”
        show_ml()
    elif page == "ğŸ§ª A/B Testing":            # ×“×£ ×‘×“×™×§×•×ª A/B
        show_ab_testing()
    elif page == "ğŸ’¾ Database":               # ×“×£ ×‘×¡×™×¡ × ×ª×•× ×™×
        show_database()
    elif page == "ğŸ“„ Reports":                # ×“×£ ×“×•×—×•×ª
        show_reports()

# ========================================================================
#                           ×¤×•× ×§×¦×™×•×ª ×¢×–×¨ ×•×ª×•×‘× ×•×ª × ×ª×•× ×™×
# ========================================================================

def show_insights_and_advice(df):
    """
    ×™×¦×™×¨×ª ×ª×•×‘× ×•×ª ×•×”××œ×¦×•×ª ×¢×œ ×‘×¡×™×¡ ×”× ×ª×•× ×™×
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - × ×™×ª×•×— ××™×›×•×ª ×”× ×ª×•× ×™× ×•×™×¦×™×¨×ª ×ª×•×‘× ×•×ª ××•×˜×•××˜×™×•×ª
    - ×”×¦×’×ª ×”××œ×¦×•×ª ×œ×©×™×¤×•×¨ ×”× ×ª×•× ×™× ×•×”× ×™×ª×•×—
    - ×–×™×”×•×™ ×“×¤×•×¡×™× ×•×—×¨×™×’×•×ª ×‘× ×ª×•× ×™×
    - ××ª×Ÿ ×¢×¦×•×ª ××¢×©×™×•×ª ×œ××©×ª××©
    
    ×¤×¨××˜×¨×™×:
        df (DataFrame): ××¡×’×¨×ª ×”× ×ª×•× ×™× ×œ× ×™×ª×•×—
    
    ×”×—×–×¨×”: ×œ×œ× - ×”×¤×•× ×§×¦×™×” ××¦×™×’×” ××ª ×”×ª×•×¦××•×ª ×™×©×™×¨×•×ª ×‘×××©×§
    """
    
    if df is None or len(df) == 0:
        return
    
    insights = []
    advice = []
    
    # Data size analysis
    rows, cols = df.shape
    if rows > 100000:
        insights.append(f"ğŸ“Š Large dataset: {rows:,} rows - excellent sample for analysis!")
        advice.append("ğŸ’¡ Recommend using sampling to speed up visualization")
    elif rows < 100:
        insights.append(f"ğŸ“Š Small dataset: {rows} rows")
        advice.append("âš ï¸ Small sample may limit statistical significance of conclusions")
    
    # Data quality analysis
    missing_pct = (df.isnull().sum().sum() / (rows * cols)) * 100
    if missing_pct > 20:
        insights.append(f"âŒ High percentage of missing data: {missing_pct:.1f}%")
        advice.append("ğŸ”§ Data cleaning needed - fill or remove missing values")
    elif missing_pct < 5:
        insights.append(f"âœ… Excellent data quality: only {missing_pct:.1f}% missing")
        advice.append("ğŸ¯ Data ready for deep analysis and machine learning")
    
    # Data type analysis
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    text_cols = df.select_dtypes(include=['object']).columns
    
    if len(numeric_cols) > len(text_cols):
        insights.append(f"ğŸ”¢ Predominantly numeric data: {len(numeric_cols)} out of {cols} columns")
        advice.append("ğŸ“ˆ Perfect for correlation analysis and regression models")
    elif len(text_cols) > len(numeric_cols):
        insights.append(f"ğŸ“ Lots of text data: {len(text_cols)} out of {cols} columns")
        advice.append("ğŸ”¤ Consider NLP analysis or categorical variable encoding")
    
    # Correlation analysis
    if len(numeric_cols) >= 2:
        corr_matrix = df[numeric_cols].corr()
        high_corr = np.where(np.abs(corr_matrix) > 0.8)
        high_corr_pairs = [(corr_matrix.index[i], corr_matrix.columns[j]) 
                          for i, j in zip(high_corr[0], high_corr[1]) if i != j]
        
        if len(high_corr_pairs) > 0:
            insights.append(f"ğŸ”— Strong correlations found between variables")
            advice.append("âš¡ Use correlation analysis to identify dependencies")
    
    # Duplicate analysis
    duplicates = df.duplicated().sum()
    if duplicates > 0:
        insights.append(f"ğŸ”„ Duplicates found: {duplicates} ({duplicates/rows*100:.1f}%)")
        advice.append("ğŸ§¹ Recommend removing duplicates for analysis accuracy")
    else:
        insights.append("âœ… No duplicates detected")
    
    # Outlier analysis
    outlier_cols = []
    for col in numeric_cols:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        outliers = df[(df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)]
        if len(outliers) > 0:
            outlier_cols.append(col)
    
    if outlier_cols:
        insights.append(f"ğŸ¯ Outliers detected in {len(outlier_cols)} columns")
        advice.append("ğŸ” Investigate outliers - they may contain important information")
    
    # Display insights and advice
    if insights:
        st.markdown("### ğŸ’¡ Data Insights")
        for insight in insights:
            st.markdown(f'<div class="insight-box">{insight}</div>', unsafe_allow_html=True)
    
    if advice:
        st.markdown("### ğŸ¯ Recommendations")
        for adv in advice:
            st.markdown(f'<div class="advice-box">{adv}</div>', unsafe_allow_html=True)

def show_dashboard():
    """
    ×”×¦×’×ª ×œ×•×— ×”×‘×§×¨×” ×”×¨××©×™ ×©×œ ×”××¤×œ×™×§×¦×™×”
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - ×”×¦×’×ª ××¡×š ×¤×ª×™×—×” ××˜×¨×§×˜×™×‘×™ ×¢× ××¤×©×¨×•×™×•×ª ×˜×¢×™× ×ª × ×ª×•× ×™×
    - ×”×¦×’×ª ××˜×¨×™×§×•×ª ××¤×ª×— ×•×ª×•×‘× ×•×ª ×¢×œ ×”× ×ª×•× ×™× ×”×˜×¢×•× ×™×
    - ××ª×Ÿ ×’×™×©×” ××”×™×¨×” ×œ×¤×•× ×§×¦×™×•×ª ×”× ×™×ª×•×— ×”×—×©×•×‘×•×ª
    - ×™×¦×™×¨×ª ×××©×§ ××¨×›×–×™ ×œ× ×™×•×•×˜ ×‘×™×Ÿ ×”×™×›×•×œ×•×ª ×”×©×•× ×•×ª
    
    ×”×ª× ×”×’×•×ª:
    - ×× ××™×Ÿ × ×ª×•× ×™× ×˜×¢×•× ×™×: ×”×¦×’×ª ××¡×š ×¤×ª×™×—×” ×¢× ×›×¤×ª×•×¨×™ ×˜×¢×™× ×ª ×“×•×’××”
    - ×× ×™×© × ×ª×•× ×™×: ×”×¦×’×ª ×œ×•×— ×‘×§×¨×” ××œ× ×¢× ××˜×¨×™×§×•×ª ×•×ª×¨×©×™××™×
    - ×›×•×œ×œ ×›×¤×ª×•×¨×™× ×œ× ×™×ª×•×— ××•×˜×•××˜×™ ×•×™×¦×™×¨×ª ×ª×•×‘× ×•×ª ×—×›××•×ª
    
    ×”×—×–×¨×”: ×œ×œ× - ×”×¤×•× ×§×¦×™×” ××¦×™×’×” ××ª ×”×ª×•×›×Ÿ ×™×©×™×¨×•×ª ×‘×××©×§
    """
    st.markdown("## ğŸ  Welcome to DataBot Analytics Pro!")
    
    # Enhanced Dashboard with multiple sections
    if 'data' not in st.session_state:
        # Initial welcome section
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.markdown("### ğŸš€ Get Started")
            if st.button("ğŸ² Load Demo Data"):
                demo_data = create_demo_data()
                st.session_state.data = demo_data
                st.success("Demo data loaded! ğŸ‰")
                st.rerun()
            
            if st.button("ğŸ›’ Load E-commerce Data"):
                ecommerce_data = create_ecommerce_data()
                st.session_state.data = ecommerce_data
                st.success("E-commerce data loaded! ğŸ’°")
                st.rerun()
            
            if st.button("ğŸ“Š Generate Financial Data"):
                financial_data = create_financial_data()
                st.session_state.data = financial_data
                st.success("Financial data loaded! ğŸ’¹")
                st.rerun()
        
        with col2:
            st.markdown("### ğŸ“‹ Quick Info")
            st.info("ğŸ” DataBot Analytics Pro provides comprehensive data analysis capabilities")
            st.write("**Features:**")
            st.write("â€¢ Advanced visualizations")
            st.write("â€¢ Machine learning models")
            st.write("â€¢ Statistical analysis")
            st.write("â€¢ A/B testing")
            st.write("â€¢ Custom reports")
            st.write("â€¢ SQL database operations")
        
        # Feature showcase
        st.markdown("### âœ¨ Feature Highlights")
        
        feature_col1, feature_col2, feature_col3 = st.columns(3)
        
        with feature_col1:
            st.markdown("""
            **ğŸ“ˆ Advanced Analytics**
            - Interactive visualizations
            - 3D scatter plots
            - Correlation heatmaps
            - Distribution analysis
            """)
        
        with feature_col2:
            st.markdown("""
            **ğŸ¤– Machine Learning**
            - K-means clustering
            - PCA analysis
            - Anomaly detection
            - Feature importance
            """)
        
        with feature_col3:
            st.markdown("""
            **ğŸ“Š Business Intelligence**
            - Executive dashboards
            - A/B testing
            - Statistical reports
            - Data quality metrics
            """)
        
        return
    
    # Main dashboard when data is loaded
    df = st.session_state.data
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    text_cols = df.select_dtypes(include=['object']).columns
    
    # Enhanced header with action buttons
    col1, col2, col3 = st.columns([2, 1, 1])
    
    with col1:
        st.markdown("### ğŸ“Š Data Overview Dashboard")
    with col2:
        if st.button("ğŸ” Auto-Analysis"):
            auto_analyze_data()
    with col3:
        if st.button("ğŸ¯ Smart Insights"):
            generate_smart_insights(df)
    
    # Key metrics section
    st.markdown("### ğŸ“ˆ Key Metrics")
    metric_col1, metric_col2, metric_col3, metric_col4, metric_col5 = st.columns(5)
    
    with metric_col1:
        st.metric("ğŸ“ Total Rows", f"{len(df):,}")
    with metric_col2:
        st.metric("ğŸ“Š Columns", f"{len(df.columns)}")
    with metric_col3:
        st.metric("ğŸ”¢ Numeric", f"{len(numeric_cols)}")
    with metric_col4:
        missing_pct = (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100
        st.metric("âŒ Missing %", f"{missing_pct:.1f}%")
    with metric_col5:
        quality_score = calculate_data_quality(df)
        st.metric("â­ Quality Score", f"{quality_score:.1f}/10")
    
    # Data insights section
    show_insights_and_advice(df)
    
    # Interactive dashboard sections
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š Quick Viz", "ğŸ” Data Explorer", "ğŸ¯ Recommendations", "ğŸ“ˆ Trends"])
    
    with tab1:
        # Quick visualizations
        if len(numeric_cols) > 0:
            viz_col1, viz_col2 = st.columns(2)
            
            with viz_col1:
                # Auto-select best chart for first numeric column
                selected_col = st.selectbox("Select metric for quick viz", numeric_cols, key="quick_viz")
                
                if len(numeric_cols) >= 2:
                    fig = px.line(df.reset_index(), x='index', y=selected_col, 
                                title=f"Trend: {selected_col}")
                    st.plotly_chart(fig, use_container_width=True)
                else:
                    fig = px.histogram(df, x=selected_col, title=f"Distribution: {selected_col}")
                    st.plotly_chart(fig, use_container_width=True)
            
            with viz_col2:
                if len(numeric_cols) > 1:
                    # Correlation heatmap
                    corr_matrix = df[numeric_cols].corr()
                    fig = px.imshow(corr_matrix, title="Correlation Matrix", 
                                  color_continuous_scale="RdBu")
                    st.plotly_chart(fig, use_container_width=True)
                elif len(text_cols) > 0:
                    # Category distribution
                    cat_col = text_cols[0]
                    value_counts = df[cat_col].value_counts().head(10)
                    fig = px.bar(x=value_counts.index, y=value_counts.values,
                               title=f"Top Categories: {cat_col}")
                    st.plotly_chart(fig, use_container_width=True)
    
    with tab2:
        # Data explorer
        st.markdown("#### ğŸ” Interactive Data Explorer")
        
        # Filter controls
        filter_col1, filter_col2 = st.columns(2)
        
        with filter_col1:
            if len(text_cols) > 0:
                selected_category = st.selectbox("Filter by category", ["All"] + list(text_cols))
                if selected_category != "All":
                    category_values = st.multiselect(
                        f"Select {selected_category} values",
                        df[selected_category].unique(),
                        default=df[selected_category].unique()[:5]
                    )
                    if category_values:
                        df_filtered = df[df[selected_category].isin(category_values)]
                    else:
                        df_filtered = df
                else:
                    df_filtered = df
            else:
                df_filtered = df
        
        with filter_col2:
            if len(numeric_cols) > 0:
                selected_numeric = st.selectbox("Filter by numeric range", ["None"] + list(numeric_cols))
                if selected_numeric != "None":
                    min_val, max_val = st.slider(
                        f"Select {selected_numeric} range",
                        float(df[selected_numeric].min()),
                        float(df[selected_numeric].max()),
                        (float(df[selected_numeric].min()), float(df[selected_numeric].max()))
                    )
                    df_filtered = df_filtered[
                        (df_filtered[selected_numeric] >= min_val) & 
                        (df_filtered[selected_numeric] <= max_val)
                    ]
        
        # Display filtered data
        st.write(f"**Filtered Data:** {len(df_filtered)} rows (from {len(df)} total)")
        st.dataframe(df_filtered.head(20))
        
        # Quick stats on filtered data
        if len(df_filtered) > 0 and len(numeric_cols) > 0:
            st.markdown("#### ğŸ“Š Filtered Data Statistics")
            quick_stats = df_filtered[numeric_cols].describe().round(2)
            st.dataframe(quick_stats)
    
    with tab3:
        # Smart recommendations
        st.markdown("#### ğŸ¯ Smart Recommendations")
        generate_dashboard_recommendations(df)
    
    with tab4:
        # Trend analysis
        st.markdown("#### ğŸ“ˆ Trend Analysis")
        if len(numeric_cols) > 0:
            trend_col = st.selectbox("Select column for trend analysis", numeric_cols, key="trend_analysis")
            
            # Create trend visualization
            df_trend = df.copy()
            df_trend['index'] = range(len(df_trend))
            
            # Calculate moving average
            window_size = min(20, len(df_trend) // 10)
            if window_size > 1:
                df_trend[f'{trend_col}_MA'] = df_trend[trend_col].rolling(window=window_size).mean()
            
            fig = go.Figure()
            
            # Original data
            fig.add_trace(go.Scatter(
                x=df_trend['index'],
                y=df_trend[trend_col],
                mode='lines',
                name='Original',
                opacity=0.6
            ))
            
            # Moving average
            if window_size > 1:
                fig.add_trace(go.Scatter(
                    x=df_trend['index'],
                    y=df_trend[f'{trend_col}_MA'],
                    mode='lines',
                    name=f'Moving Average ({window_size})',
                    line=dict(width=3)
                ))
            
            fig.update_layout(
                title=f"Trend Analysis: {trend_col}",
                xaxis_title="Data Points",
                yaxis_title=trend_col
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
            # Trend insights
            trend_analysis = analyze_trend(df[trend_col])
            st.info(f"ğŸ“ˆ Trend Analysis: {trend_analysis}")
            
            # Additional trend metrics
            if len(df) > 10:
                first_quarter = df[trend_col][:len(df)//4].mean()
                last_quarter = df[trend_col][-len(df)//4:].mean()
                overall_change = ((last_quarter - first_quarter) / first_quarter) * 100
                
                if abs(overall_change) > 5:
                    change_direction = "increased" if overall_change > 0 else "decreased"
                    st.write(f"ğŸ“Š Overall trend: {trend_col} has {change_direction} by {abs(overall_change):.1f}%")
        
        # Volatility analysis
        if len(numeric_cols) > 0:
            st.markdown("#### ğŸ“Š Volatility Analysis")
            volatility_data = {}
            
            for col in numeric_cols[:5]:  # Analyze top 5 numeric columns
                cv = (df[col].std() / df[col].mean()) * 100 if df[col].mean() != 0 else 0
                volatility_data[col] = cv
            
            volatility_df = pd.DataFrame(list(volatility_data.items()), 
                                       columns=['Metric', 'Coefficient of Variation (%)'])
            
            fig = px.bar(volatility_df, x='Metric', y='Coefficient of Variation (%)',
                        title="Data Volatility Analysis")
            st.plotly_chart(fig, use_container_width=True)

def generate_smart_insights(df):
    """
    ×™×¦×™×¨×ª ×ª×•×‘× ×•×ª ×—×›××•×ª ×‘×××¦×¢×•×ª × ×™×ª×•×— ××ª×§×“× ×©×œ ×”× ×ª×•× ×™×
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - ×‘×™×¦×•×¢ × ×™×ª×•×— ××¢××™×§ ×•××•×˜×•××˜×™ ×©×œ ××¡×’×¨×ª ×”× ×ª×•× ×™×
    - ×–×™×”×•×™ ×“×¤×•×¡×™×, ×§×©×¨×™× ×•×—×¨×™×’×•×ª ×‘× ×ª×•× ×™×
    - ×™×¦×™×¨×ª ×”××œ×¦×•×ª ××•×ª×××•×ª ××™×©×™×ª ×œ××©×ª××©
    - ×”×¦×’×ª ×ª×•×‘× ×•×ª ×‘×¦×•×¨×” ×‘×”×™×¨×” ×•××•×‘× ×ª
    
    ×¤×¨××˜×¨×™×:
        df (DataFrame): ××¡×’×¨×ª ×”× ×ª×•× ×™× ×œ× ×™×ª×•×—
    
    ×¡×•×’×™ ×”×ª×•×‘× ×•×ª ×©××™×•×¦×¨×•×ª:
    - × ×™×ª×•×— ×’×•×“×œ ×”×“××˜×” ×•×”××œ×¦×•×ª ×¢×œ ×˜×›× ×™×§×•×ª ××ª××™××•×ª
    - ×–×™×”×•×™ ×‘×¢×™×•×ª ×‘××™×›×•×ª ×”× ×ª×•× ×™× (×¢×¨×›×™× ×—×¡×¨×™×)
    - ××¦×™××ª ××ª×××™× ×—×–×§×™× ×‘×™×Ÿ ××©×ª× ×™×
    - ×–×™×”×•×™ ×¢×¨×›×™× ×—×¨×™×’×™× (outliers)
    - × ×™×ª×•×— ×”×ª×¤×œ×’×•×™×•×ª ×•×”××œ×¦×•×ª ×¢×œ ×˜×¨× ×¡×¤×•×¨××¦×™×•×ª
    - ×”×¢×¨×›×ª ×¨××ª ×”×™×™×—×•×“×™×•×ª ×‘××©×ª× ×™× ×§×˜×’×•×¨×™××œ×™×™×
    
    ×”×—×–×¨×”: ×œ×œ× - ×”×¤×•× ×§×¦×™×” ××¦×™×’×” ××ª ×”×ª×•×‘× ×•×ª ×™×©×™×¨×•×ª ×‘×××©×§
    """
    st.markdown("### ğŸ§  Smart Insights")
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    text_cols = df.select_dtypes(include=['object']).columns
    
    insights = []
    
    # Data size insights
    if len(df) > 100000:
        insights.append("ğŸ¯ **Big Data Opportunity**: Your dataset is large enough for advanced machine learning techniques")
    elif len(df) < 100:
        insights.append("âš ï¸ **Small Sample Alert**: Consider collecting more data for robust statistical analysis")
    
    # Missing data insights
    missing_cols = df.columns[df.isnull().sum() > 0]
    if len(missing_cols) > 0:
        worst_missing = df.isnull().sum().idxmax()
        missing_pct = (df[worst_missing].isnull().sum() / len(df)) * 100
        insights.append(f"ğŸ”§ **Data Quality Focus**: '{worst_missing}' has {missing_pct:.1f}% missing values - prioritize cleaning")
    
    # Correlation insights
    if len(numeric_cols) >= 2:
        corr_matrix = df[numeric_cols].corr()
        high_corr_pairs = []
        
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_val = abs(corr_matrix.iloc[i, j])
                if corr_val > 0.8:
                    high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))
        
        if high_corr_pairs:
            best_pair = max(high_corr_pairs, key=lambda x: x[2])
            insights.append(f"ğŸ”— **Strong Relationship**: '{best_pair[0]}' and '{best_pair[1]}' are highly correlated ({best_pair[2]:.3f})")
    
    # Outlier insights
    outlier_summary = {}
    for col in numeric_cols:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        outliers = df[(df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)]
        outlier_summary[col] = len(outliers)
    
    if outlier_summary:
        max_outlier_col = max(outlier_summary, key=outlier_summary.get)
        if outlier_summary[max_outlier_col] > 0:
            outlier_pct = (outlier_summary[max_outlier_col] / len(df)) * 100
            insights.append(f"ğŸ¯ **Outlier Alert**: '{max_outlier_col}' has {outlier_summary[max_outlier_col]} outliers ({outlier_pct:.1f}%)")
    
    # Categorical insights
    if len(text_cols) > 0:
        for col in text_cols[:3]:
            unique_ratio = df[col].nunique() / len(df)
            if unique_ratio > 0.8:
                insights.append(f"ğŸ†” **High Uniqueness**: '{col}' might be an identifier (80%+ unique values)")
            elif unique_ratio < 0.1:
                insights.append(f"ğŸ“Š **Low Diversity**: '{col}' has limited categories - good for grouping analysis")
    
    # Distribution insights
    for col in numeric_cols[:3]:
        skewness = df[col].skew()
        if abs(skewness) > 2:
            direction = "right" if skewness > 0 else "left"
            insights.append(f"ğŸ“ˆ **Skewed Distribution**: '{col}' is highly {direction}-skewed - consider transformation")
    
    # Display insights
    if insights:
        for insight in insights:
            st.write(f"â€¢ {insight}")
    else:
        st.info("ğŸ¤– No specific insights detected. Your data appears well-balanced!")

def generate_dashboard_recommendations(df):
    """
    ×™×¦×™×¨×ª ×”××œ×¦×•×ª ××¢×©×™×•×ª ×œ×œ×•×— ×”×‘×§×¨×” ×¢×œ ×‘×¡×™×¡ × ×™×ª×•×— ×”× ×ª×•× ×™×
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - × ×™×ª×•×— ××‘× ×” ×”× ×ª×•× ×™× ×•×™×¦×™×¨×ª ×”××œ×¦×•×ª ××•×ª×××•×ª
    - ×–×™×”×•×™ ×”×–×“×× ×•×™×•×ª ×œ× ×™×ª×•×— ××ª×§×“× ×‘××¡×’×¨×ª ×”× ×ª×•× ×™×
    - ×”×¦×¢×ª ×“×¨×›×™ ×¤×¢×•×œ×” ×§×•× ×§×¨×˜×™×•×ª ×œ×—×§×¨ ×”× ×ª×•× ×™×
    - ××ª×Ÿ ×›×™×•×•× ×™× ×œ×©×™××•×© ×‘××œ×’×•×¨×™×ª××™ ×œ××™×“×ª ××›×•× ×”
    
    ×¤×¨××˜×¨×™×:
        df (DataFrame): ××¡×’×¨×ª ×”× ×ª×•× ×™× ×œ× ×™×ª×•×—
    
    ×¡×•×’×™ ×”×”××œ×¦×•×ª ×©××™×•×¦×¨×•×ª:
    - ×”××œ×¦×•×ª ×¢×œ ×¡×•×’×™ ×ª×¨×©×™××™× ××ª××™××™×
    - ×”×¦×¢×•×ª ×œ× ×™×ª×•×—×™× ×¡×˜×˜×™×¡×˜×™×™× ×¡×¤×¦×™×¤×™×™×
    - ×›×™×•×•× ×™× ×œ×™×™×©×•× ××œ×’×•×¨×™×ª××™ ×œ××™×“×ª ××›×•× ×”
    - ×”××œ×¦×•×ª ×¢×œ ×¦×¢×“×™ × ×™×§×•×™ × ×ª×•× ×™× × ×“×¨×©×™×
    - ×”×¦×¢×•×ª ×œ×‘×“×™×§×•×ª A/B ××¢× ×™×™× ×•×ª
    
    ×”×—×–×¨×”: ×œ×œ× - ×”×¤×•× ×§×¦×™×” ××¦×™×’×” ××ª ×”×”××œ×¦×•×ª ×™×©×™×¨×•×ª ×‘×××©×§
    """
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    text_cols = df.select_dtypes(include=['object']).columns
    
    recommendations = []
    
    # Analysis recommendations
    if len(numeric_cols) >= 3:
        recommendations.append({
            "title": "ğŸ¤– Machine Learning Opportunity",
            "description": "With 3+ numeric variables, you can perform clustering and PCA analysis",
            "action": "Go to Machine Learning section",
            "priority": "High"
        })
    
    if len(numeric_cols) >= 2:
        recommendations.append({
            "title": "ğŸ“Š Correlation Analysis",
            "description": "Explore relationships between your numeric variables",
            "action": "Check Charts â†’ Heatmap or Scatter Plot",
            "priority": "Medium"
        })
    
    # Data quality recommendations
    missing_pct = (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100
    if missing_pct > 5:
        recommendations.append({
            "title": "ğŸ”§ Data Cleaning Needed",
            "description": f"Your data has {missing_pct:.1f}% missing values",
            "action": "Use Data Upload â†’ Fill Missing Values",
            "priority": "High"
        })
    
    # Visualization recommendations
    if len(text_cols) > 0 and len(numeric_cols) > 0:
        recommendations.append({
            "title": "ğŸ“ˆ Category Analysis",
            "description": "Compare numeric metrics across categories",
            "action": "Try Charts â†’ Bar Chart or Box Plot",
            "priority": "Medium"
        })
    
    if len(numeric_cols) >= 3:
        recommendations.append({
            "title": "ğŸŒ 3D Visualization",
            "description": "Explore 3D relationships in your data",
            "action": "Use Charts â†’ 3D Scatter Plot",
            "priority": "Medium"
        })
    
    # Statistical recommendations
    if len(df) > 1000:
        recommendations.append({
            "title": "ğŸ§ª A/B Testing Ready",
            "description": "Your sample size is suitable for statistical testing",
            "action": "Explore A/B Testing section",
            "priority": "Low"
        })
    
    # Business recommendations
    if 'revenue' in [col.lower() for col in df.columns] or 'sales' in [col.lower() for col in df.columns]:
        recommendations.append({
            "title": "ğŸ’° Business Analytics",
            "description": "Generate business intelligence reports",
            "action": "Create Executive Summary in Reports",
            "priority": "High"
        })
    
    # Display recommendations
    for i, rec in enumerate(recommendations):
        priority_color = {
            "High": "ğŸ”´",
            "Medium": "ğŸŸ¡", 
            "Low": "ğŸŸ¢"
        }
        
        with st.expander(f"{priority_color[rec['priority']]} {rec['title']}", expanded=(i < 2)):
            st.write(rec['description'])
            st.info(f"**Recommended Action:** {rec['action']}")

def create_financial_data():
    """
    ×™×¦×™×¨×ª × ×ª×•× ×™ ×“×•×’××” ×¤×™× × ×¡×™×™×/×”×©×§×¢×•×ª ×œ×¦×•×¨×›×™ ×”×“×’××”
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - ×™×¦×™×¨×ª ××¡×’×¨×ª × ×ª×•× ×™× ×¡×™× ×ª×˜×™×ª ×”×“×•××” ×œ× ×ª×•× ×™× ×¤×™× × ×¡×™×™× ×××™×ª×™×™×
    - ×”×“××™×” ×©×œ ××—×™×¨×™ ×× ×™×•×ª, × ×¤×—×™ ××¡×—×¨ ×•××“×“×™× ×¤×™× × ×¡×™×™× ×©×•× ×™×
    - ×™×¦×™×¨×ª ×§×•×¨×œ×¦×™×•×ª ×¨×™××œ×™×¡×˜×™×•×ª ×‘×™×Ÿ ×”××©×ª× ×™× ×”×¤×™× × ×¡×™×™×
    - ××ª×Ÿ ×‘×¡×™×¡ ×œ× ×™×ª×•×— ×•×•×™×–×•××œ×™×–×¦×™×” ×©×œ × ×ª×•× ×™× ×¤×™× × ×¡×™×™×
    
    ×”× ×ª×•× ×™× ×©× ×•×¦×¨×™× ×›×•×œ×œ×™×:
    - ××—×™×¨×™ ×× ×™×•×ª ×¢× ×˜×¨× ×“ ×•×•×œ×˜×™×œ×™×•×ª ×¨×™××œ×™×¡×˜×™×™×
    - × ×¤×—×™ ××¡×—×¨ (Volume) ×¢× ×”×ª×¤×œ×’×•×ª ×œ×•×’-× ×•×¨××œ×™×ª  
    - ×©×•×•×™ ×©×•×§ (Market Cap) ××‘×•×¡×¡ ×¢×œ ××—×™×¨ ×”×× ×™×”
    - ×™×—×¡ ××—×™×¨ ×œ×¨×•×•×— (P/E Ratio) ××©×ª× ×” ×œ×¤×™ ×¡×§×˜×•×¨
    - ×ª×©×•××ª ×“×™×‘×™×“× ×“ (Dividend Yield)
    - ××©×ª× ×™× ×§×˜×’×•×¨×™××œ×™×™×: ×¡×§×˜×•×¨, ×¨××ª ×¡×™×›×•×Ÿ
    - ××“×“ ESG (Environmental, Social, Governance)
    - ××˜×¨×™×§×•×ª ××—×•×©×‘×•×ª: ×ª×©×•××” ×™×•××™×ª, ×•×•×œ×˜×™×œ×™×•×ª, ×××•×¦×¢ × ×¢
    
    ×”×—×–×¨×”:
        DataFrame: ××¡×’×¨×ª × ×ª×•× ×™× ×¢× 500 ×¨×©×•××•×ª ×©×œ × ×ª×•× ×™× ×¤×™× × ×¡×™×™× ×¡×™× ×ª×˜×™×™×
    """
    np.random.seed(456)
    
    n_records = 500
    
    # Create realistic financial data
    dates = pd.date_range('2023-01-01', periods=n_records, freq='D')
    
    # Simulate stock prices with some trend and volatility
    initial_price = 100
    returns = np.random.normal(0.001, 0.02, n_records)  # Daily returns
    prices = [initial_price]
    
    for r in returns[1:]:
        prices.append(prices[-1] * (1 + r))
    
    data = {
        'Date': dates,
        'Stock_Price': prices,
        'Volume': np.random.lognormal(10, 1, n_records).astype(int),
        'Market_Cap': np.array(prices) * np.random.uniform(1000000, 5000000, n_records),
        'P_E_Ratio': np.random.uniform(10, 30, n_records),
        'Dividend_Yield': np.random.uniform(0, 0.05, n_records),
        'Sector': np.random.choice(['Technology', 'Healthcare', 'Finance', 'Energy'], n_records),
        'Risk_Level': np.random.choice(['Low', 'Medium', 'High'], n_records),
        'ESG_Score': np.random.uniform(1, 10, n_records)
    }
    
    df = pd.DataFrame(data)
    
    # Add calculated metrics
    df['Daily_Return'] = df['Stock_Price'].pct_change()
    df['Volatility'] = df['Daily_Return'].rolling(window=30).std()
    df['Moving_Avg_50'] = df['Stock_Price'].rolling(window=50).mean()
    
    # Add some correlations
    df.loc[df['Sector'] == 'Technology', 'P_E_Ratio'] *= 1.5  # Tech stocks higher P/E
    df.loc[df['Risk_Level'] == 'High', 'Volatility'] *= 1.3  # High risk = more volatile
    
    return df

def show_upload():
    """
    ×”×¦×’×ª ×××©×§ ×”×¢×œ××ª ×§×‘×¦×™× ×¢× ×ª××™×›×” ×œ××›×©×™×¨×™× × ×™×™×“×™× ×•×“×¡×§×˜×•×¤
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - ××ª×Ÿ ××¤×©×¨×•×™×•×ª ×œ×”×¢×œ××ª ×§×‘×¦×™ × ×ª×•× ×™× ×‘×¤×•×¨××˜×™× ×©×•× ×™× (CSV, Excel, JSON)
    - ×”×˜××¢×ª ××•×¤×˜×™××™×–×¦×™×•×ª ××™×•×—×“×•×ª ×œ××›×©×™×¨×™× × ×™×™×“×™×
    - ××ª×Ÿ ×—×œ×•×¤×•×ª ×œ×˜×¢×™× ×ª × ×ª×•× ×™× ×¢×‘×•×¨ ××©×ª××©×™ ××›×©×™×¨×™× × ×™×™×“×™×
    - ×™×™×©×•× ×× ×’× ×•× ×™ ×”×’× ×” ××¤× ×™ ×©×’×™××•×ª AxiosError
    
    ×¤×™×¦'×¨×™× ××¨×›×–×™×™×:
    - ×–×™×”×•×™ ××•×˜×•××˜×™ ×©×œ ××›×©×™×¨×™× × ×™×™×“×™× ×•×”×ª×××ª ×××©×§
    - ×”×’×‘×œ×ª ×’×•×“×œ ×§×‘×¦×™× (5MB ×œ××›×©×™×¨×™× × ×™×™×“×™×, 200MB ×œ×“×¡×§×˜×•×¤)
    - ×ª××™×›×” ×‘×”×¢×œ××” ××¨×•×‘×” ×©×œ ×§×‘×¦×™×
    - ××¤×©×¨×•×™×•×ª ×—×œ×•×¤×™×•×ª: ×‘×•×˜ ×˜×œ×’×¨×, ×˜×¢×™× ×ª × ×ª×•× ×™ ×“×•×’××”
    - ××¡×›×™ ×¢×–×¨×” ×•××¤×©×¨×•×™×•×ª × ×™×§×•×™ × ×ª×•× ×™×
    - ×ª×¦×•×’×” ××§×“×™××” ×©×œ ×”× ×ª×•× ×™× ×œ×¤× ×™ ×¢×™×‘×•×“
    
    ×”×ª× ×”×’×•×ª ×”××¢×¨×›×ª:
    - ×‘××¦×‘ × ×™×™×“: ×”×’×‘×œ×•×ª ×§×¤×“× ×™×•×ª ×™×•×ª×¨ ×•×›×œ×™× × ×•×—×™× ×™×•×ª×¨
    - ×‘××¦×‘ ×“×¡×§×˜×•×¤: ×™×›×•×œ×•×ª ××œ××•×ª ×•×ª××™×›×” ×‘×§×‘×¦×™× ×’×“×•×œ×™×
    - ×©××™×¨×ª ×”× ×ª×•× ×™× ×‘××¦×‘ ×”×¡×©×Ÿ ×œ×”××©×š ×¢×‘×•×“×”
    
    ×”×—×–×¨×”: ×œ×œ× - ×”×¤×•× ×§×¦×™×” ××¦×™×’×” ××ª ×”×××©×§ ×™×©×™×¨×•×ª
    """
    st.markdown("## ğŸ“‚ Data Upload")
    
    # Desktop-optimized file upload (no mobile complexity)
    max_size = 200 * 1024 * 1024  # 200MB for desktop
    st.markdown("### ğŸ“ File Upload - **Up to 200MB**")

    uploaded_files = st.file_uploader(
        "Select files",
        type=['csv', 'xlsx', 'xls', 'json'],
        accept_multiple_files=True,
        help=f"Supported formats: CSV, Excel, JSON. Max size: {max_size // (1024*1024)}MB"
    )
    
    # ========================================================================
    #                           ××¤×©×¨×•×ª ×—×œ×•×¤×™×ª - ×‘×•×˜ ×˜×œ×’×¨×
    # ========================================================================
    # ×”×¦×’×ª ×›×¤×ª×•×¨ ×™×¤×” ×œ×‘×•×˜ ×”×˜×œ×’×¨× ×›××œ×˜×¨× ×˜×™×‘×” ×œ×˜×¢×™× ×ª ×§×‘×¦×™×
    # ×”×›×¤×ª×•×¨ ××•×¦×’ ×¨×§ ×›××©×¨ ×œ× ×”×•×¢×œ×• ×§×‘×¦×™× ×œ××¢×¨×›×ª
    if not uploaded_files:
        st.markdown("---")
        st.markdown("### ğŸ¤– Or Use Telegram Bot:")
        
        # ×›×¤×ª×•×¨ ××™× ×˜×¨××§×˜×™×‘×™ ×œ×¤×ª×™×—×ª ×”×‘×•×˜ ×¢× ×× ×™××¦×™×”
        if st.button("ğŸ“± Open DataBot", use_container_width=True, type="secondary"):
            st.balloons()  # ×”×¦×’×ª ×‘×œ×•× ×™× ×œ×—×’×™×’×”
            st.success("ğŸš€ Opening Telegram Bot...")
            st.markdown("ğŸ‘‰ **[Click here to open bot](https://t.me/maydatabot123_bot)**")
            st.info("ğŸ’¡ The bot offers stable file uploads and mobile-friendly interface!")

    if uploaded_files:
        # Desktop processing - clean and fast
        st.info("ğŸš€ **Processing files...**")
        
        # Clear any cached data that might cause conflicts
        if 'data' in st.session_state:
            del st.session_state['data']
        
        dfs = []
        
        # Progress bar for desktop processing
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, uploaded_file in enumerate(uploaded_files):
            try:
                # Check file size
                file_size = uploaded_file.size if hasattr(uploaded_file, 'size') else len(uploaded_file.getvalue())
                
                if file_size > max_size:
                    st.error(f"âŒ {uploaded_file.name}: File too large ({file_size/(1024*1024):.1f}MB). Max: {max_size/(1024*1024)}MB")
                    continue
                
                status_text.text(f"ğŸ“‚ Processing {uploaded_file.name}...")
                file_name = uploaded_file.name.lower()
                
                # Desktop processing - no retries needed
                max_retries = 1
                df = None
                
                for attempt in range(max_retries):
                    try:
                        if file_name.endswith('.csv'):
                            # Try to determine delimiter
                            try:
                                sample = str(uploaded_file.read(1024))
                                uploaded_file.seek(0)
                                
                                # Desktop processing - full file reading
                                if ';' in sample:
                                    df = pd.read_csv(uploaded_file, sep=';', encoding='utf-8')
                                else:
                                    df = pd.read_csv(uploaded_file, encoding='utf-8')
                            except UnicodeDecodeError:
                                # Fallback encoding for problematic files
                                uploaded_file.seek(0)
                                df = pd.read_csv(uploaded_file, encoding='latin-1')
                                
                        elif file_name.endswith(('.xlsx', '.xls')):
                            df = pd.read_excel(uploaded_file)
                        elif file_name.endswith('.json'):
                            df = pd.read_json(uploaded_file)
                        else:
                            st.error(f"âŒ Unsupported format: {file_name}")
                            break
                        
                        # If successful, break retry loop
                        if df is not None:
                            break
                            
                    except Exception as retry_error:
                        if attempt < max_retries - 1:
                            st.warning(f"âš ï¸ Attempt {attempt + 1} failed for {uploaded_file.name}, retrying...")
                            time.sleep(1)  # Brief pause before retry
                        else:
                            raise retry_error
                
                if df is not None:
                    
                    dfs.append(df)
                    st.success(f"âœ… {uploaded_file.name} â€” Loaded {len(df)} rows, {len(df.columns)} columns")
                
                # Update progress
                progress_bar.progress((i + 1) / len(uploaded_files))

            except Exception as e:
                error_msg = str(e).lower()
                st.error(f"âš ï¸ Error reading {uploaded_file.name}: {str(e)}")
                
                # Specific AxiosError handling
                if 'network' in error_msg or 'axios' in error_msg or 'timeout' in error_msg:
                    st.error("ğŸš¨ **AxiosError/Network Error Detected!**")
                    st.info("ğŸ”§ **Try these solutions:**")
                    st.markdown("""
                    - âœ… **Enable Mobile Mode** in sidebar
                    - ğŸ”„ **Refresh page** and try again
                    - ğŸ“¶ **Check internet connection**
                    - ğŸ“± **Use Telegram bot** instead
                    - ğŸ“ **Try smaller file** (<5MB)
                    """)
        
        # Clear progress indicators
        progress_bar.empty()
        status_text.empty()

        if dfs:
            if len(dfs) == 1:
                combined_df = dfs[0]
            else:
                # ×‘×“×™×§×ª ×–×™×›×¨×•×Ÿ ×œ×¤× ×™ ××™×—×•×“ ×§×‘×¦×™×
                total_rows = sum(len(df) for df in dfs)
                total_memory_mb = sum(df.memory_usage(deep=True).sum() / (1024*1024) for df in dfs)
                
                # ××–×”×¨×” ×× ×”× ×ª×•× ×™× ×’×“×•×œ×™× ××“×™
                if total_rows > 1000000:  # ×™×•×ª×¨ ×××™×œ×™×•×Ÿ ×©×•×¨×•×ª
                    st.warning(f"âš ï¸ **Large dataset detected:** {total_rows:,} total rows, ~{total_memory_mb:.1f} MB")
                    st.info("This may use significant memory. Consider processing files individually if you experience issues.")
                elif total_memory_mb > 500:  # ×™×•×ª×¨ ×-500MB
                    st.warning(f"âš ï¸ **Memory usage:** ~{total_memory_mb:.1f} MB - Processing large dataset...")
                
                # Combine files with improved error handling
                try:
                    with st.spinner("ğŸ”„ Combining files..."):
                        combined_df = pd.concat(dfs, ignore_index=True)
                except MemoryError:
                    st.error("âŒ **Memory Error:** Dataset too large to combine. Try processing files individually.")
                    combined_df = dfs[0]  # Use first file as fallback
                except Exception as e:
                    st.error(f"Error combining files: {e}")
                    combined_df = dfs[0]  # Use first file
            
            st.session_state.data = combined_df
            st.success(f"ğŸ“Š Total loaded: {len(combined_df)} rows, {len(combined_df.columns)} columns")
            
            # Preview
            st.markdown("### ğŸ‘€ Preview")
            st.dataframe(combined_df.head(10))
            
            # Show insights
            show_insights_and_advice(combined_df)
            
            # Data cleaning tools
            st.markdown("### ğŸ”§ Data Cleaning")
            col1, col2, col3 = st.columns(3)
            
            with col1:
                if st.button("ğŸ—‘ï¸ Remove Duplicates"):
                    initial_len = len(combined_df)
                    combined_df = combined_df.drop_duplicates()
                    st.session_state.data = combined_df
                    removed = initial_len - len(combined_df)
                    st.success(f"Removed {removed} duplicates")
                    if removed > 0:
                        st.rerun()
            
            with col2:
                if st.button("ğŸ”§ Fill Missing Values"):
                    combined_df = fill_missing_values(combined_df)
                    st.session_state.data = combined_df
                    st.success("Missing values filled!")
                    st.rerun()
            
            with col3:
                if st.button("ğŸ“Š Basic Statistics"):
                    st.markdown("#### ğŸ“ˆ Descriptive Statistics")
                    numeric_cols = combined_df.select_dtypes(include=[np.number]).columns
                    if len(numeric_cols) > 0:
                        st.dataframe(combined_df[numeric_cols].describe())
                    else:
                        st.info("No numeric columns for analysis")

def show_charts():
    """
    ×”×¦×’×ª ×××©×§ ×•×™×–×•××œ×™×–×¦×™×•×ª × ×ª×•× ×™× ××™× ×˜×¨××§×˜×™×‘×™×•×ª
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - ××ª×Ÿ ××¤×©×¨×•×™×•×ª ×™×¦×™×¨×ª ×ª×¨×©×™××™× ××’×•×•× ×™× ×•××™× ×˜×¨××§×˜×™×‘×™×™×
    - ×ª××™×›×” ×‘×•×™×–×•××œ×™×–×¦×™×•×ª ×“×•-××™××“×™×•×ª ×•×ª×œ×ª-××™××“×™×•×ª
    - ×™×¦×™×¨×ª ×ª×¨×©×™××™ ×”×ª×¤×œ×’×•×ª, ×§×•×¨×œ×¦×™×” ×•× ×™×ª×•×— ×˜×¨× ×“×™×
    - ××¤×©×¨×•×™×•×ª ×”×ª×××” ××™×©×™×ª ×©×œ ×”×ª×¨×©×™××™×
    
    ×¡×•×’×™ ×”×ª×¨×©×™××™× ×”×–××™× ×™×:
    - ×ª×¨×©×™××™ ×¢××•×“×•×ª ×•×¤×™×–×•×¨ (Scatter plots)
    - ×ª×¨×©×™××™ ×§×• ×•×˜×¨× ×“×™× ×–×× ×™×™×
    - ×”×™×¡×˜×•×’×¨××•×ª ×•×”×ª×¤×œ×’×•×™×•×ª
    - ××¤×•×ª ×—×•× ×©×œ ××ª×××™× (Heatmaps)
    - ×ª×¨×©×™××™× ×ª×œ×ª-××™××“×™×™× (3D Scatter)
    - ×ª×¨×©×™××™ Box Plot ×œ×–×™×”×•×™ ×¢×¨×›×™× ×—×¨×™×’×™×
    - ×ª×¨×©×™××™ ×¢×•×’×” (Pie Charts) ×œ××©×ª× ×™× ×§×˜×’×•×¨×™××œ×™×™×
    
    ×¤×™×¦'×¨×™× ××ª×§×“××™×:
    - ××¤×©×¨×•×™×•×ª ×¡×™× ×•×Ÿ ×“×™× ××™×•×ª ×©×œ ×”× ×ª×•× ×™×
    - ×‘×—×™×¨×ª ×¦×‘×¢×™× ×•×¡×’× ×•× ×•×ª ×”×ª×¨×©×™××™×
    - ×™×›×•×œ×•×ª ×–×•× ×•×”×’×“×œ×” ×‘××¦×‘×™× ××™× ×˜×¨××§×˜×™×‘×™×™×
    - ××¤×©×¨×•×™×•×ª ×”×•×¨×“×” ×•×™×™×¦×•× ×”×ª×¨×©×™××™×
    - × ×™×ª×•×— ×˜×¨× ×“×™× ××•×˜×•××˜×™ ×¢× ×”××œ×¦×•×ª
    
    ×”×—×–×¨×”: ×œ×œ× - ×”×¤×•× ×§×¦×™×” ××¦×™×’×” ××ª ×”×××©×§ ×™×©×™×¨×•×ª
    """
    st.markdown("## ğŸ“ˆ Data Visualization")
    
    if 'data' not in st.session_state:
        st.warning("ğŸ“‚ Please load data first!")
        return
    
    df = st.session_state.data
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    text_cols = df.select_dtypes(include=['object', 'string']).columns.tolist()
    datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()
    
    # Chart type selection
    chart_type = st.selectbox(
        "ğŸ“Š Select chart type", 
        ["ğŸ“ˆ Line Chart", "ğŸ“Š Bar Chart", "ğŸ”µ Scatter Plot", "ğŸŒ 3D Scatter Plot",
         "ğŸ“‰ Area Chart", "ğŸ—ºï¸ Heatmap", "ğŸ¥§ Pie Chart", 
         "ğŸ“¦ Box Plot", "ğŸ“Š Histogram", "ğŸ» Violin Plot"]
    )
    
    if chart_type == "ğŸ“ˆ Line Chart" and len(numeric_cols) > 0:
        st.markdown("### ğŸ“ˆ Line Chart")
        
        col1, col2 = st.columns(2)
        with col1:
            y_col = st.selectbox("Y-axis", numeric_cols)
        with col2:
            x_col = st.selectbox("X-axis (optional)", ["Index"] + list(df.columns))
        
        if x_col == "Index":
            fig = px.line(df, y=y_col, title=f"Trend: {y_col}")
        else:
            fig = px.line(df, x=x_col, y=y_col, title=f"{y_col} by {x_col}")
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Insights for line chart
        if len(numeric_cols) > 0:
            trend_analysis = analyze_trend(df[y_col])
            st.info(f"ğŸ“ˆ Trend: {trend_analysis}")
    
    elif chart_type == "ğŸ“Š Bar Chart":
        st.markdown("### ğŸ“Š Bar Chart")
        
        if len(numeric_cols) > 0 and len(text_cols) > 0:
            col1, col2 = st.columns(2)
            with col1:
                cat_col = st.selectbox("Category", text_cols)
            with col2:
                val_col = st.selectbox("Value", numeric_cols)
            
            # Data aggregation
            agg_data = df.groupby(cat_col)[val_col].mean().reset_index()
            fig = px.bar(agg_data, x=cat_col, y=val_col, 
                        title=f"Average {val_col} by {cat_col}")
            st.plotly_chart(fig, use_container_width=True)
            
            # Insights
            top_category = agg_data.loc[agg_data[val_col].idxmax(), cat_col]
            st.success(f"ğŸ† Leading category: {top_category}")
        else:
            st.warning("Need categorical and numeric columns")
    
    elif chart_type == "ğŸŒ 3D Scatter Plot" and len(numeric_cols) >= 3:
        st.markdown("### ğŸŒ 3D Scatter Plot")
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            x_col = st.selectbox("X-axis", numeric_cols, key="3d_x")
        with col2:
            y_col = st.selectbox("Y-axis", [col for col in numeric_cols if col != x_col], key="3d_y")
        with col3:
            z_col = st.selectbox("Z-axis", [col for col in numeric_cols if col not in [x_col, y_col]], key="3d_z")
        with col4:
            color_col = st.selectbox("Color by", ["None"] + text_cols, key="3d_color")
        
        # Create 3D scatter plot
        fig = go.Figure()
        
        if color_col == "None":
            fig.add_trace(go.Scatter3d(
                x=df[x_col],
                y=df[y_col],
                z=df[z_col],
                mode='markers',
                marker=dict(
                    size=5,
                    color=df[z_col],
                    colorscale='Viridis',
                    colorbar=dict(title=z_col),
                    opacity=0.8
                ),
                text=[f'{x_col}: {x}<br>{y_col}: {y}<br>{z_col}: {z}' 
                      for x, y, z in zip(df[x_col], df[y_col], df[z_col])],
                hovertemplate='%{text}<extra></extra>'
            ))
        else:
            for category in df[color_col].unique():
                mask = df[color_col] == category
                fig.add_trace(go.Scatter3d(
                    x=df[mask][x_col],
                    y=df[mask][y_col],
                    z=df[mask][z_col],
                    mode='markers',
                    name=str(category),
                    marker=dict(size=5, opacity=0.8),
                    text=[f'{x_col}: {x}<br>{y_col}: {y}<br>{z_col}: {z}<br>{color_col}: {c}' 
                          for x, y, z, c in zip(df[mask][x_col], df[mask][y_col], 
                                               df[mask][z_col], df[mask][color_col])],
                    hovertemplate='%{text}<extra></extra>'
                ))
        
        fig.update_layout(
            title=f"3D Scatter Plot: {x_col} vs {y_col} vs {z_col}",
            scene=dict(
                xaxis_title=x_col,
                yaxis_title=y_col,
                zaxis_title=z_col,
                camera=dict(
                    eye=dict(x=1.2, y=1.2, z=1.2)
                )
            ),
            width=800,
            height=600
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # 3D correlation analysis
        st.markdown("#### ğŸ”— 3D Correlation Analysis")
        correlations_3d = {
            f"{x_col} - {y_col}": df[x_col].corr(df[y_col]),
            f"{x_col} - {z_col}": df[x_col].corr(df[z_col]),
            f"{y_col} - {z_col}": df[y_col].corr(df[z_col])
        }
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric(f"{x_col} â†” {y_col}", f"{correlations_3d[f'{x_col} - {y_col}']:.3f}")
        with col2:
            st.metric(f"{x_col} â†” {z_col}", f"{correlations_3d[f'{x_col} - {z_col}']:.3f}")
        with col3:
            st.metric(f"{y_col} â†” {z_col}", f"{correlations_3d[f'{y_col} - {z_col}']:.3f}")
        
        # 3D insights
        max_corr_3d = max(abs(corr) for corr in correlations_3d.values())
        if max_corr_3d > 0.7:
            st.success(f"ğŸ”— Strong 3D relationships detected! Max correlation: {max_corr_3d:.3f}")
        elif max_corr_3d > 0.5:
            st.info(f"ğŸ“Š Moderate 3D relationships found. Max correlation: {max_corr_3d:.3f}")
        else:
            st.warning(f"ğŸ“‰ Weak 3D relationships. Max correlation: {max_corr_3d:.3f}")
    
    elif chart_type == "ğŸ”µ Scatter Plot" and len(numeric_cols) >= 2:
        st.markdown("### ğŸ”µ Scatter Plot")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            x_col = st.selectbox("X-axis", numeric_cols)
        with col2:
            y_col = st.selectbox("Y-axis", [col for col in numeric_cols if col != x_col])
        with col3:
            color_col = st.selectbox("Color by", ["None"] + list(text_cols))
        
        if color_col == "None":
            fig = px.scatter(df, x=x_col, y=y_col, title=f"{y_col} vs {x_col}")
        else:
            fig = px.scatter(df, x=x_col, y=y_col, color=color_col, 
                           title=f"{y_col} vs {x_col} (color: {color_col})")
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Correlation analysis
        correlation = df[x_col].corr(df[y_col])
        if abs(correlation) > 0.7:
            st.success(f"ğŸ”— Strong correlation: {correlation:.3f}")
        elif abs(correlation) > 0.3:
            st.info(f"ğŸ“Š Moderate correlation: {correlation:.3f}")
        else:
            st.warning(f"ğŸ“‰ Weak correlation: {correlation:.3f}")
    
    elif chart_type == "ğŸ—ºï¸ Heatmap" and len(numeric_cols) >= 2:
        st.markdown("### ğŸ—ºï¸ Correlation Heatmap")
        
        corr_matrix = df[numeric_cols].corr()
        fig = px.imshow(corr_matrix, 
                       title="Correlation Matrix",
                       color_continuous_scale="RdBu",
                       aspect="auto")
        st.plotly_chart(fig, use_container_width=True)
        
        # Find strong correlations
        strong_corrs = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_val = corr_matrix.iloc[i, j]
                if abs(corr_val) > 0.7:
                    strong_corrs.append(
                        (corr_matrix.columns[i], corr_matrix.columns[j], corr_val)
                    )
        
        if strong_corrs:
            st.markdown("#### ğŸ”— Strong Correlations:")
            for var1, var2, corr in strong_corrs:
                st.write(f"â€¢ {var1} â†” {var2}: {corr:.3f}")
    
    elif chart_type == "ğŸ¥§ Pie Chart" and len(text_cols) > 0:
        st.markdown("### ğŸ¥§ Pie Chart")
        
        cat_col = st.selectbox("Select category", text_cols)
        value_counts = df[cat_col].value_counts().head(10)  # Top-10 categories
        
        fig = px.pie(values=value_counts.values, names=value_counts.index,
                    title=f"Distribution of {cat_col}")
        st.plotly_chart(fig, use_container_width=True)
        
        # Statistics
        st.write(f"ğŸ“Š Total unique values: {df[cat_col].nunique()}")
        dominant_cat = value_counts.index[0]
        dominant_pct = (value_counts.iloc[0] / len(df)) * 100
        st.info(f"ğŸ¯ Dominant category: {dominant_cat} ({dominant_pct:.1f}%)")
    
    elif chart_type == "ğŸ“¦ Box Plot" and len(numeric_cols) > 0:
        st.markdown("### ğŸ“¦ Box Plot")
        
        col1, col2 = st.columns(2)
        with col1:
            num_col = st.selectbox("Numeric column", numeric_cols)
        with col2:
            group_col = st.selectbox("Grouping", ["None"] + list(text_cols))
        
        if group_col == "None":
            fig = px.box(df, y=num_col, title=f"Distribution of {num_col}")
        else:
            fig = px.box(df, x=group_col, y=num_col, 
                        title=f"Distribution of {num_col} by {group_col}")
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Outlier analysis
        Q1 = df[num_col].quantile(0.25)
        Q3 = df[num_col].quantile(0.75)
        IQR = Q3 - Q1
        outliers = df[(df[num_col] < Q1 - 1.5*IQR) | (df[num_col] > Q3 + 1.5*IQR)]
        
        if len(outliers) > 0:
            st.warning(f"âš ï¸ Outliers detected: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)")
        else:
            st.success("âœ… No outliers detected")

    elif chart_type == "ğŸ“Š Histogram" and len(numeric_cols) > 0:
        st.markdown("### ğŸ“Š Histogram")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            hist_col = st.selectbox("Select column", numeric_cols, key="hist_col")
        with col2:
            bins = st.slider("Number of bins", 10, 100, 30)
        with col3:
            show_normal = st.checkbox("Show normal curve", False)
        
        fig = px.histogram(df, x=hist_col, nbins=bins, title=f"Distribution: {hist_col}")
        
        if show_normal:
            # Add normal distribution overlay
            mean_val = df[hist_col].mean()
            std_val = df[hist_col].std()
            x_range = np.linspace(df[hist_col].min(), df[hist_col].max(), 100)
            normal_curve = stats.norm.pdf(x_range, mean_val, std_val)
            
            # Scale normal curve to match histogram
            normal_curve = normal_curve * len(df) * (df[hist_col].max() - df[hist_col].min()) / bins
            
            fig.add_trace(go.Scatter(
                x=x_range,
                y=normal_curve,
                mode='lines',
                name='Normal Distribution',
                line=dict(color='red', width=2)
            ))
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Distribution statistics
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Mean", f"{df[hist_col].mean():.2f}")
        with col2:
            st.metric("Std Dev", f"{df[hist_col].std():.2f}")
        with col3:
            st.metric("Skewness", f"{df[hist_col].skew():.2f}")
        with col4:
            st.metric("Kurtosis", f"{df[hist_col].kurtosis():.2f}")
    
    elif chart_type == "ğŸ» Violin Plot" and len(numeric_cols) > 0:
        st.markdown("### ğŸ» Violin Plot")
        
        col1, col2 = st.columns(2)
        with col1:
            violin_col = st.selectbox("Numeric column", numeric_cols, key="violin_col")
        with col2:
            group_by = st.selectbox("Group by", ["None"] + text_cols, key="violin_group")
        
        if group_by == "None":
            fig = go.Figure()
            fig.add_trace(go.Violin(
                y=df[violin_col],
                name=violin_col,
                box_visible=True,
                meanline_visible=True
            ))
            fig.update_layout(title=f"Violin Plot: {violin_col}")
        else:
            fig = px.violin(df, y=violin_col, x=group_by, box=True,
                          title=f"Violin Plot: {violin_col} by {group_by}")
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Violin plot insights
        if group_by != "None":
            st.markdown("#### ğŸ“Š Group Comparison")
            group_stats = df.groupby(group_by)[violin_col].agg(['mean', 'std', 'median']).round(2)
            st.dataframe(group_stats)
    
    elif chart_type == "ğŸ“‰ Area Chart" and len(numeric_cols) > 0:
        st.markdown("### ğŸ“‰ Area Chart")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            area_cols = st.multiselect("Select columns", numeric_cols, default=numeric_cols[:min(3, len(numeric_cols))])
        with col2:
            cumulative = st.checkbox("Cumulative", False)
        with col3:
            normalize = st.checkbox("Normalize (0-1)", False)
        
        if area_cols:
            df_area = df[area_cols].copy()
            
            if normalize:
                df_area = (df_area - df_area.min()) / (df_area.max() - df_area.min())
            
            if cumulative:
                df_area = df_area.cumsum()
            
            df_area['index'] = range(len(df_area))
            
            fig = px.area(df_area, x='index', y=area_cols,
                         title="Area Chart" + (" (Cumulative)" if cumulative else "") + (" (Normalized)" if normalize else ""))
            st.plotly_chart(fig, use_container_width=True)
    
    else:
        if len(numeric_cols) < 2:
            st.warning("âš ï¸ Need at least 2 numeric columns for this chart type")
        elif len(text_cols) == 0 and chart_type in ["ğŸ“Š Bar Chart", "ğŸ¥§ Pie Chart"]:
            st.warning("âš ï¸ Need categorical columns for this chart type")
        else:
            st.info("ğŸ“Š Select appropriate data types for the chosen chart")

def analyze_trend(series):
    """
    × ×™×ª×•×— ×˜×¨× ×“×™× ××ª×§×“× ×©×œ ×¡×“×¨×ª × ×ª×•× ×™×
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - ×‘×™×¦×•×¢ × ×™×ª×•×— ×¨×’×¨×¡×™×” ×œ×™× ××¨×™×ª ×œ×–×™×”×•×™ ×›×™×•×•×Ÿ ×”×˜×¨× ×“
    - ×—×™×©×•×‘ ×¢×•×¦××ª ×”×˜×¨× ×“ ×‘×××¦×¢×•×ª ××§×“× ×”××ª××
    - ×”×¢×¨×›×ª ×©×™×¢×•×¨ ×”×©×™× ×•×™ ×”××—×•×–×™ ×‘×¡×“×¨×”
    - ××ª×Ÿ ×ª×™××•×¨ ××™×œ×•×œ×™ ××¤×•×¨×˜ ×©×œ ×”×˜×¨× ×“
    
    ×¤×¨××˜×¨×™×:
        series (pandas.Series): ×¡×“×¨×ª ×”× ×ª×•× ×™× ×œ× ×™×ª×•×—
    
    ×”×—×–×¨×”:
        str: ×ª×™××•×¨ ××¤×•×¨×˜ ×©×œ ×”×˜×¨× ×“ ×›×•×œ×œ ×›×™×•×•×Ÿ, ×¢×•×¦××” ×•×©×™×¢×•×¨ ×©×™× ×•×™
        
    ×“×•×’×××•×ª ×œ×ª×•×¦××•×ª:
    - "Strong upward trend (RÂ²=0.891, +15.3% change)"
    - "Weak downward trend (RÂ²=0.234, -3.2% change)"
    """
    if len(series) < 2:
        return "Insufficient data"
    
    # Remove NaN values
    clean_series = series.dropna()
    if len(clean_series) < 2:
        return "Insufficient clean data"
    
    # Linear regression trend
    x = np.arange(len(clean_series))
    z = np.polyfit(x, clean_series, 1)
    slope = z[0]
    
    # Calculate trend strength
    correlation = np.corrcoef(x, clean_series)[0, 1]
    
    # Determine trend direction and strength
    if abs(correlation) > 0.7:
        strength = "strong"
    elif abs(correlation) > 0.4:
        strength = "moderate"
    else:
        strength = "weak"
    
    if slope > 0:
        direction = "upward"
    elif slope < 0:
        direction = "downward"
    else:
        direction = "flat"
    
    # Calculate percentage change
    first_val = clean_series.iloc[0]
    last_val = clean_series.iloc[-1]
    pct_change = ((last_val - first_val) / first_val) * 100 if first_val != 0 else 0
    
    return f"{strength.title()} {direction} trend (RÂ²={correlation**2:.3f}, {pct_change:+.1f}% change)"

def analyze_trend(series):
    """
    × ×™×ª×•×— ×˜×¨× ×“×™× ×¤×©×•×˜ ×©×œ ×¡×“×¨×ª × ×ª×•× ×™×
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - ×”×©×•×•××” ×‘×™×Ÿ ×”×—×¦×™ ×”×¨××©×•×Ÿ ×œ×—×¦×™ ×”×©× ×™ ×©×œ ×”×¡×“×¨×”
    - ×—×™×©×•×‘ ×©×™×¢×•×¨ ×”×©×™× ×•×™ ×‘×™×Ÿ ×”×—×œ×§×™×
    - ×§×‘×™×¢×ª ×›×™×•×•×Ÿ ×”×˜×¨× ×“ ×¢×œ ×‘×¡×™×¡ ×”×©×•×•××” ×–×•
    
    ×¤×¨××˜×¨×™×:
        series (pandas.Series): ×¡×“×¨×ª ×”× ×ª×•× ×™× ×œ× ×™×ª×•×—
    
    ×”×—×–×¨×”:
        str: ×ª×™××•×¨ ×§×¦×¨ ×©×œ ×”×˜×¨× ×“
        
    ×“×•×’×××•×ª ×œ×ª×•×¦××•×ª:
    - "Growing trend (+8.5%)"
    - "Declining trend (-12.3%)"
    - "Stable trend (+2.1%)"
    """
    if len(series) < 2:
        return "Insufficient data"
    
    # Simple trend analysis
    first_half = series[:len(series)//2].mean()
    second_half = series[len(series)//2:].mean()
    
    change_pct = ((second_half - first_half) / first_half) * 100
    
    if change_pct > 5:
        return f"Growing trend (+{change_pct:.1f}%)"
    elif change_pct < -5:
        return f"Declining trend ({change_pct:.1f}%)"
    else:
        return f"Stable trend ({change_pct:.1f}%)"

def show_stats():
    """
    ×”×¦×’×ª ×××©×§ × ×™×ª×•×— ×¡×˜×˜×™×¡×˜×™ ××ª×§×“× ×©×œ ×”× ×ª×•× ×™×
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - ××ª×Ÿ ×›×œ×™× ××§×™×¤×™× ×œ× ×™×ª×•×— ×¡×˜×˜×™×¡×˜×™ ×©×œ ××¡×’×¨×ª ×”× ×ª×•× ×™×
    - ×”×¦×’×ª ×¡×˜×˜×™×¡×˜×™×§×•×ª ×ª×™××•×¨×™×•×ª ×•×—×™×¤×•×© ×“×¤×•×¡×™×
    - ×‘×™×¦×•×¢ ×‘×“×™×§×•×ª ×”×©×¢×¨×•×ª ×¡×˜×˜×™×¡×˜×™×•×ª
    - ×–×™×”×•×™ ×¢×¨×›×™× ×—×¨×™×’×™× ×•×× ×•××œ×™×•×ª ×‘× ×ª×•× ×™×
    
    ×™×›×•×œ×•×ª ×”× ×™×ª×•×— ×”×¡×˜×˜×™×¡×˜×™:
    - ×¡×˜×˜×™×¡×˜×™×§×•×ª ×ª×™××•×¨×™×•×ª: ×××•×¦×¢, ×—×¦×™×•×Ÿ, ×¡×˜×™×™×ª ×ª×§×Ÿ
    - ××“×“×™ ×¤×™×–×•×¨ ×•×¦×•×¨×ª ×”×ª×¤×œ×’×•×ª: ××¡×™××˜×¨×™×” (skewness) ×•×—×“×•×ª (kurtosis) 
    - × ×™×ª×•×— ××ª×××™× ×‘×™×Ÿ ××©×ª× ×™× ×¢× ××˜×¨×™×¦×ª ×§×•×¨×œ×¦×™×”
    - ×‘×“×™×§×•×ª × ×•×¨××œ×™×•×ª ×©×œ ×”×”×ª×¤×œ×’×•×™×•×ª
    - ×–×™×”×•×™ ×¢×¨×›×™× ×—×¨×™×’×™× ×‘×©×™×˜×•×ª ××ª×§×“××•×ª
    - × ×™×ª×•×— ×©×•× ×•×ª (ANOVA) ×œ×§×‘×•×¦×•×ª ×©×•× ×•×ª
    - ×‘×“×™×§×•×ª t-test ×œ××©×ª× ×™× ×¨×¦×™×¤×™×
    - ×‘×“×™×§×•×ª chi-square ×œ××©×ª× ×™× ×§×˜×’×•×¨×™××œ×™×™×
    
    ×¤×™×¦'×¨×™× × ×•×¡×¤×™×:
    - ×•×™×–×•××œ×™×–×¦×™×” ×©×œ ×”×ª×¤×œ×’×•×™×•×ª
    - ×“×•×—×•×ª ×¡×˜×˜×™×¡×˜×™×™× ××¤×•×¨×˜×™×
    - ×”××œ×¦×•×ª ×¢×œ ×‘×“×™×§×•×ª ×¡×˜×˜×™×¡×˜×™×•×ª × ×•×¡×¤×•×ª
    
    ×”×—×–×¨×”: ×œ×œ× - ×”×¤×•× ×§×¦×™×” ××¦×™×’×” ××ª ×”×××©×§ ×™×©×™×¨×•×ª
    """
    st.markdown("## ğŸ“Š Statistical Analysis")
    
    if 'data' not in st.session_state:
        st.warning("ğŸ“‚ Please load data first!")
        return
    
    df = st.session_state.data
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    if len(numeric_cols) == 0:
        st.warning("ğŸ”¢ No numeric columns in data for analysis")
        return
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.markdown("### ğŸ“ˆ Descriptive Statistics")
        stats_df = df[numeric_cols].describe()
        st.dataframe(stats_df)
        
        # Distributions
        st.markdown("### ğŸ“Š Distribution Analysis")
        selected_col = st.selectbox("Select column", numeric_cols)
        
        col_a, col_b = st.columns(2)
        
        with col_a:
            # Histogram
            fig_hist = px.histogram(df, x=selected_col, title=f"Histogram: {selected_col}",
                                  nbins=30)
            st.plotly_chart(fig_hist, use_container_width=True)
        
        with col_b:
            # Q-Q plot for normality testing
            from scipy import stats as scipy_stats
            data_clean = df[selected_col].dropna()
            
            fig_qq = go.Figure()
            
            # Theoretical quantiles of normal distribution
            theoretical_quantiles = scipy_stats.norm.ppf(np.linspace(0.01, 0.99, len(data_clean)))
            sample_quantiles = np.sort(data_clean)
            
            fig_qq.add_trace(go.Scatter(
                x=theoretical_quantiles,
                y=sample_quantiles,
                mode='markers',
                name='Data'
            ))
            
            # Normal distribution line
            fig_qq.add_trace(go.Scatter(
                x=theoretical_quantiles,
                y=theoretical_quantiles * data_clean.std() + data_clean.mean(),
                mode='lines',
                name='Normal Distribution',
                line=dict(color='red', dash='dash')
            ))
            
            fig_qq.update_layout(title=f"Q-Q Plot: {selected_col}",
                               xaxis_title="Theoretical Quantiles",
                               yaxis_title="Sample Quantiles")
            st.plotly_chart(fig_qq, use_container_width=True)
    
    with col2:
        st.markdown("### ğŸ§ª Statistical Tests")
        
        # Normality test
        if st.button("ğŸ”¬ Normality Test"):
            data_sample = df[selected_col].dropna().sample(min(5000, len(df[selected_col].dropna())))
            stat, p_value = stats.shapiro(data_sample)
            
            st.metric("Test Statistic", f"{stat:.4f}")
            st.metric("P-value", f"{p_value:.4f}")
            
            if p_value > 0.05:
                st.success("âœ… Data follows normal distribution")
            else:
                st.warning("âŒ Data does not follow normal distribution")
        
        # Correlation analysis
        if len(numeric_cols) >= 2:
            st.markdown("#### ğŸ”— Correlation Analysis")
            
            col1_test = st.selectbox("Variable 1", numeric_cols, key="corr1")
            col2_test = st.selectbox("Variable 2", 
                                   [col for col in numeric_cols if col != col1_test], 
                                   key="corr2")
            
            if st.button("ğŸ“Š Analyze Correlation"):
                # Data cleaning
                data1 = df[col1_test].dropna()
                data2 = df[col2_test].dropna()
                common_idx = data1.index.intersection(data2.index)
                data1 = data1[common_idx]
                data2 = data2[common_idx]
                
                # Pearson correlation
                corr_pearson, p_pearson = stats.pearsonr(data1, data2)
                
                # Spearman correlation
                corr_spearman, p_spearman = stats.spearmanr(data1, data2)
                
                st.metric("Pearson Correlation", f"{corr_pearson:.3f}")
                st.metric("P-value (Pearson)", f"{p_pearson:.4f}")
                st.metric("Spearman Correlation", f"{corr_spearman:.3f}")
                st.metric("P-value (Spearman)", f"{p_spearman:.4f}")
                
                # Interpretation
                if abs(corr_pearson) > 0.8:
                    st.success("ğŸ”— Very strong correlation!")
                elif abs(corr_pearson) > 0.6:
                    st.info("ğŸ“ˆ Strong correlation")
                elif abs(corr_pearson) > 0.3:
                    st.warning("ğŸ“Š Moderate correlation")
                else:
                    st.error("ğŸ“‰ Weak correlation")
        
        # Outlier analysis
        st.markdown("#### ğŸ¯ Outlier Analysis")
        if st.button("ğŸ” Find Outliers"):
            outliers_info = detect_outliers_advanced(df[selected_col])
            
            st.write("**Detection Methods:**")
            for method, data in outliers_info.items():
                st.write(f"â€¢ {method}: {data['count']} outliers ({data['percentage']:.1f}%)")

def detect_outliers_advanced(series):
    """
    ×–×™×”×•×™ ×¢×¨×›×™× ×—×¨×™×’×™× ××ª×§×“× ×‘×××¦×¢×•×ª ××¡×¤×¨ ×©×™×˜×•×ª
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - ×–×™×”×•×™ ×¢×¨×›×™× ×—×¨×™×’×™× ×‘×¡×“×¨×ª × ×ª×•× ×™× ×‘×©×™×˜×•×ª ×¡×˜×˜×™×¡×˜×™×•×ª ××’×•×•× ×•×ª
    - ×”×©×•×•××” ×‘×™×Ÿ ×©×™×˜×•×ª ×–×™×”×•×™ ×©×•× ×•×ª ×œ×§×‘×œ×ª ×ª××•× ×” ××§×™×¤×”
    - ×”×¢×¨×›×ª ×¨××ª ×”×—×•××¨×” ×©×œ ×”×¢×¨×›×™× ×”×—×¨×™×’×™×
    - ××ª×Ÿ ×”××œ×¦×•×ª ×œ×˜×™×¤×•×œ ×‘×¢×¨×›×™× ×—×¨×™×’×™×
    
    ×©×™×˜×•×ª ×–×™×”×•×™ ×”××™×•×©××•×ª:
    - ×©×™×˜×ª IQR (Interquartile Range) - ×”×§×œ××¡×™×ª
    - ×©×™×˜×ª Z-Score - ×‘×”×ª×‘×¡×¡ ×¢×œ ×¡×˜×™×™×ª ×ª×§×Ÿ
    - ×©×™×˜×ª Modified Z-Score - ×¢××™×“×” ×™×•×ª×¨ ×œ×¢×¨×›×™× ×—×¨×™×’×™×
    - × ×™×ª×•×— ××—×•×–×•× ×™× - ×–×™×”×•×™ ×¢×¨×›×™× ×§×™×¦×•× ×™×™×
    
    ×¤×¨××˜×¨×™×:
        series (pandas.Series): ×¡×“×¨×ª ×”× ×ª×•× ×™× ×œ×‘×“×™×§×”
    
    ×”×—×–×¨×”:
        dict: ××™×œ×•×Ÿ ×¢× ×ª×•×¦××•×ª ×›×œ ×©×™×˜×•×ª ×”×–×™×”×•×™:
            - 'iqr_outliers': ×¨×©×™××ª ×¢×¨×›×™× ×—×¨×™×’×™× ×‘×©×™×˜×ª IQR
            - 'z_score_outliers': ×¨×©×™××ª ×¢×¨×›×™× ×—×¨×™×’×™× ×‘Z-Score
            - 'modified_z_outliers': ×¨×©×™××ª ×¢×¨×›×™× ×—×¨×™×’×™× ×‘Modified Z-Score
            - 'summary': ×¡×™×›×•× ×›××•×ª×™ ×©×œ ×”×¢×¨×›×™× ×”×—×¨×™×’×™×
    """
    
    results = {}
    clean_data = series.dropna()
    
    # IQR method
    Q1 = clean_data.quantile(0.25)
    Q3 = clean_data.quantile(0.75)
    IQR = Q3 - Q1
    iqr_outliers = clean_data[(clean_data < Q1 - 1.5*IQR) | (clean_data > Q3 + 1.5*IQR)]
    
    results['IQR'] = {
        'count': len(iqr_outliers),
        'percentage': len(iqr_outliers) / len(clean_data) * 100
    }
    
    # Z-score method
    z_scores = np.abs((clean_data - clean_data.mean()) / clean_data.std())
    z_outliers = clean_data[z_scores > 3]
    
    results['Z-Score'] = {
        'count': len(z_outliers),
        'percentage': len(z_outliers) / len(clean_data) * 100
    }
    
    # Modified Z-score (median)
    median = clean_data.median()
    mad = np.median(np.abs(clean_data - median))
    modified_z_scores = 0.6745 * (clean_data - median) / mad
    modified_z_outliers = clean_data[np.abs(modified_z_scores) > 3.5]
    
    results['Modified Z-Score'] = {
        'count': len(modified_z_outliers),
        'percentage': len(modified_z_outliers) / len(clean_data) * 100
    }
    
    return results

def show_ml():
    """
    ×”×¦×’×ª ×××©×§ ×œ××™×“×ª ××›×•× ×” ×¢× ××œ×’×•×¨×™×ª××™× ××ª×§×“××™×
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - ××ª×Ÿ ×›×œ×™× ×œ××™×“×ª ××›×•× ×” ×‘×¡×™×¡×™×™× ×•××ª×§×“××™× ×œ× ×™×ª×•×— × ×ª×•× ×™×
    - ×™×™×©×•× ××œ×’×•×¨×™×ª××™ ×§×œ××¡×˜×¨×™× ×’, ×”×¤×—×ª×ª ××™××“×™× ×•×× ×•××œ×™×”
    - ×•×™×–×•××œ×™×–×¦×™×” ××™× ×˜×¨××§×˜×™×‘×™×ª ×©×œ ×ª×•×¦××•×ª ××œ×’×•×¨×™×ª××™ ×œ××™×“×ª ××›×•× ×”
    - ×”×¢×¨×›×” ×•×‘×™×¦×•×¢×™× ×©×œ ××•×“×œ×™× ×©×•× ×™×
    
    ××œ×’×•×¨×™×ª××™ ×œ××™×“×ª ×”××›×•× ×” ×”×–××™× ×™×:
    
    1. K-Means Clustering:
       - ×§×™×‘×•×¥ × ×ª×•× ×™× ×œ×§×‘×•×¦×•×ª ×”×•××•×’× ×™×•×ª
       - ×‘×—×™×¨×” ××•×˜×•××˜×™×ª ××• ×™×“× ×™×ª ×©×œ ××¡×¤×¨ ×§×‘×•×¦×•×ª
       - ×•×™×–×•××œ×™×–×¦×™×” ×©×œ ×”×§×œ××¡×˜×¨×™× ×‘×××“×™× ×©×•× ×™×
       - ×”×¢×¨×›×ª ××™×›×•×ª ×”×§×œ××¡×˜×¨×™× ×’ ×¢× Silhouette Score
    
    2. PCA (Principal Component Analysis):
       - ×”×¤×—×ª×ª ××™××“×™× ×ª×•×š ×©××™×¨×” ×¢×œ ××™×“×¢ ××§×¡×™××œ×™
       - ×”×¦×’×ª ××—×•×– ×”×©×•× ×•×ª ×”××•×¡×‘×¨×ª ×‘×›×œ ×¨×›×™×‘
       - ×•×™×–×•××œ×™×–×¦×™×” ×“×•-××™××“×™×ª ×•×ª×œ×ª-××™××“×™×ª ×©×œ ×”× ×ª×•× ×™×
       - × ×™×ª×•×— ×ª×¨×•××ª ×”××©×ª× ×™× ×”××§×•×¨×™×™× ×œ×¨×›×™×‘×™×
    
    3. Anomaly Detection:
       - ×–×™×”×•×™ × ×§×•×“×•×ª ×—×¨×™×’×•×ª ×‘× ×ª×•× ×™×
       - ×©×™××•×© ×‘××œ×’×•×¨×™×ª× Isolation Forest
       - ×”×“×’×©×” ×•×™×–×•××œ×™×ª ×©×œ ×”× ×§×•×“×•×ª ×”×—×¨×™×’×•×ª
       - × ×™×ª×•×— ×××¤×™×™× ×™ ×”× ×§×•×“×•×ª ×”×—×¨×™×’×•×ª
    
    4. Feature Importance Analysis:
       - ×”×¢×¨×›×ª ×—×©×™×‘×•×ª ×”××©×ª× ×™× ×”×©×•× ×™×
       - ×•×™×–×•××œ×™×–×¦×™×” ×©×œ ×”×“×™×¨×•×’ ×”×—×©×™×‘×•×ª
       - ×”××œ×¦×•×ª ×¢×œ ××©×ª× ×™× ×œ×©××™×¨×” ××• ×”×¡×¨×”
    
    ×¤×™×¦'×¨×™× × ×•×¡×¤×™×:
    - ××¤×©×¨×•×™×•×ª ×”×ª×××” ×©×œ ×¤×¨××˜×¨×™ ×”××œ×’×•×¨×™×ª××™×
    - ×”×©×•×•××” ×‘×™×Ÿ ×ª×•×¦××•×ª ×©×™×˜×•×ª ×©×•× ×•×ª
    - ×™×™×¦×•× ×ª×•×¦××•×ª ×•××•×“×œ×™× ×××•×× ×™×
    - ×”××œ×¦×•×ª ×¢×œ ×”×©×™××•×© ×”××™×˜×‘×™ ×‘×›×œ ××œ×’×•×¨×™×ª×
    
    ×”×—×–×¨×”: ×œ×œ× - ×”×¤×•× ×§×¦×™×” ××¦×™×’×” ××ª ×”×××©×§ ×™×©×™×¨×•×ª
    """
    st.markdown("## ğŸ¤– Machine Learning")
    
    if 'data' not in st.session_state:
        st.warning("ğŸ“‚ Please load data first!")
        return
    
    df = st.session_state.data
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    if len(numeric_cols) < 2:
        st.warning("ğŸ”¢ Need at least 2 numeric columns for ML analysis!")
        return
    
    ml_type = st.selectbox(
        "ğŸ¤– Select analysis type",
        ["ğŸ¯ Clustering", "ğŸ“‰ PCA Analysis", "ğŸ” Anomaly Detection", "ğŸ“Š Feature Importance"]
    )
    
    if ml_type == "ğŸ¯ Clustering":
        st.markdown("### ğŸ¯ K-Means Clustering")
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            selected_features = st.multiselect(
                "Select features", 
                numeric_cols, 
                default=numeric_cols[:min(3, len(numeric_cols))]
            )
            
            n_clusters = st.slider("Number of clusters", 2, 10, 3)
        
        with col2:
            st.markdown("#### âš™ï¸ Settings")
            scale_data = st.checkbox("Normalize data", True)
            random_state = st.number_input("Random State", 0, 1000, 42)
        
        if len(selected_features) >= 2 and st.button("ğŸš€ Run Clustering"):
            with st.spinner("Performing clustering..."):
                # Data preparation
                X = df[selected_features].dropna()
                
                if scale_data:
                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(X)
                else:
                    X_scaled = X.values
                
                # Clustering
                kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)
                clusters = kmeans.fit_predict(X_scaled)
                
                # Add clusters to data
                df_clustered = X.copy()
                df_clustered['Cluster'] = clusters
                
                # Visualization
                if len(selected_features) >= 2:
                    fig = px.scatter(
                        df_clustered, 
                        x=selected_features[0], 
                        y=selected_features[1],
                        color='Cluster',
                        title="K-Means Clustering Results",
                        color_discrete_sequence=px.colors.qualitative.Set1
                    )
                    
                    # Add centroids
                    if scale_data:
                        centroids_original = scaler.inverse_transform(kmeans.cluster_centers_)
                    else:
                        centroids_original = kmeans.cluster_centers_
                    
                    fig.add_trace(go.Scatter(
                        x=centroids_original[:, 0],
                        y=centroids_original[:, 1],
                        mode='markers',
                        marker=dict(symbol='x', size=15, color='black'),
                        name='Centroids'
                    ))
                    
                    st.plotly_chart(fig, use_container_width=True)
                
                # Cluster statistics
                st.markdown("### ğŸ“Š Cluster Statistics")
                cluster_stats = df_clustered.groupby('Cluster')[selected_features].agg(['mean', 'count'])
                st.dataframe(cluster_stats)
                
                # Cluster analysis
                st.markdown("### ğŸ’¡ Cluster Analysis")
                for i in range(n_clusters):
                    cluster_data = df_clustered[df_clustered['Cluster'] == i]
                    cluster_size = len(cluster_data)
                    cluster_pct = (cluster_size / len(df_clustered)) * 100
                    
                    st.write(f"**Cluster {i}**: {cluster_size} points ({cluster_pct:.1f}%)")
                    
                    # Cluster characteristics
                    for feature in selected_features[:3]:  # Show first 3 features
                        mean_val = cluster_data[feature].mean()
                        overall_mean = df_clustered[feature].mean()
                        diff_pct = ((mean_val - overall_mean) / overall_mean) * 100
                        
                        if abs(diff_pct) > 10:
                            if diff_pct > 0:
                                st.write(f"  â€¢ {feature}: above average by {diff_pct:.1f}%")
                            else:
                                st.write(f"  â€¢ {feature}: below average by {abs(diff_pct):.1f}%")
                
                # Clustering quality metric
                from sklearn.metrics import silhouette_score
                silhouette_avg = silhouette_score(X_scaled, clusters)
                st.metric("Silhouette Score", f"{silhouette_avg:.3f}")
                
                if silhouette_avg > 0.7:
                    st.success("ğŸ‰ Excellent clustering quality!")
                elif silhouette_avg > 0.5:
                    st.info("ğŸ‘ Good clustering quality")
                elif silhouette_avg > 0.3:
                    st.warning("âš ï¸ Satisfactory quality")
                else:
                    st.error("âŒ Poor clustering quality")
    
    elif ml_type == "ğŸ“‰ PCA Analysis":
        st.markdown("### ğŸ“‰ Principal Component Analysis (PCA)")
        
        selected_features = st.multiselect(
            "Select features", 
            numeric_cols, 
            default=numeric_cols
        )
        
        if len(selected_features) >= 2 and st.button("ğŸ” Perform PCA"):
            with st.spinner("Performing PCA analysis..."):
                # Data preparation
                X = df[selected_features].dropna()
                
                # Normalization
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X)
                
                # PCA
                pca = PCA()
                X_pca = pca.fit_transform(X_scaled)
                
                # Explained variance
                explained_variance = pca.explained_variance_ratio_
                cumulative_variance = np.cumsum(explained_variance)
                
                col1, col2 = st.columns(2)
                
                with col1:
                    # Scree plot
                    fig_scree = px.bar(
                        x=range(1, len(explained_variance) + 1),
                        y=explained_variance,
                        title="Explained Variance by Component",
                        labels={'x': 'Principal Component', 'y': 'Explained Variance'}
                    )
                    st.plotly_chart(fig_scree, use_container_width=True)
                
                with col2:
                    # Cumulative explained variance
                    fig_cum = px.line(
                        x=range(1, len(cumulative_variance) + 1),
                        y=cumulative_variance,
                        title="Cumulative Explained Variance",
                        labels={'x': 'Number of Components', 'y': 'Cumulative Variance'}
                    )
                    fig_cum.add_hline(y=0.95, line_dash="dash", line_color="red", 
                                     annotation_text="95% variance")
                    st.plotly_chart(fig_cum, use_container_width=True)
                
                # PCA scatter plot (first 2 components)
                if len(X_pca) > 0:
                    pca_df = pd.DataFrame({
                        'PC1': X_pca[:, 0],
                        'PC2': X_pca[:, 1] if X_pca.shape[1] > 1 else np.zeros(len(X_pca))
                    })
                    
                    fig_scatter = px.scatter(
                        pca_df, x='PC1', y='PC2',
                        title=f"PCA: first 2 components (explain {cumulative_variance[1]*100:.1f}% variance)"
                    )
                    st.plotly_chart(fig_scatter, use_container_width=True)
                
                # Feature importance for first components
                st.markdown("### ğŸ“Š Feature Contribution to Principal Components")
                
                components_df = pd.DataFrame(
                    pca.components_[:min(3, len(pca.components_))].T,
                    columns=[f'PC{i+1}' for i in range(min(3, len(pca.components_)))],
                    index=selected_features
                )
                
                fig_components = px.bar(
                    components_df.reset_index().melt(id_vars='index'),
                    x='index', y='value', color='variable',
                    title="Feature Contribution to Principal Components",
                    labels={'index': 'Features', 'value': 'Contribution', 'variable': 'Component'}
                )
                st.plotly_chart(fig_components, use_container_width=True)
                
                # Recommendations
                components_95 = np.where(cumulative_variance >= 0.95)[0]
                if len(components_95) > 0:
                    n_components_95 = components_95[0] + 1
                    st.info(f"ğŸ’¡ To explain 95% variance, {n_components_95} components out of {len(selected_features)} are sufficient")
                    
                    reduction_pct = (1 - n_components_95/len(selected_features)) * 100
                    st.success(f"ğŸ¯ Possible dimensionality reduction by {reduction_pct:.1f}%")
    
    elif ml_type == "ğŸ” Anomaly Detection":
        st.markdown("### ğŸ” Anomaly Detection")
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            selected_features = st.multiselect(
                "Select features", 
                numeric_cols, 
                default=numeric_cols[:min(3, len(numeric_cols))]
            )
        
        with col2:
            method = st.selectbox("Method", ["Isolation Forest", "Local Outlier Factor", "One-Class SVM"])
            contamination = st.slider("Anomaly fraction", 0.01, 0.3, 0.1)
        
        if len(selected_features) >= 1 and st.button("ğŸ” Find Anomalies"):
            with st.spinner("Searching for anomalies..."):
                # Data preparation
                X = df[selected_features].dropna()
                
                # Normalization
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X)
                
                # Anomaly detection
                if method == "Isolation Forest":
                    from sklearn.ensemble import IsolationForest
                    detector = IsolationForest(contamination=contamination, random_state=42)
                elif method == "Local Outlier Factor":
                    from sklearn.neighbors import LocalOutlierFactor
                    detector = LocalOutlierFactor(contamination=contamination)
                elif method == "One-Class SVM":
                    from sklearn.svm import OneClassSVM
                    detector = OneClassSVM(gamma='scale', nu=contamination)
                
                if method == "Local Outlier Factor":
                    anomaly_labels = detector.fit_predict(X_scaled)
                else:
                    anomaly_labels = detector.fit_predict(X_scaled)
                
                # Create DataFrame with results
                results_df = X.copy()
                results_df['Anomaly'] = anomaly_labels == -1
                results_df['Type'] = results_df['Anomaly'].map({True: 'Anomaly', False: 'Normal'})
                
                # Visualization
                if len(selected_features) >= 2:
                    fig = px.scatter(
                        results_df,
                        x=selected_features[0],
                        y=selected_features[1],
                        color='Type',
                        title=f"Anomaly Detection: {method}",
                        color_discrete_map={'Normal': 'blue', 'Anomaly': 'red'}
                    )
                    st.plotly_chart(fig, use_container_width=True)
                
                # Statistics
                n_anomalies = sum(anomaly_labels == -1)
                anomaly_pct = (n_anomalies / len(X)) * 100
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Total points", len(X))
                with col2:
                    st.metric("Anomalies found", n_anomalies)
                with col3:
                    st.metric("Anomaly percentage", f"{anomaly_pct:.2f}%")
                
                # Show top anomalies
                if n_anomalies > 0:
                    st.markdown("### ğŸš¨ Top-10 Anomalies")
                    anomalies = results_df[results_df['Anomaly']].head(10)
                    st.dataframe(anomalies[selected_features])
                    
                    # Anomaly analysis
                    st.markdown("### ğŸ“Š Anomaly Characteristics")
                    for feature in selected_features:
                        normal_mean = results_df[~results_df['Anomaly']][feature].mean()
                        anomaly_mean = results_df[results_df['Anomaly']][feature].mean()
                        
                        if not pd.isna(anomaly_mean) and not pd.isna(normal_mean):
                            diff_pct = ((anomaly_mean - normal_mean) / normal_mean) * 100
                            if abs(diff_pct) > 5:
                                direction = "higher" if diff_pct > 0 else "lower"
                                st.write(f"â€¢ **{feature}**: anomalies on average {direction} by {abs(diff_pct):.1f}%")
    
    elif ml_type == "ğŸ“Š Feature Importance":
        st.markdown("### ğŸ“Š Feature Importance Analysis")
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # Feature selection
            selected_features = st.multiselect(
                "Select features for analysis", 
                numeric_cols, 
                default=numeric_cols[:min(5, len(numeric_cols))]
            )
            
            # Target variable selection
            target_col = st.selectbox(
                "Select target variable",
                numeric_cols,
                help="Variable to predict using other features"
            )
        
        with col2:
            st.markdown("#### âš™ï¸ Settings")
            algorithm = st.selectbox(
                "Algorithm", 
                ["Random Forest", "Gradient Boosting", "Extra Trees"],
                help="Machine learning algorithm for feature importance"
            )
            n_estimators = st.slider("Number of trees", 10, 200, 100)
            random_state = st.number_input("Random State", 0, 1000, 42)
        
        if len(selected_features) >= 2 and target_col and target_col not in selected_features:
            if st.button("ğŸ“Š Calculate Feature Importance"):
                with st.spinner("Calculating feature importance..."):
                    # Data preparation
                    feature_data = df[selected_features + [target_col]].dropna()
                    
                    if len(feature_data) < 10:
                        st.warning("âš ï¸ Not enough data for reliable feature importance analysis.")
                        return
                    
                    X = feature_data[selected_features]
                    y = feature_data[target_col]
                    
                    # Choose algorithm
                    if algorithm == "Random Forest":
                        from sklearn.ensemble import RandomForestRegressor
                        model = RandomForestRegressor(
                            n_estimators=n_estimators, 
                            random_state=random_state,
                            n_jobs=-1
                        )
                    elif algorithm == "Gradient Boosting":
                        from sklearn.ensemble import GradientBoostingRegressor
                        model = GradientBoostingRegressor(
                            n_estimators=n_estimators, 
                            random_state=random_state
                        )
                    else:  # Extra Trees
                        from sklearn.ensemble import ExtraTreesRegressor
                        model = ExtraTreesRegressor(
                            n_estimators=n_estimators, 
                            random_state=random_state,
                            n_jobs=-1
                        )
                    
                    # Fit model
                    model.fit(X, y)
                    
                    # Get feature importance
                    importance_scores = model.feature_importances_
                    feature_importance_df = pd.DataFrame({
                        'Feature': selected_features,
                        'Importance': importance_scores,
                        'Importance_Pct': (importance_scores / importance_scores.sum()) * 100
                    }).sort_values('Importance', ascending=False)
                    
                    # Model performance
                    train_score = model.score(X, y)
                    
                    # Display results
                    col1, col2 = st.columns([1, 1])
                    
                    with col1:
                        st.markdown("### ğŸ“Š Feature Importance Ranking")
                        st.dataframe(
                            feature_importance_df,
                            use_container_width=True,
                            hide_index=True
                        )
                        
                        st.metric("Model RÂ² Score", f"{train_score:.4f}")
                    
                    with col2:
                        # Bar chart
                        fig_bar = px.bar(
                            feature_importance_df,
                            x='Importance_Pct',
                            y='Feature',
                            orientation='h',
                            title=f'Feature Importance ({algorithm})',
                            labels={'Importance_Pct': 'Importance (%)', 'Feature': 'Features'},
                            color='Importance_Pct',
                            color_continuous_scale='viridis'
                        )
                        fig_bar.update_layout(
                            yaxis={'categoryorder': 'total ascending'},
                            height=400
                        )
                        st.plotly_chart(fig_bar, use_container_width=True)
                    
                    # Detailed insights
                    st.markdown("### ğŸ” Insights")
                    
                    # Top 3 features
                    top_features = feature_importance_df.head(3)
                    st.markdown("**ğŸ¥‡ Top 3 Most Important Features:**")
                    for i, (_, row) in enumerate(top_features.iterrows(), 1):
                        emoji = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰"][i-1]
                        st.write(f"{emoji} **{row['Feature']}**: {row['Importance_Pct']:.1f}% importance")
                    
                    # Cumulative importance
                    feature_importance_df['Cumulative_Pct'] = feature_importance_df['Importance_Pct'].cumsum()
                    features_80pct = len(feature_importance_df[feature_importance_df['Cumulative_Pct'] <= 80])
                    features_90pct = len(feature_importance_df[feature_importance_df['Cumulative_Pct'] <= 90])
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("Features for 80% importance", f"{features_80pct}/{len(selected_features)}")
                    with col2:
                        st.metric("Features for 90% importance", f"{features_90pct}/{len(selected_features)}")
                    with col3:
                        least_important = feature_importance_df.iloc[-1]['Feature']
                        st.metric("Least important feature", least_important)
                    
                    # Feature correlation with target
                    st.markdown("### ğŸ¯ Feature-Target Correlations")
                    correlations = []
                    for feature in selected_features:
                        corr = feature_data[feature].corr(feature_data[target_col])
                        correlations.append({
                            'Feature': feature,
                            'Correlation': corr,
                            'Abs_Correlation': abs(corr)
                        })
                    
                    corr_df = pd.DataFrame(correlations).sort_values('Abs_Correlation', ascending=False)
                    
                    fig_corr = px.bar(
                        corr_df,
                        x='Feature',
                        y='Correlation',
                        title=f'Feature Correlations with {target_col}',
                        color='Correlation',
                        color_continuous_scale='RdBu_r'
                    )
                    fig_corr.add_hline(y=0, line_dash="dash", line_color="black")
                    st.plotly_chart(fig_corr, use_container_width=True)
                    
                    # Recommendations
                    st.markdown("### ğŸ’¡ Recommendations")
                    high_importance = feature_importance_df[feature_importance_df['Importance_Pct'] > 15]
                    low_importance = feature_importance_df[feature_importance_df['Importance_Pct'] < 2]
                    
                    if len(high_importance) > 0:
                        st.success(f"ğŸ¯ **Focus on high-impact features**: {', '.join(high_importance['Feature'].tolist())}")
                    
                    if len(low_importance) > 0:
                        st.info(f"ğŸ” **Consider removing low-impact features**: {', '.join(low_importance['Feature'].tolist())}")
                    
                    if train_score < 0.5:
                        st.warning("âš ï¸ **Low model performance**: Consider feature engineering or different algorithms")
                    elif train_score > 0.8:
                        st.success("âœ… **Good model performance**: Features explain the target well")
        
        elif target_col in selected_features:
            st.warning("âš ï¸ Target variable cannot be in the feature list!")
        elif len(selected_features) < 2:
            st.warning("âš ï¸ Select at least 2 features for analysis!")

def show_ab_testing():
    """
    ×”×¦×’×ª ×××©×§ ×œ×‘×™×¦×•×¢ ×‘×“×™×§×•×ª A/B ×•×‘×“×™×§×•×ª ×¡×˜×˜×™×¡×˜×™×•×ª ×”×©×•×•××ª×™×•×ª
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - ××ª×Ÿ ×›×œ×™× ××ª×§×“××™× ×œ×‘×™×¦×•×¢ ×‘×“×™×§×•×ª A/B ××“×¢×™×•×ª ×•××”×™×× ×•×ª
    - ×”×©×•×•××” ×¡×˜×˜×™×¡×˜×™×ª ×‘×™×Ÿ ×§×‘×•×¦×•×ª ××• ××¦×‘×™× ×©×•× ×™×
    - ×”×¢×¨×›×ª ××©××¢×•×ª ×¡×˜×˜×™×¡×˜×™×ª ×©×œ ×”×‘×“×œ×™× ×©× ×¦×¤×•
    - ×™×¦×™×¨×ª ×”××œ×¦×•×ª ×¢×¡×§×™×•×ª ××‘×•×¡×¡×•×ª × ×ª×•× ×™×
    
    ×¡×•×’×™ ×”×‘×“×™×§×•×ª ×”×–××™× ×•×ª:
    
    1. Independent T-Test:
       - ×”×©×•×•××” ×‘×™×Ÿ ×××•×¦×¢×™× ×©×œ ×©×ª×™ ×§×‘×•×¦×•×ª ×‘×œ×ª×™ ×ª×œ×•×™×•×ª
       - ×‘×“×™×§×ª ×”×‘×“×œ×™× ××©××¢×•×ª×™×™× ×‘×¨××ª ×‘×™×˜×—×•×Ÿ ×’×‘×•×”×”
       - ×”×¢×¨×›×ª ×’×•×“×œ ×”××¤×§×˜ (Effect Size) Cohen's d
       - ×”××œ×¦×•×ª ×œ×’×•×“×œ ××“×’× × ×“×¨×©
    
    2. Chi-Square Test:
       - ×”×©×•×•××” ×‘×™×Ÿ ×”×ª×¤×œ×’×•×™×•×ª ×§×˜×’×•×¨×™××œ×™×•×ª
       - ×‘×“×™×§×ª ×¢×¦×××•×ª ×‘×™×Ÿ ××©×ª× ×™× ×§×˜×’×•×¨×™××œ×™×™×
       - × ×™×ª×•×— ×˜×‘×œ××•×ª ×¦×•×œ×‘×•×ª (Contingency Tables)
       - ×”×¢×¨×›×ª ×¢×•×¦××ª ×”×§×©×¨ ×‘×™×Ÿ ××©×ª× ×™×
    
    3. Welch's T-Test:
       - ×’×¨×¡×” ×¢××™×“×” ×©×œ t-test ×œ×§×‘×•×¦×•×ª ×¢× ×©×•× ×•×™×•×ª ×©×•× ×•×ª
       - ××ª××™× ×œ××§×¨×™× ×©×‘×”× ×”× ×—×ª ×©×•×•×™×ª ×”×©×•× ×•×™×•×ª ×œ× ××ª×§×™×™××ª
       - × ×™×ª×•×— ××ª×§×“× ×™×•×ª×¨ ×œ××“×’××™× ×œ× ×××•×–× ×™×
    
    4. Mann-Whitney U Test:
       - ×‘×“×™×§×” × ×•×Ÿ-×¤×¨××˜×¨×™×ª ×œ×§×‘×•×¦×•×ª ×¢× ×”×ª×¤×œ×’×•×ª ×œ× × ×•×¨××œ×™×ª
       - ×”×©×•×•××ª ×—×¦×™×•× ×™× ×‘××§×•× ×××•×¦×¢×™×
       - ×¢××™×“×•×ª ×’×‘×•×”×” ×œ×¢×¨×›×™× ×—×¨×™×’×™×
    
    ×¤×™×¦'×¨×™× ××ª×§×“××™×:
    - ×™×¦×™×¨×ª × ×ª×•× ×™ ×“×•×’××” ×œ×‘×“×™×§×•×ª
    - ×—×™×©×•×‘ Power Analysis ×œ×§×‘×™×¢×ª ×’×•×“×œ ××“×’×
    - ×•×™×–×•××œ×™×–×¦×™×” ×©×œ ×”×ª×¤×œ×’×•×™×•×ª ×•×”×‘×“×œ×™×
    - ×“×•×—×•×ª ××¤×•×¨×˜×™× ×¢× ×¤×¨×©× ×•×ª ×¡×˜×˜×™×¡×˜×™×ª
    - ×”××œ×¦×•×ª ×œ×¤×¢×•×œ×•×ª ×¢×ª×™×“×™×•×ª
    - ×—×™×©×•×‘ ×¨××ª ×‘×™×˜×—×•×Ÿ ×•×¨×•×•×—×™ ×¡××š
    
    ×”×—×–×¨×”: ×œ×œ× - ×”×¤×•× ×§×¦×™×” ××¦×™×’×” ××ª ×”×××©×§ ×™×©×™×¨×•×ª
    """
    st.markdown("## ğŸ§ª A/B Testing")
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.markdown("### ğŸ² Generate Test Data")
        if st.button("ğŸ“Š Create A/B Test Data"):
            ab_data = generate_ab_test_data()
            st.session_state.data = ab_data
            st.success("A/B test data created!")
            st.rerun()
    
    if 'data' not in st.session_state:
        st.warning("ğŸ“‚ Load data or create A/B test data")
        return
    
    df = st.session_state.data
    
    with col2:
        st.markdown("### âš™ï¸ Test Configuration")
        
        # Automatic detection of group and metric
        group_cols = [col for col in df.columns if 'group' in col.lower() or 'variant' in col.lower() or 'test' in col.lower()]
        metric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        
        if group_cols:
            group_col = st.selectbox("Group column", group_cols, index=0)
        else:
            group_col = st.selectbox("Group column", df.columns)
        
        if metric_cols:
            metric_col = st.selectbox("Metric for analysis", metric_cols, index=0)
        else:
            metric_col = st.selectbox("Metric for analysis", df.columns)
    
    if st.button("ğŸ§ª Conduct A/B Test Analysis"):
        try:
            with st.spinner("Analyzing A/B test..."):
                # Data preparation
                test_data = df[[group_col, metric_col]].dropna()
                
                # Get unique groups
                groups = test_data[group_col].unique()
                
                if len(groups) != 2:
                    st.warning(f"âš ï¸ Found {len(groups)} groups. A/B test works with 2 groups.")
                    st.write("Available groups:", groups)
                    
                    # Take first 2 groups
                    if len(groups) > 2:
                        groups = groups[:2]
                        test_data = test_data[test_data[group_col].isin(groups)]
                        st.info(f"Analyzing groups: {groups[0]} vs {groups[1]}")
                
                # Split into control and treatment groups
                control = test_data[test_data[group_col] == groups[0]][metric_col]
                treatment = test_data[test_data[group_col] == groups[1]][metric_col]
                
                # Basic statistics
                control_mean = control.mean()
                treatment_mean = treatment.mean()
                control_std = control.std()
                treatment_std = treatment.std()
                
                # Statistical tests
                # T-test (Welch's t-test for unequal variances)
                t_stat, p_value_ttest = stats.ttest_ind(control, treatment, equal_var=False)
                
                # Mann-Whitney U test (non-parametric)
                try:
                    u_stat, p_value_mannwhitney = stats.mannwhitneyu(control, treatment, alternative='two-sided')
                except Exception as e:
                    st.warning(f"Mann-Whitney test failed: {e}")
                    p_value_mannwhitney = np.nan
                
                # Effect (difference of means)
                effect = treatment_mean - control_mean
                effect_pct = (effect / control_mean) * 100 if control_mean != 0 else 0
                
                # Cohen's d (effect size) - corrected calculation
                pooled_std = np.sqrt(((len(control) - 1) * control_std**2 + (len(treatment) - 1) * treatment_std**2) / (len(control) + len(treatment) - 2))
                cohens_d = effect / pooled_std if pooled_std != 0 else 0
                
                # Confidence interval for the difference
                se_diff = np.sqrt(control_std**2/len(control) + treatment_std**2/len(treatment))
                ci_lower = effect - 1.96 * se_diff
                ci_upper = effect + 1.96 * se_diff
                
                # Statistical power calculation
                from scipy.stats import norm
                pooled_se = np.sqrt(control_std**2/len(control) + treatment_std**2/len(treatment))
                z_score = abs(effect) / pooled_se if pooled_se != 0 else 0
                current_power = 1 - norm.cdf(1.96 - z_score) + norm.cdf(-1.96 - z_score) if pooled_se != 0 else 0
                
                # Sample size for 80% power
                if control_std > 0 and treatment_std > 0:
                    pooled_variance = (control_std**2 + treatment_std**2) / 2
                    n_per_group_80 = 2 * pooled_variance * ((1.96 + 0.84)**2) / (effect**2) if effect != 0 else np.inf
                else:
                    n_per_group_80 = np.inf
                
                # Display results
                st.markdown("### ğŸ“Š A/B Test Results")
                
                # Group metrics
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric(f"{groups[0]} (mean)", f"{control_mean:.3f}")
                with col2:
                    st.metric(f"{groups[1]} (mean)", f"{treatment_mean:.3f}")
                with col3:
                    st.metric("Difference", f"{effect:.3f}")
                with col4:
                    st.metric("Change %", f"{effect_pct:+.2f}%")
                
                # Statistical significance
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("P-value (t-test)", f"{p_value_ttest:.4f}")
                with col2:
                    if not np.isnan(p_value_mannwhitney):
                        st.metric("P-value (Mann-Whitney)", f"{p_value_mannwhitney:.4f}")
                    else:
                        st.metric("P-value (Mann-Whitney)", "N/A")
                with col3:
                    st.metric("Cohen's d", f"{cohens_d:.3f}")
                with col4:
                    st.metric("95% CI", f"[{ci_lower:.3f}, {ci_upper:.3f}]")
                
                # Results interpretation
                alpha = 0.05
                
                # Statistical significance
                st.markdown("#### ğŸ“Š Statistical Significance")
                if p_value_ttest < alpha:
                    st.success(f"ğŸ‰ **Statistically significant difference!** (t-test, p = {p_value_ttest:.4f})")
                    if ci_lower > 0:
                        st.info("âœ… Confidence interval doesn't include 0 - effect is likely real")
                    elif ci_upper < 0:
                        st.info("âœ… Confidence interval doesn't include 0 - effect is likely real")
                else:
                    st.warning(f"âŒ **No statistically significant difference** (t-test, p = {p_value_ttest:.4f})")
                    st.info("ğŸ” This could mean: no real effect, insufficient sample size, or high variability")
                
                if not np.isnan(p_value_mannwhitney):
                    if p_value_mannwhitney < alpha:
                        st.success(f"ğŸ‰ **Mann-Whitney test also significant!** (p = {p_value_mannwhitney:.4f})")
                    else:
                        st.warning(f"âŒ **Mann-Whitney test not significant** (p = {p_value_mannwhitney:.4f})")
                
                # Effect size interpretation
                st.markdown("#### ğŸ“ Effect Size Analysis")
                if abs(cohens_d) < 0.2:
                    effect_size = "negligible"
                    effect_color = "ğŸ”µ"
                elif abs(cohens_d) < 0.5:
                    effect_size = "small"
                    effect_color = "ğŸŸ¡"
                elif abs(cohens_d) < 0.8:
                    effect_size = "medium"
                    effect_color = "ğŸŸ "
                else:
                    effect_size = "large"
                    effect_color = "ğŸ”´"
                
                st.write(f"{effect_color} **Effect size: {effect_size}** (Cohen's d = {cohens_d:.3f})")
                
                # Practical significance
                if abs(effect_pct) > 10:
                    st.success(f"ğŸ’° **Practically significant**: {abs(effect_pct):.1f}% change")
                elif abs(effect_pct) > 5:
                    st.info(f"ğŸ“Š **Moderate practical impact**: {abs(effect_pct):.1f}% change")
                else:
                    st.warning(f"ğŸ“‰ **Small practical impact**: {abs(effect_pct):.1f}% change")
                
                # Visualization
                col1, col2 = st.columns(2)
                
                with col1:
                    # Box plot
                    fig_box = px.box(test_data, x=group_col, y=metric_col, 
                                    title=f"Distribution of {metric_col} by Groups")
                    st.plotly_chart(fig_box, use_container_width=True)
                
                with col2:
                    # Histogram
                    fig_hist = go.Figure()
                    
                    fig_hist.add_trace(go.Histogram(
                        x=control,
                        name=f"{groups[0]}",
                        opacity=0.7,
                        nbinsx=30
                    ))
                    
                    fig_hist.add_trace(go.Histogram(
                        x=treatment,
                        name=f"{groups[1]}",
                        opacity=0.7,
                        nbinsx=30
                    ))
                    
                    fig_hist.update_layout(
                        title="Group Distributions",
                        xaxis_title=metric_col,
                        yaxis_title="Frequency",
                        barmode='overlay'
                    )
                    
                    st.plotly_chart(fig_hist, use_container_width=True)
                
                # Statistical power and sample size
                st.markdown("### ğŸ“ˆ Statistical Power Analysis")
                
                power_col1, power_col2, power_col3 = st.columns(3)
                
                with power_col1:
                    st.metric("Current Power", f"{current_power:.3f}")
                
                with power_col2:
                    st.metric("Current Sample Size", f"{len(control) + len(treatment):,}")
                
                with power_col3:
                    if n_per_group_80 != np.inf and n_per_group_80 > 0:
                        st.metric("Sample Size for 80% Power", f"{int(n_per_group_80 * 2):,}")
                    else:
                        st.metric("Sample Size for 80% Power", "N/A")
                
                # Power interpretation
                if current_power >= 0.8:
                    st.success("âœ… **Sufficient statistical power** (â‰¥0.8)")
                    st.write("Your test has enough power to detect meaningful differences")
                elif current_power >= 0.5:
                    st.warning("âš ï¸ **Moderate statistical power** (0.5-0.8)")
                    st.write("Consider increasing sample size for more reliable results")
                else:
                    st.error("âŒ **Low statistical power** (<0.5)")
                    st.write("High risk of missing real effects (Type II error)")
                
                # Sample size recommendations
                if n_per_group_80 != np.inf and n_per_group_80 > 0:
                    current_total = len(control) + len(treatment)
                    recommended_total = int(n_per_group_80 * 2)
                    
                    if current_total < recommended_total:
                        additional_needed = recommended_total - current_total
                        st.info(f"ğŸ’¡ **Recommendation**: Collect {additional_needed:,} more samples for 80% power")
                    else:
                        st.success("ğŸ¯ Your sample size exceeds the requirement for 80% power!")
                
                # Recommendations
                st.markdown("### ğŸ’¡ Recommendations & Conclusions")
                
                recommendations = []
                
                # Statistical significance + practical significance
                if p_value_ttest < 0.05 and abs(effect_pct) > 5:
                    recommendations.append("ğŸ¯ **Strong evidence for effect**: Both statistically and practically significant")
                    recommendations.append("âœ… **Action recommended**: Implement the tested change")
                elif p_value_ttest < 0.05 and abs(effect_pct) <= 5:
                    recommendations.append("ğŸ“Š **Statistically significant but small effect**: Consider cost-benefit analysis")
                    recommendations.append("ğŸ¤” **Decision needed**: Is small improvement worth implementation cost?")
                elif p_value_ttest >= 0.05 and abs(effect_pct) > 10:
                    recommendations.append("ğŸ“ˆ **Large effect but not statistically significant**: Increase sample size")
                    recommendations.append("ğŸ”„ **Action recommended**: Continue testing with more data")
                else:
                    recommendations.append("ğŸ“‹ **No convincing evidence of effect**: Consider alternative approaches")
                    recommendations.append("ğŸ”„ **Options**: Test different variants or longer duration")
                
                # Power-based recommendations
                if current_power < 0.8:
                    recommendations.append("ğŸ“Š **Increase statistical power**: Larger sample size needed for reliable conclusions")
                
                # Effect size recommendations
                if abs(cohens_d) >= 0.5:
                    recommendations.append("ğŸ’ª **Meaningful effect size detected**: Worth further investigation")
                elif abs(cohens_d) < 0.2:
                    recommendations.append("ğŸ“‰ **Very small effect**: Question if this change is worth pursuing")
                
                # Data quality recommendations
                control_cv = (control_std / control_mean) * 100 if control_mean != 0 else 0
                treatment_cv = (treatment_std / treatment_mean) * 100 if treatment_mean != 0 else 0
                
                if control_cv > 100 or treatment_cv > 100:
                    recommendations.append("âš ï¸ **High variability detected**: Consider data cleaning or longer measurement period")
                
                # Display recommendations
                for rec in recommendations:
                    st.write(f"â€¢ {rec}")
                
                # Summary conclusion
                st.markdown("#### ğŸ¯ Executive Summary")
                
                if p_value_ttest < 0.05 and abs(effect_pct) > 5 and current_power > 0.8:
                    conclusion = "ğŸŸ¢ **STRONG POSITIVE RESULT**: Proceed with implementation"
                elif p_value_ttest < 0.05 and abs(effect_pct) > 2:
                    conclusion = "ğŸŸ¡ **MODERATE POSITIVE RESULT**: Consider implementation with monitoring"
                elif p_value_ttest >= 0.05 and current_power > 0.8:
                    conclusion = "ğŸ”´ **NO SIGNIFICANT EFFECT**: Do not implement this change"
                else:
                    conclusion = "ğŸŸ¡ **INCONCLUSIVE RESULT**: Need more data for reliable conclusion"
                
                st.write(conclusion)
                
                # Key metrics summary
                summary_metrics = {
                    "Control Mean": f"{control_mean:.3f}",
                    "Treatment Mean": f"{treatment_mean:.3f}",
                    "Difference": f"{effect:.3f}",
                    "% Change": f"{effect_pct:+.2f}%",
                    "P-value": f"{p_value_ttest:.4f}",
                    "Effect Size": f"{cohens_d:.3f}",
                    "Statistical Power": f"{current_power:.3f}",
                    "Significance": "Yes" if p_value_ttest < 0.05 else "No"
                }
                
                st.markdown("#### ğŸ“‹ Key Metrics Summary")
                summary_df = pd.DataFrame([
                    {"Metric": k, "Value": v} for k, v in summary_metrics.items()
                ])
                st.dataframe(summary_df, use_container_width=True)
                
        except Exception as e:
            st.error(f"âš ï¸ Error in A/B test analysis: {str(e)}")
            st.info("ğŸ’¡ **Troubleshooting tips:**")
            st.write("â€¢ Check that your group column has exactly 2 distinct values")
            st.write("â€¢ Ensure your metric column contains numeric data")
            st.write("â€¢ Verify there are no empty or invalid values")
            st.write("â€¢ Try using demo A/B test data to test the functionality")

def generate_ab_test_data():
    """
    ×™×¦×™×¨×ª × ×ª×•× ×™ ×“×•×’××” ×¨×™××œ×™×¡×˜×™×™× ×œ×‘×“×™×§×•×ª A/B
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - ×™×¦×™×¨×ª ××¡×’×¨×ª × ×ª×•× ×™× ×¡×™× ×ª×˜×™×ª ×”××“××” × ×™×¡×•×™ A/B ×××™×ª×™
    - ×”×“××™×” ×©×œ ×©×ª×™ ×§×‘×•×¦×•×ª: ×‘×§×¨×” (Control) ×•× ×™×¡×™×•×Ÿ (Treatment)
    - ×™×¦×™×¨×ª ×”×‘×“×œ×™× ×¨×™××œ×™×¡×˜×™×™× ×‘××˜×¨×™×§×•×ª ×”×¢×¡×§×™×•×ª
    - ××ª×Ÿ ×“×•×’××” ×¢×‘×•×“×” ×œ×ª×¨×’×•×œ ×‘×“×™×§×•×ª A/B
    
    ×”××‘× ×” ×©×œ × ×ª×•× ×™ ×”×“×•×’××”:
    - Group: ××©×ª× ×” ×§×˜×’×•×¨×™××œ×™ (A = Control, B = Treatment)
    - Conversion_Rate: ×©×™×¢×•×¨ ×”××¨×” (××˜×¨×™×§×” ×¢×™×§×¨×™×ª)
    - Revenue: ×”×›× ×¡×•×ª ×××•×¦×¢×•×ª
    - Page_Views: ××¡×¤×¨ ×¦×¤×™×•×ª ×‘×¢××•×“
    - Session_Duration: ××•×¨×š ×”×¡×©×Ÿ ×‘×“×§×•×ª
    - User_Satisfaction: ×“×™×¨×•×’ ×©×‘×™×¢×•×ª ×¨×¦×•×Ÿ
    - Device_Type: ×¡×•×’ ××›×©×™×¨ (Desktop/Mobile/Tablet)
    - Age_Group: ×§×‘×•×¦×•×ª ×’×™×œ ×©×•× ×•×ª
    
    ×”×¤×¨××˜×¨×™× ×”×¡×˜×˜×™×¡×˜×™×™×:
    - ×§×‘×•×¦×ª A: ×©×™×¢×•×¨ ×”××¨×” ×©×œ 12% ×‘×××•×¦×¢
    - ×§×‘×•×¦×ª B: ×©×™×¢×•×¨ ×”××¨×” ×©×œ 15% ×‘×××•×¦×¢ (×©×™×¤×•×¨ ×©×œ 25%)
    - ×”×ª×¤×œ×’×•×ª × ×•×¨××œ×™×ª ×¢× ×¨×¢×© ×¨×™××œ×™×¡×˜×™
    - ×§×•×¨×œ×¦×™×•×ª ×˜×‘×¢×™×•×ª ×‘×™×Ÿ ×”××©×ª× ×™× ×”×©×•× ×™×
    
    ×”×—×–×¨×”:
        DataFrame: ××¡×’×¨×ª × ×ª×•× ×™× ×¢× 1000 ×¨×©×•××•×ª ×©×œ × ×™×¡×•×™ A/B ××“×•××”
    """
    np.random.seed(42)
    
    n_control = 1000
    n_treatment = 1000
    
    # Control group (~10% conversion)
    control_data = {
        'group': ['Control'] * n_control,
        'user_id': range(1, n_control + 1),
        'conversion_rate': np.random.beta(2, 18, n_control),  # ~10% conversion
        'revenue': np.random.exponential(25, n_control),
        'clicks': np.random.poisson(100, n_control),
        'time_on_site': np.random.normal(120, 30, n_control),  # seconds
        'pages_viewed': np.random.poisson(3, n_control)
    }
    
    # Treatment group (~20% improvement)
    treatment_data = {
        'group': ['Treatment'] * n_treatment,
        'user_id': range(n_control + 1, n_control + n_treatment + 1),
        'conversion_rate': np.random.beta(2.4, 17.6, n_treatment),  # ~12% conversion
        'revenue': np.random.exponential(30, n_treatment),  # higher revenue
        'clicks': np.random.poisson(110, n_treatment),  # more clicks
        'time_on_site': np.random.normal(140, 35, n_treatment),  # more time
        'pages_viewed': np.random.poisson(3.5, n_treatment)  # more pages
    }
    
    # Combine data
    control_df = pd.DataFrame(control_data)
    treatment_df = pd.DataFrame(treatment_data)
    
    return pd.concat([control_df, treatment_df], ignore_index=True)

def _db_path() -> str:
    """
    ×”×—×–×¨×ª × ×ª×™×‘ ×œ×§×•×‘×¥ ×‘×¡×™×¡ ×”× ×ª×•× ×™× SQLite ×”×¢×™×§×¨×™ ×©×œ ×”××¤×œ×™×§×¦×™×”
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - ××ª×Ÿ × ×ª×™×‘ ×§×‘×•×¢ ×œ×§×•×‘×¥ ×‘×¡×™×¡ ×”× ×ª×•× ×™× ×©×‘×• × ×©××¨×™× ×”×§×‘×¦×™× ×”××•×¢×œ×™×
    - ×¨×™×›×•×– × ×™×”×•×œ ×”× ×ª×™×‘ ×‘××§×•× ××—×“ ×œ×§×œ×•×ª ×ª×—×–×•×§×”
    - ×ª××™×›×” ×‘×©××™×¨×” ×§×‘×•×¢×” ×©×œ × ×ª×•× ×™× ×‘×™×Ÿ ×”×¨×¦×•×ª ×”××¤×œ×™×§×¦×™×”
    
    ×”×—×–×¨×”:
        str: × ×ª×™×‘ ×œ×§×•×‘×¥ "uploaded_data.db" ×”××©××© ×œ××—×¡×•×Ÿ ×”× ×ª×•× ×™×
    """
    return "uploaded_data.db"

def _run_sql(query: str) -> pd.DataFrame:
    """
    ×‘×™×¦×•×¢ ×©××™×œ×ª×ª SQL ×¢×œ ×‘×¡×™×¡ ×”× ×ª×•× ×™× ×”×¢×™×§×¨×™ ×©×œ ×”××¤×œ×™×§×¦×™×”
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - ×‘×™×¦×•×¢ ×©××™×œ×ª×•×ª SQL ×¢×œ ×‘×¡×™×¡ ×”× ×ª×•× ×™× ×”××§×•××™
    - ×˜×™×¤×•×œ ×‘×—×™×‘×•×¨ ×•×‘×¡×’×™×¨×” ×”×‘×˜×•×—×” ×©×œ ×‘×¡×™×¡ ×”× ×ª×•× ×™×
    - ×”××¨×” ××•×˜×•××˜×™×ª ×©×œ ×ª×•×¦××•×ª SQL ×œ××¡×’×¨×ª × ×ª×•× ×™× pandas
    - ×‘×“×™×§×ª ×§×™×•× ×‘×¡×™×¡ ×”× ×ª×•× ×™× ×œ×¤× ×™ ×‘×™×¦×•×¢ ×”×©××™×œ×ª×”
    
    ×¤×¨××˜×¨×™×:
        query (str): ×©××™×œ×ª×ª SQL ×œ×‘×™×¦×•×¢
    
    ×”×—×–×¨×”:
        pd.DataFrame: ×ª×•×¦××•×ª ×”×©××™×œ×ª×” ×›××¡×’×¨×ª × ×ª×•× ×™×
        
    ×–×•×¨×§:
        FileNotFoundError: ×× ×‘×¡×™×¡ ×”× ×ª×•× ×™× ×œ× ×§×™×™× (× ×“×¨×© ×œ×”×¢×œ×•×ª ×§×‘×¦×™× ×§×•×“×)
        Exception: ×‘××§×¨×” ×©×œ ×©×’×™××” ×‘×‘×™×¦×•×¢ ×”×©××™×œ×ª×”
    """
    db = _db_path()
    if not os.path.exists(db):
        raise FileNotFoundError("Database not found. Please upload CSV files first.")
    conn = sqlite3.connect(db)
    try:
        return pd.read_sql_query(query, conn)
    finally:
        conn.close()

def _show_query_insights(df: pd.DataFrame) -> None:
    """
    ×”×¦×’×ª ×ª×•×‘× ×•×ª ×§×•××¤×§×˜×™×•×ª ×¢×œ ×ª×•×¦××•×ª ×”×©××™×œ×ª×” ×¢× ×•×™×–×•××œ×™×–×¦×™×” ××”×™×¨×”
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - ××ª×Ÿ ×¡×™×›×•× ××”×™×¨ ×©×œ ×”×ª×•×¦××•×ª ×”××¡×¤×¨×™×•×ª ××”×©××™×œ×ª×”
    - ×”×¦×’×ª ××˜×¨×™×§×•×ª ××¤×ª×—: ×¡×›×•×, ×××•×¦×¢, ×—×¦×™×•×Ÿ, ×˜×•×•×—
    - ×™×¦×™×¨×ª ×ª×¨×©×™× ×¢××•×“×•×ª ××”×™×¨ ×× ×”×ª×•×¦××•×ª ×§×˜× ×•×ª ×“×™×™×Ÿ
    - ×¢×–×¨×” ×‘×”×‘× ×” ××”×™×¨×” ×©×œ ×”×ª×•×¦××•×ª ×œ×œ× ×¦×•×¨×š ×‘× ×™×ª×•×— ×¢××•×§
    
    ×¤×¨××˜×¨×™×:
        df (pd.DataFrame): ××¡×’×¨×ª ×”× ×ª×•× ×™× ×¢× ×ª×•×¦××•×ª ×”×©××™×œ×ª×”
        
    ×¤×œ×˜:
    - ××˜×¨×™×§×•×ª ××¡×¤×¨×™×•×ª ×œ××©×ª× ×™× × ×•××¨×™×™× (×¢×“ 4 ×¢××•×“×•×ª)
    - ×ª×¨×©×™× ×¢××•×“×•×ª ×× ×™×© ×¤×—×•×ª ×-20 ×©×•×¨×•×ª
    - ×¡×™×›×•× ×‘×¡×™×¡×™ ×©×œ ××‘× ×” ×”× ×ª×•× ×™×
    
    ×”×—×–×¨×”: ×œ×œ× - ×”×¤×•× ×§×¦×™×” ××¦×™×’×” ××ª ×”×ª×•×¦××•×ª ×™×©×™×¨×•×ª
    """
    if df.empty:
        return

    num_cols = df.select_dtypes(include=["number"]).columns.tolist()
    if num_cols:
        st.markdown("#### ğŸ“Š Numeric Statistics â€” *Quick insights*")
        for col in num_cols[:4]:
            c1, c2, c3, c4 = st.columns(4)
            with c1:
                st.metric(f"{col} Â· Sum", f"{df[col].sum():,.2f}")
            with c2:
                st.metric(f"{col} Â· Avg", f"{df[col].mean():.2f}")
            with c3:
                st.metric(f"{col} Â· Max", f"{df[col].max():,.2f}")
            with c4:
                st.metric(f"{col} Â· Min", f"{df[col].min():.2f}")

    # Small categorical Ã— numeric bar if feasible
    if len(df) <= 20 and len(df.columns) >= 2:
        num_cols = df.select_dtypes(include=["number"]).columns.tolist()
        cat_cols = [c for c in df.columns if c not in num_cols]
        if num_cols and cat_cols:
            fig = px.bar(df, x=cat_cols[0], y=num_cols[0],
                         title=f"{num_cols[0]} by {cat_cols[0]}")
            st.plotly_chart(fig, use_container_width=True)

    st.caption("**Explanation (EN):** We summarize numeric columns (sum/avg/min/max). "
               "If the result set is small and has both a categorical and a numeric field, "
               "we also render a quick bar chart.")

# ---------- Main section (DROP-IN REPLACEMENT for your show_database) ----------

# If _HAS_PG was not defined above for some reason, define it safely here too
try:
    _HAS_PG
except NameError:
    try:
        import psycopg2  # noqa: F401
        _HAS_PG = True
    except Exception:
        _HAS_PG = False


# ---------------------- Helpers ----------------------

def _validate_sql_query(sql: str) -> tuple[bool, str]:
    """
    ×•×œ×™×“×¦×™×” ×©×œ ×©××™×œ×ª×ª SQL ×œ×× ×™×¢×ª SQL Injection
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - ×œ×•×•×“× ×©×”×©××™×œ×ª×” ××›×™×œ×” ×¨×§ ×¤×§×•×“×•×ª ×‘×˜×•×—×•×ª ×œ×§×¨×™××”
    - ×œ×× ×•×¢ ×¤×§×•×“×•×ª ××¡×•×›× ×•×ª ×©×™×›×•×œ×•×ª ×œ×’×¨×•× × ×–×§
    - ×œ×”×—×–×™×¨ ×”×•×“×¢×ª ×©×’×™××” ××¤×•×¨×˜×ª ×‘××§×¨×” ×©×œ ×‘×¢×™×”
    
    ×¤×¨××˜×¨×™×:
        sql (str): ×©××™×œ×ª×ª SQL ×œ×‘×“×™×§×”
    
    ×”×—×–×¨×”:
        tuple: (bool - ×”×× ×”×©××™×œ×ª×” ×‘×˜×•×—×”, str - ×”×•×“×¢×ª ×©×’×™××” ×× ×§×™×™××ª)
    """
    if not sql or not sql.strip():
        return False, "SQL query cannot be empty"
    
    # ×”×¡×¨×ª ×¨×•×•×—×™× ×•×ª×•×•×™× ××™×•×—×“×™× ××”×”×ª×—×œ×” ×•×”×¡×•×£
    sql_clean = sql.strip().upper()
    
    # ×¨×©×™××ª ×¤×§×•×“×•×ª ××•×ª×¨×•×ª (×¨×§ ×œ×§×¨×™××”)
    allowed_commands = ['SELECT', 'WITH', 'SHOW', 'DESCRIBE', 'EXPLAIN']
    
    # ×¨×©×™××ª ×¤×§×•×“×•×ª ××¡×•×¨×•×ª (×›×œ ××” ×©×™×›×•×œ ×œ×©× ×•×ª ××• ×œ××—×•×§)
    forbidden_commands = [
        'DROP', 'DELETE', 'INSERT', 'UPDATE', 'ALTER', 'CREATE', 
        'TRUNCATE', 'REPLACE', 'EXEC', 'EXECUTE', 'CALL'
    ]
    
    # ×‘×“×™×§×” ×©×”×©××™×œ×ª×” ××ª×—×™×œ×” ×‘×¤×§×•×“×” ××•×ª×¨×ª
    starts_with_allowed = any(sql_clean.startswith(cmd) for cmd in allowed_commands)
    if not starts_with_allowed:
        return False, f"SQL must start with one of: {', '.join(allowed_commands)}"
    
    # ×‘×“×™×§×” ×©××™×Ÿ ×¤×§×•×“×•×ª ××¡×•×¨×•×ª
    for forbidden in forbidden_commands:
        if forbidden in sql_clean:
            return False, f"Forbidden SQL command detected: {forbidden}"
    
    # ×‘×“×™×§×ª ×ª×•×•×™× ×—×©×•×“×™×
    suspicious_chars = [';--', '/*', '*/', 'xp_', 'sp_']
    for char in suspicious_chars:
        if char in sql_clean:
            return False, f"Suspicious SQL pattern detected: {char}"
    
    return True, "SQL query is safe"

def _sqlite_db_path() -> str:
    return "uploaded_data.db"

def _normalize_table_name(raw: str, fallback_idx: int = 1) -> str:
    name = raw.rsplit(".csv", 1)[0]
    name = "".join(ch.lower() if ch.isalnum() else "_" for ch in name)
    name = "_".join(filter(None, name.split("_")))
    if not name:
        name = f"table_{fallback_idx}"
    if name and name[0].isdigit():
        name = f"t_{name}"
    return name

def _sqlite_run_sql(query: str) -> pd.DataFrame:
    db = _sqlite_db_path()
    if not os.path.exists(db):
        raise FileNotFoundError("SQLite database not found. Please upload CSV files first.")
    conn = sqlite3.connect(db)
    try:
        return pd.read_sql_query(query, conn)
    finally:
        conn.close()

def _pg_run_sql(conn_kwargs: dict, query: str) -> pd.DataFrame:
    if not _HAS_PG:
        raise RuntimeError("psycopg2 is not installed. Run: pip install psycopg2-binary")
    import psycopg2  # safe: only executed when _HAS_PG True
    conn = psycopg2.connect(**conn_kwargs)
    try:
        return pd.read_sql(query, conn)
    finally:
        conn.close()

def _numeric_quick_insights(df: pd.DataFrame) -> None:
    if df.empty:
        return
    num_cols = df.select_dtypes(include=["number"]).columns.tolist()
    if num_cols:
        st.markdown("#### ğŸ“Š Numeric Statistics â€” *Quick insights*")
        for col in num_cols[:4]:
            c1, c2, c3, c4 = st.columns(4)
            with c1: st.metric(f"{col} Â· Sum", f"{df[col].sum():,.2f}")
            with c2: st.metric(f"{col} Â· Avg", f"{df[col].mean():.2f}")
            with c3: st.metric(f"{col} Â· Max", f"{df[col].max():.2f}")
            with c4: st.metric(f"{col} Â· Min", f"{df[col].min():.2f}")
    if len(df) <= 20 and len(df.columns) >= 2:
        num_cols = df.select_dtypes(include=["number"]).columns.tolist()
        cat_cols = [c for c in df.columns if c not in num_cols]
        if num_cols and cat_cols:
            fig = px.bar(df, x=cat_cols[0], y=num_cols[0],
                         title=f"{num_cols[0]} by {cat_cols[0]}")
            st.plotly_chart(fig, use_container_width=True)
    st.caption("**Explanation (EN):** We summarize numeric columns (sum/avg/min/max). "
               "If the result set is small and has both a categorical and a numeric field, "
               "we also render a quick bar chart.")


# ---------------------- Main UI ----------------------
def show_database():
    """
    ×”×¦×’×ª ×××©×§ ×¢×‘×•×“×” ×¢× ×‘×¡×™×¡×™ × ×ª×•× ×™× ×•×©××™×œ×ª×•×ª SQL
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - ××ª×Ÿ ×›×œ×™× ××ª×§×“××™× ×œ×¢×‘×•×“×” ×¢× ×‘×¡×™×¡×™ × ×ª×•× ×™× ×©×•× ×™×
    - ×ª××™×›×” ×‘×©××™×œ×ª×•×ª SQL ×™×©×™×¨×•×ª ×¢×œ ×”× ×ª×•× ×™× ×©×”×•×¢×œ×•
    - ××¤×©×¨×•×™×•×ª ×—×™×‘×•×¨ ×œ×‘×¡×™×¡×™ × ×ª×•× ×™× ×—×™×¦×•× ×™×™× (PostgreSQL)
    - ×™×¦×™×¨×ª ×˜×‘×œ××•×ª ×–×× ×™×•×ª ×œ×©××™×œ×ª×•×ª ××•×¨×›×‘×•×ª
    
    ×¡×•×’×™ ×‘×¡×™×¡×™ ×”× ×ª×•× ×™× ×”× ×ª××›×™×:
    
    1. SQLite (Local):
       - ×‘×¡×™×¡ × ×ª×•× ×™× ××§×•××™ ×”××•×ª×§×Ÿ ×¢× ×”××¤×œ×™×§×¦×™×”
       - ×™×¦×™×¨×” ××•×˜×•××˜×™×ª ×©×œ ×˜×‘×œ××•×ª ××”×§×‘×¦×™× ×©×”×•×¢×œ×•
       - ×©××™×œ×ª×•×ª ××”×™×¨×•×ª ×•×§×œ×•×ª ×¢×œ × ×ª×•× ×™× ×§×˜× ×™×-×‘×™× ×•× ×™×™×
       - ×œ× ×“×•×¨×© ×”×’×“×¨×•×ª ×—×™×‘×•×¨ × ×•×¡×¤×•×ª
    
    2. PostgreSQL (Remote):
       - ×—×™×‘×•×¨ ×œ×‘×¡×™×¡×™ × ×ª×•× ×™× PostgreSQL ××¨×•×—×§×™×
       - ×ª××™×›×” ×‘×©××™×œ×ª×•×ª ××•×¨×›×‘×•×ª ×¢×œ × ×ª×•× ×™× ×’×“×•×œ×™×
       - ××¤×©×¨×•×™×•×ª ××‘×˜×—×” ×•×—×™×‘×•×¨ ××ª×§×“××•×ª
       - ×“×•×¨×© ×¤×¨××˜×¨×™ ×—×™×‘×•×¨: ×©×¨×ª, ××©×ª××©, ×¡×™×¡××”
    
    3. In-Memory (Demo):
       - ×‘×¡×™×¡ × ×ª×•× ×™× ×–×× ×™ ×‘×–×™×›×¨×•×Ÿ ×œ×¦×•×¨×›×™ ×”×“×’××”
       - ××™×“×™××œ×™ ×œ×‘×“×™×§×•×ª ××”×™×¨×•×ª ×•×œ×™××•×“ SQL
       - ×œ× ×©×•××¨ × ×ª×•× ×™× ×‘×™×Ÿ ×”×¤×¢×œ×•×ª ×”××¤×œ×™×§×¦×™×”
    
    ×¤×™×¦'×¨×™× ××ª×§×“××™×:
    - ×¢×•×¨×š SQL ××™× ×˜×¨××§×˜×™×‘×™ ×¢× ×”×“×’×©×ª ×ª×—×‘×™×¨
    - ×©××™×œ×ª×•×ª ××•×›× ×•×ª ××¨××© ×œ×“×•×’××” ×•×œ××™×“×”
    - × ×™×ª×•×— ××•×˜×•××˜×™ ×©×œ ×ª×•×¦××•×ª ×”×©××™×œ×ª×•×ª
    - ×•×™×–×•××œ×™×–×¦×™×” ××”×™×¨×” ×©×œ ×”×ª×•×¦××•×ª
    - ×ª×•×‘× ×•×ª ×¢×œ ×‘×™×¦×•×¢×™ ×”×©××™×œ×ª×•×ª
    - ×™×™×¦×•× ×ª×•×¦××•×ª ×œ×§×‘×¦×™× ××• ×œ×–×™×›×¨×•×Ÿ ×”××¤×œ×™×§×¦×™×”
    - ×”×ª×¨××•×ª ×¢×œ ×©×’×™××•×ª ×•×˜×™×¤×™× ×œ×©×™×¤×•×¨ ×”×©××™×œ×ª×•×ª
    
    ×“×•×’×××•×ª ×©×™××•×©:
    - × ×™×ª×•×— × ×ª×•× ×™× ×¢×¡×§×™×™× ×¢× GROUP BY ×•-aggregations
    - ×–×™×”×•×™ ×˜×¨× ×“×™× ×¢× Window Functions
    - ×—×™×‘×•×¨ ×˜×‘×œ××•×ª (JOINs) ×œ× ×™×ª×•×— ×¨×‘-××™××“×™
    - ×¡×™× ×•×Ÿ ×•×—×™×¤×•×© ××ª×§×“× ×‘× ×ª×•× ×™×
    
    ×”×—×–×¨×”: ×œ×œ× - ×”×¤×•× ×§×¦×™×” ××¦×™×’×” ××ª ×”×××©×§ ×™×©×™×¨×•×ª
    """
    st.markdown("## ğŸ’¾ Database and SQL")

    backend = st.radio(
        "Choose database backend",
        ["SQLite (uploaded CSV files)", "PostgreSQL (connect to server)"],
        horizontal=False,
    )

    # ---------- SQLite MODE (files -> local SQLite) ----------
    if backend.startswith("SQLite"):
        left, right = st.columns([2, 1])

        # LEFT: upload & SQL
        with left:
            st.markdown("### ğŸ”— Database Management â€” *Upload & Inspect*")

            c1, c2 = st.columns([1, 1])
            with c1:
                replace_db = st.checkbox(
                    "Replace database on upload",
                    value=True,
                    help="If enabled, the existing SQLite DB will be deleted before loading new files."
                )
            with c2:
                if st.button("ğŸ§¹ Reset database"):
                    if os.path.exists(_sqlite_db_path()):
                        os.remove(_sqlite_db_path())
                    st.success("Database reset. Upload new CSV files to create fresh tables.")
                    st.stop()

            uploaded = st.file_uploader(
                "Upload CSV files",
                type=["csv"],
                accept_multiple_files=True,
                help="Each CSV becomes a table inside uploaded_data.db"
            )

            if uploaded and st.button("ğŸ“ Load files into SQLite"):
                try:
                    if replace_db and os.path.exists(_sqlite_db_path()):
                        os.remove(_sqlite_db_path())
                    conn = sqlite3.connect(_sqlite_db_path())
                    loaded = []
                    for idx, f in enumerate(uploaded, start=1):
                        df = pd.read_csv(f)
                        name = _normalize_table_name(f.name, fallback_idx=idx)
                        df.to_sql(name, conn, if_exists="replace", index=False)
                        loaded.append((name, len(df)))
                        st.success(f"âœ… {f.name} â†’ `{name}` ({len(df):,} rows)")
                    conn.close()
                    st.success(f"ğŸ‰ Loaded {len(loaded)} table(s).")
                    st.caption("**Explanation (EN):** We created/overwrote a local SQLite DB; "
                               "now the SQL editor and quick queries will use your tables only.")
                except Exception as e:
                    st.error(f"Load error: {e}")
                    st.info("ğŸ’¡ Try simpler ASCII file names if the issue persists.")

            st.markdown("### âœï¸ SQL Editor â€” *Run Queries*")

            # List tables
            tables = []
            if os.path.exists(_sqlite_db_path()):
                try:
                    conn = sqlite3.connect(_sqlite_db_path())
                    tdf = pd.read_sql_query("SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;", conn)
                    tables = tdf["name"].tolist()
                    if tables:
                        st.markdown(f"**Available tables in `{_sqlite_db_path()}`:**")
                        for t in tables:
                            try:
                                info = pd.read_sql_query(f"SELECT * FROM pragma_table_info('{t}');", conn)
                                cnt = pd.read_sql_query(f'SELECT COUNT(*) AS c FROM "{t}";', conn)["c"].iloc[0]
                                cols = ", ".join(info["name"].tolist()) if not info.empty else "â€”"
                                st.write(f"â€¢ `{t}` â€” {cnt:,} rows Â· {cols}")
                            except Exception as e:
                                st.write(f"â€¢ `{t}` â€” structure read error: {str(e)[:80]}â€¦")
                except Exception as e:
                    st.warning(f"Could not open DB: {e}")
                finally:
                    try: conn.close()
                    except: pass
            else:
                st.info("ğŸ“‚ No database yet. Upload CSV files to create one.")

            # Templates
            if tables:
                sel_table = st.selectbox("Table for templates", tables)
                templates = {
                    "ğŸ” Show 10 rows":  f'SELECT * FROM "{sel_table}" LIMIT 10;',
                    "ğŸ”¢ Count records": f'SELECT COUNT(*) AS total_records FROM "{sel_table}";',
                    "ğŸ§± Table info":    f"SELECT * FROM pragma_table_info('{sel_table}');",
                }
                # numeric template if exists
                try:
                    conn = sqlite3.connect(_sqlite_db_path())
                    sample = pd.read_sql_query(f'SELECT * FROM "{sel_table}" LIMIT 50;', conn)
                    conn.close()
                    num_cols = sample.select_dtypes(include=["number"]).columns.tolist()
                    if num_cols:
                        c = num_cols[0]
                        templates["ğŸ“ˆ Basic stats"] = (
                            f'SELECT AVG("{c}") AS avg_{c}, SUM("{c}") AS sum_{c}, '
                            f'MAX("{c}") AS max_{c}, MIN("{c}") AS min_{c} '
                            f'FROM "{sel_table}";'
                        )
                except Exception:
                    pass
                chosen = st.selectbox("Query template", list(templates.keys()))
                template_sql = templates[chosen]
            else:
                template_sql = "-- Upload CSV files first.\nSELECT 'No tables available' AS message;"

            sql = st.text_area("SQL Query", value=template_sql, height=160)
            if st.button("â–¶ï¸ Execute (SQLite)"):
                # ×‘×“×™×§×ª ××‘×˜×—×” ×œ×× ×™×¢×ª SQL Injection
                is_safe, error_msg = _validate_sql_query(sql)
                if not is_safe:
                    st.error(f"ğŸš¨ **Security Error:** {error_msg}")
                    st.warning("Only SELECT, WITH, SHOW, DESCRIBE, and EXPLAIN queries are allowed for security reasons.")
                else:
                    try:
                        result = _sqlite_run_sql(sql)
                        st.success("âœ… Query executed successfully.")
                        st.dataframe(result, use_container_width=True)
                        _numeric_quick_insights(result)
                    except Exception as e:
                        st.error(f"Query error: {e}")

        # RIGHT: quick queries
        with right:
            st.markdown("### ğŸ“Š Quick Queries â€” *One-click analysis*")
            if not os.path.exists(_sqlite_db_path()):
                st.info("No SQLite database yet. Upload CSV files on the left.")
                return
            conn = sqlite3.connect(_sqlite_db_path())
            try:
                tdf = pd.read_sql_query("SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;", conn)
                tables = tdf["name"].tolist()
            finally:
                conn.close()
            if not tables:
                st.info("No tables found. Upload CSV files first.")
                return

            q_table = st.selectbox("Table", tables, key="quick_table_sqlite")
            # peek
            conn = sqlite3.connect(_sqlite_db_path())
            try:
                peek = pd.read_sql_query(f'SELECT * FROM "{q_table}" LIMIT 200;', conn)
            finally:
                conn.close()

            num_cols = peek.select_dtypes(include=["number"]).columns.tolist()
            cat_cols = [c for c in peek.columns if c not in num_cols]

            if st.button("ğŸ“‹ Table Summary (SQLite)"):
                total = _sqlite_run_sql(f'SELECT COUNT(*) AS c FROM "{q_table}";')
                st.metric("Total Records", f"{int(total.iloc[0,0]):,}")
                head = _sqlite_run_sql(f'SELECT * FROM "{q_table}" LIMIT 5;')
                st.dataframe(head, use_container_width=True)
                st.caption("**Explanation (EN):** Total row count and a 5-row preview.")

            if num_cols and st.button("ğŸ’° Numeric Summary (SQLite)"):
                parts = []
                for c in num_cols[:5]:
                    parts += [f'AVG("{c}") AS avg_{c}', f'SUM("{c}") AS sum_{c}',
                              f'MAX("{c}") AS max_{c}', f'MIN("{c}") AS min_{c}']
                res = _sqlite_run_sql(f'SELECT {", ".join(parts)} FROM "{q_table}";')
                for c in num_cols[:3]:
                    a1, a2, a3, a4 = st.columns(4)
                    with a1: st.metric(f"{c} Â· Avg", f"{res[f'avg_{c}'].iloc[0]:.2f}")
                    with a2: st.metric(f"{c} Â· Sum", f"{res[f'sum_{c}'].iloc[0]:,.2f}")
                    with a3: st.metric(f"{c} Â· Max", f"{res[f'max_{c}'].iloc[0]:.2f}")
                    with a4: st.metric(f"{c} Â· Min", f"{res[f'min_{c}'].iloc[0]:.2f}")
                st.caption("**Explanation (EN):** Aggregations are computed directly in SQLite.")

            if cat_cols and st.button("ğŸ‘¥ Category Analysis (SQLite)"):
                cat = cat_cols[0]
                res = _sqlite_run_sql(
                    f'SELECT "{cat}" AS category, COUNT(*) AS cnt '
                    f'FROM "{q_table}" GROUP BY "{cat}" ORDER BY cnt DESC LIMIT 15;'
                )
                if not res.empty:
                    fig = px.bar(res, x="category", y="cnt", title=f"Distribution of {cat}")
                    st.plotly_chart(fig, use_container_width=True)
                    st.dataframe(res)
                    st.caption("**Explanation (EN):** Counts the most frequent categories.")

            if num_cols and st.button("ğŸ“ˆ Recent Trend (last 100 rows, SQLite)"):
                target = num_cols[0]
                res = _sqlite_run_sql(
                    f'SELECT rowid AS idx, "{target}" FROM "{q_table}" '
                    f"ORDER BY rowid DESC LIMIT 100;"
                )
                if not res.empty:
                    res = res.sort_values("idx")
                    fig = px.line(res, x="idx", y=target, title=f"Trend of {target} (last 100 rows)")
                    st.plotly_chart(fig, use_container_width=True)
                    st.caption("**Explanation (EN):** Uses SQLite internal rowid as an approximate recent order.")

    # ---------- POSTGRESQL MODE (read-only connection) ----------
    else:
        if not _HAS_PG:
            st.error("PostgreSQL driver not found. Install it first:\n`pip install psycopg2-binary`")
            return

        left, right = st.columns([2, 1])

        # LEFT: connection & SQL editor
        with left:
            st.markdown("### ğŸ”— PostgreSQL Connection â€” *Connect & Query*")

            with st.form("pg_conn_form", clear_on_submit=False):
                host = st.text_input("Host", "localhost")
                port = st.text_input("Port", "5432")
                dbname = st.text_input("Database", "")
                user = st.text_input("User", "")
                password = st.text_input("Password", type="password")
                submitted = st.form_submit_button("ğŸ”Œ Connect")

            if submitted:
                st.session_state.pg_conn = {
                    "host": host, "port": port, "dbname": dbname,
                    "user": user, "password": password,
                }
                st.success("Connection parameters saved. Use the widgets below.")
                st.caption("**Explanation (EN):** We do not persist credentials; they live in session only.")

            if "pg_conn" not in st.session_state:
                st.info("Enter connection parameters and click **Connect**.")
                return

            conn_kwargs = st.session_state.pg_conn

            # list tables (user schemas only)
            try:
                tables_df = _pg_run_sql(conn_kwargs, """
                    SELECT table_schema, table_name
                    FROM information_schema.tables
                    WHERE table_type='BASE TABLE'
                      AND table_schema NOT IN ('pg_catalog','information_schema')
                    ORDER BY table_schema, table_name;
                """)
                st.markdown("**Available tables (schema.table):**")
                tables_df["qualified"] = tables_df["table_schema"] + "." + tables_df["table_name"]
                for _, r in tables_df.iterrows():
                    st.write(f"â€¢ `{r['qualified']}`")
            except Exception as e:
                st.error(f"Failed to list tables: {e}")
                return

            # SQL editor
            default_sql = "SELECT NOW() AS server_time;"
            sql = st.text_area("SQL Query (PostgreSQL)", value=default_sql, height=160)
            if st.button("â–¶ï¸ Execute (PostgreSQL)"):
                # ×‘×“×™×§×ª ××‘×˜×—×” ×œ×× ×™×¢×ª SQL Injection  
                is_safe, error_msg = _validate_sql_query(sql)
                if not is_safe:
                    st.error(f"ğŸš¨ **Security Error:** {error_msg}")
                    st.warning("Only SELECT, WITH, SHOW, DESCRIBE, and EXPLAIN queries are allowed for security reasons.")
                else:
                    try:
                        result = _pg_run_sql(conn_kwargs, sql)
                        st.success("âœ… Query executed successfully.")
                        st.dataframe(result, use_container_width=True)
                        _numeric_quick_insights(result)
                    except Exception as e:
                        st.error(f"Query error: {e}")

        # RIGHT: quick queries (schema-aware)
        with right:
            st.markdown("### ğŸ“Š Quick Queries â€” *One-click analysis*")

            try:
                tdf = _pg_run_sql(st.session_state.pg_conn, """
                    SELECT table_schema, table_name
                    FROM information_schema.tables
                    WHERE table_type='BASE TABLE'
                      AND table_schema NOT IN ('pg_catalog','information_schema')
                    ORDER BY table_schema, table_name;
                """)
                tdf["qualified"] = tdf["table_schema"] + "." + tdf["table_name"]
                tables = tdf["qualified"].tolist()
            except Exception as e:
                st.error(f"Could not fetch tables: {e}")
                return

            if not tables:
                st.info("No user tables found.")
                return

            q_table = st.selectbox("Table (schema.table)", tables, key="quick_table_pg")

            # peek to infer schema
            try:
                peek = _pg_run_sql(st.session_state.pg_conn, f'SELECT * FROM {q_table} LIMIT 200;')
            except Exception as e:
                st.error(f"Peek failed: {e}")
                return

            num_cols = peek.select_dtypes(include=["number"]).columns.tolist()
            cat_cols = [c for c in peek.columns if c not in num_cols]

            if st.button("ğŸ“‹ Table Summary (PostgreSQL)"):
                res = _pg_run_sql(st.session_state.pg_conn, f"SELECT COUNT(*) AS cnt FROM {q_table};")
                st.metric("Total Records", f"{int(res.iloc[0,0]):,}")
                head = _pg_run_sql(st.session_state.pg_conn, f"SELECT * FROM {q_table} LIMIT 5;")
                st.dataframe(head, use_container_width=True)
                st.caption("**Explanation (EN):** Total row count and a 5-row preview.")

            if num_cols and st.button("ğŸ’° Numeric Summary (PostgreSQL)"):
                parts = []
                for c in num_cols[:5]:
                    parts += [f'AVG("{c}") AS avg_{c}', f'SUM("{c}") AS sum_{c}',
                              f'MAX("{c}") AS max_{c}', f'MIN("{c}") AS min_{c}']
                res = _pg_run_sql(st.session_state.pg_conn, f"SELECT {', '.join(parts)} FROM {q_table};")
                for c in num_cols[:3]:
                    a1, a2, a3, a4 = st.columns(4)
                    with a1: st.metric(f"{c} Â· Avg", f"{res[f'avg_{c}'].iloc[0]:.2f}")
                    with a2: st.metric(f"{c} Â· Sum", f"{res[f'sum_{c}'].iloc[0]:,.2f}")
                    with a3: st.metric(f"{c} Â· Max", f"{res[f'max_{c}'].iloc[0]:.2f}")
                    with a4: st.metric(f"{c} Â· Min", f"{res[f'min_{c}'].iloc[0]:.2f}")
                st.caption("**Explanation (EN):** Aggregations are computed directly in PostgreSQL.")

            if cat_cols and st.button("ğŸ‘¥ Category Analysis (PostgreSQL)"):
                cat = cat_cols[0]
                res = _pg_run_sql(
                    st.session_state.pg_conn,
                    f'SELECT "{cat}" AS category, COUNT(*) AS cnt '
                    f'FROM {q_table} GROUP BY "{cat}" ORDER BY cnt DESC LIMIT 15;'
                )
                if not res.empty:
                    fig = px.bar(res, x="category", y="cnt", title=f"Distribution of {cat}")
                    st.plotly_chart(fig, use_container_width=True)
                    st.dataframe(res)
                    st.caption("**Explanation (EN):** Counts the most frequent categories.")

            if num_cols and st.button("ğŸ“ˆ Recent Trend (last 100 rows, PostgreSQL)"):
                target = num_cols[0]
                res = _pg_run_sql(
                    st.session_state.pg_conn,
                    f"SELECT {target} FROM {q_table} LIMIT 100;"
                )
                if not res.empty:
                    res = res.reset_index().rename(columns={"index": "idx"})
                    fig = px.line(res, x="idx", y=target, title=f"Trend of {target} (first 100 rows)")
                    st.plotly_chart(fig, use_container_width=True)
                    st.caption("**Explanation (EN):** No guaranteed ordering without an ID/timestamp; "
                               "we show a simple trend over the fetched sample.")

def show_reports():
    """
    ×”×¦×’×ª ×××©×§ ×™×¦×™×¨×ª ×“×•×—×•×ª ××§×™×¤×™× ×•××§×¦×•×¢×™×™×
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - ×™×¦×™×¨×ª ×“×•×—×•×ª × ×™×ª×•×— ××§×™×¤×™× ×‘×¤×•×¨××˜×™× ×©×•× ×™×
    - ×”×¤×§×ª ×ª×•×‘× ×•×ª ×¢×¡×§×™×•×ª ××•×‘× ×•×ª ×•×× ×•×¡×—×•×ª ×”×™×˜×‘
    - ××ª×Ÿ ××¤×©×¨×•×™×•×ª ×”×ª×××” ××™×©×™×ª ×©×œ ×ª×•×›×Ÿ ×”×“×•×—×•×ª
    - ×”×›× ×ª ×“×•×—×•×ª ××•×ª×××™× ×œ×§×”×œ×™ ×™×¢×“ ×©×•× ×™× (×× ×”×œ×™×, ×× ×œ×™×¡×˜×™×, ×œ×§×•×—×•×ª)
    
    ×¡×•×’×™ ×”×“×•×—×•×ª ×”×–××™× ×™×:
    
    1. Business Summary Report:
       - ×¡×™×›×•× ×¢×¡×§×™ ×‘×¨××” ×’×‘×•×”×” ×œ×× ×”×œ×™× ×‘×›×™×¨×™×
       - ××˜×¨×™×§×•×ª ××¤×ª×— ×•××“×“×™ ×‘×™×¦×•×¢ ×¢×™×§×¨×™×™× (KPIs)
       - ×”××œ×¦×•×ª ××¡×˜×¨×˜×’×™×•×ª ××‘×•×¡×¡×•×ª × ×ª×•× ×™×
       - ×¤×•×¨××˜ ×§×¦×¨ ×•×××•×§×“ ×œ×§×‘×œ×ª ×”×—×œ×˜×•×ª ××”×™×¨×”
    
    2. Detailed Analysis Report:
       - ×“×•×— ××¤×•×¨×˜ ×•×˜×›× ×™ ×œ×× ×œ×™×¡×˜×™× ×•××•××—×™ × ×ª×•× ×™×
       - × ×™×ª×•×— ×¡×˜×˜×™×¡×˜×™ ××¢××™×§ ×¢× ×‘×“×™×§×•×ª ×”×©×¢×¨×•×ª
       - ×ª×¨×©×™××™× ×•×™×–×•××œ×™×™× ××ª×§×“××™×
       - ××˜×¨×™×§×•×ª ××¤×•×¨×˜×•×ª ×•××¡×§× ×•×ª ××‘×•×¡×¡×•×ª × ×ª×•× ×™×
    
    3. Executive Summary:
       - ×“×•×— ×§×¦×¨ ×œ×× ×”×œ×™× ×¢× ×“×’×© ×¢×œ ×ª×•×¦××•×ª ×¢×¡×§×™×•×ª
       - ×ª××¦×™×ª ×”×××¦××™× ×”×—×©×•×‘×™× ×‘×™×•×ª×¨
       - ×”××œ×¦×•×ª ×œ×¤×¢×•×œ×” ×¢× ×¡×“×¨ ×¢×“×™×¤×•×™×•×ª
       - ×¢×™×¦×•×‘ ××§×¦×•×¢×™ ×”××ª××™× ×œ×©×™×ª×•×£ ×¢× ×‘×¢×œ×™ ×¢× ×™×™×Ÿ
    
    4. Custom Report Builder:
       - ×™×¦×™×¨×ª ×“×•×— ××•×ª×× ××™×©×™×ª ×¢×œ ×¤×™ ×”×¦×¨×›×™×
       - ×‘×—×™×¨×” ×’××™×©×” ×©×œ ×¨×›×™×‘×™ ×”×“×•×—: ×ª×¨×©×™××™×, ×¡×˜×˜×™×¡×˜×™×§×•×ª, ××ª×××™×
       - ××¤×©×¨×•×ª ×œ×›×œ×•×œ ××• ×œ×”×—×¨×™×’ ×¨×›×™×‘×™× ×¡×¤×¦×™×¤×™×™×
       - ×”×ª×××” ×œ×“×¨×™×©×•×ª ×¡×¤×¦×™×¤×™×•×ª ×©×œ ×”××¨×’×•×Ÿ
    
    ×¤×™×¦'×¨×™× ××ª×§×“××™× ×‘×“×•×—×•×ª:
    - ×•×™×–×•××œ×™×–×¦×™×•×ª ××™× ×˜×¨××§×˜×™×‘×™×•×ª ×•××•×ª×××•×ª
    - × ×™×ª×•×— ×˜×¨× ×“×™× ×•×ª×—×–×™×•×ª ×‘×¡×™×¡×™×•×ª
    - ×–×™×”×•×™ ×¢×¨×›×™× ×—×¨×™×’×™× ×•×× ×•××œ×™×•×ª ××©××¢×•×ª×™×•×ª
    - × ×™×ª×•×— ×§×•×¨×œ×¦×™×•×ª ×•×”××œ×¦×•×ª ×œ×”××©×š ××—×§×¨
    - ×¡×™×›×•× ××™×›×•×ª ×”× ×ª×•× ×™× ×•×××™× ×•×ª ×”×ª×•×¦××•×ª
    - ×”××œ×¦×•×ª ×œ×©×™×¤×•×¨ ××™×¡×•×£ ×•× ×™×”×•×œ ×”× ×ª×•× ×™×
    
    ×¤×•×¨××˜×™ ×”×™×™×¦×•×:
    - ×”×¦×’×” ×™×©×™×¨×” ×‘××¤×œ×™×§×¦×™×”
    - ×™×™×¦×•× ×œ×§×‘×¦×™ ×˜×§×¡×˜ ×•×”×“×¤×¡×”
    - ×©××™×¨×” ×‘××¦×‘ ×”×¡×©×Ÿ ×œ×”××©×š ×¢×‘×•×“×”
    
    ×”×—×–×¨×”: ×œ×œ× - ×”×¤×•× ×§×¦×™×” ××¦×™×’×” ××ª ×”×××©×§ ×™×©×™×¨×•×ª
    """
    st.markdown("## ğŸ“„ Report Generation")
    
    if 'data' not in st.session_state:
        st.warning("ğŸ“‚ Please load data first!")
        return
    
    df = st.session_state.data
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.markdown("### ğŸ“Š Report Configuration")
        
        report_type = st.selectbox(
            "Report Type",
            ["ğŸ“ˆ Brief Overview", "ğŸ“Š Detailed Analysis", "ğŸ¯ Custom Report", "ğŸ“‹ Executive Summary"]
        )
        
        # Report settings
        include_charts = st.checkbox("Include charts", value=True)
        include_stats = st.checkbox("Include statistics", value=True)
        include_correlations = st.checkbox("Include correlations", value=True)
        include_outliers = st.checkbox("Include outlier analysis", value=False)
        
        if st.button("ğŸ“„ Generate Report"):
            with st.spinner("Generating report..."):
                if report_type == "ğŸ“ˆ Brief Overview":
                    report_content = generate_executive_summary(df)
                elif report_type == "ğŸ“Š Detailed Analysis":
                    report_content = generate_detailed_analysis(df, include_charts, include_stats, include_correlations)
                elif report_type == "ğŸ“‹ Executive Summary":
                    report_content = generate_business_summary(df)
                else:
                    report_content = generate_custom_report(df, include_charts, include_stats, include_correlations, include_outliers)
                
                st.markdown("### ğŸ“‹ Report Preview")
                st.markdown(report_content)
    
    with col2:
        st.markdown("### ğŸ’¾ Data Export")
        
        if st.button("ğŸ“¥ Download CSV"):
            csv = df.to_csv(index=False)
            st.download_button(
                label="ğŸ’¾ Download as CSV",
                data=csv,
                file_name=f'data_export_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv',
                mime='text/csv'
            )
        
        if st.button("ğŸ“Š Download Excel"):
            output = io.BytesIO()
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                df.to_excel(writer, sheet_name='Data', index=False)
                
                # Add statistics sheet
                numeric_cols = df.select_dtypes(include=[np.number]).columns
                if len(numeric_cols) > 0:
                    stats_df = df[numeric_cols].describe()
                    stats_df.to_excel(writer, sheet_name='Statistics')
                
            excel_data = output.getvalue()
            
            st.download_button(
                label="ğŸ’¾ Download as Excel",
                data=excel_data,
                file_name=f'data_export_{datetime.now().strftime("%Y%m%d_%H%M%S")}.xlsx',
                mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
            )
        
        # Export report to text file
        if 'report_content' in locals():
            st.download_button(
                label="ğŸ“„ Download Report",
                data=report_content,
                file_name=f'report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.md',
                mime='text/markdown'
            )

def generate_business_summary(df):
    """
    ×™×¦×™×¨×ª ×“×•×— ×¡×™×›×•× ××ª××§×“ ×‘×ª×•×¦××•×ª ×¢×¡×§×™×•×ª
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - ×™×¦×™×¨×ª ×¡×™×›×•× ×¢×¡×§×™ ××§×¦×•×¢×™ ×•××•×‘×Ÿ ×œ×× ×”×œ×™×
    - ×”×“×’×©×ª ××˜×¨×™×§×•×ª ××¤×ª×— ×•××“×“×™ ×‘×™×¦×•×¢ ×¢×™×§×¨×™×™×
    - ××ª×Ÿ ×¤×¨×©× ×•×ª ×¢×¡×§×™×ª ×œ×ª×•×¦××•×ª ×”× ×™×ª×•×—
    - ×”×¦×¢×ª ×”××œ×¦×•×ª ××¡×˜×¨×˜×’×™×•×ª ×œ×§×‘×œ×ª ×”×—×œ×˜×•×ª
    
    ×¤×¨××˜×¨×™×:
        df (DataFrame): ××¡×’×¨×ª ×”× ×ª×•× ×™× ×œ× ×™×ª×•×—
    
    ×ª×•×›×Ÿ ×”×“×•×—:
    - ×¡×§×™×¨×” ×›×œ×œ×™×ª ×©×œ ×”× ×ª×•× ×™×
    - ××˜×¨×™×§×•×ª ××¤×ª×— ×•××“×“×™ ×‘×™×¦×•×¢
    - ×ª×•×‘× ×•×ª ×¢×¡×§×™×•×ª ××¨×›×–×™×•×ª
    - ×”××œ×¦×•×ª ×œ×¤×¢×•×œ×”
    
    ×”×—×–×¨×”:
        str: ×“×•×— ×¢×¡×§×™ ××¢×•×¦×‘ ×›××—×¨×•×–×ª ×˜×§×¡×˜
    """
    israel_tz = pytz.timezone('Asia/Jerusalem')
    now_israel = datetime.now(israel_tz)
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    text_cols = df.select_dtypes(include=['object']).columns

    summary = f"""
# ğŸ“Š Executive Summary
*Created: {now_israel.strftime('%Y-%m-%d %H:%M')} (Israel time)*

## ğŸ¯ Key Metrics

**Data Overview:**
- ğŸ“ Total records: **{len(df):,}**
- ğŸ“Š Number of columns: **{len(df.columns)}**
- ğŸ”¢ Numeric columns: **{len(numeric_cols)}**
- ğŸ“‹ Categorical columns: **{len(text_cols)}**

## ğŸ’¡ Main Findings

"""
    missing_pct = (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100
    if missing_pct < 5:
        summary += "âœ… High data quality - less than 5% missing values\n\n"
    elif missing_pct < 15:
        summary += "âš ï¸ Satisfactory data quality - requires attention to missing values\n\n"
    else:
        summary += "âŒ Data cleaning required - high percentage of missing values\n\n"

    if len(numeric_cols) > 0:
        summary += "### ğŸ“ˆ Numeric Metrics\n\n"
        for col in numeric_cols[:5]:
            col_stats = df[col].describe()
            summary += f"**{col}:**\n"
            summary += f"- Average value: {col_stats['mean']:.2f}\n"
            summary += f"- Median: {col_stats['50%']:.2f}\n"
            summary += f"- Range: {col_stats['min']:.2f} - {col_stats['max']:.2f}\n\n"

    if len(numeric_cols) >= 2:
        corr_matrix = df[numeric_cols].corr()
        high_corr_pairs = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_val = corr_matrix.iloc[i, j]
                if abs(corr_val) > 0.7:
                    high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))
        if high_corr_pairs:
            summary += "### ğŸ”— Strong Relationships\n\n"
            for var1, var2, corr in high_corr_pairs[:3]:
                summary += f"- **{var1}** â†” **{var2}**: {corr:.3f}\n"
            summary += "\n"

    summary += "## ğŸ¯ Recommendations\n\n"
    recommendations = []
    if missing_pct > 10:
        recommendations.append("ğŸ”§ Perform data cleaning and handle missing values")
    if len(numeric_cols) >= 3:
        recommendations.append("ğŸ“Š Consider applying machine learning methods")
    if len(text_cols) > 0:
        recommendations.append("ğŸ“ Perform categorical variable analysis")
    if len(df) > 10000:
        recommendations.append("âš¡ Use optimization methods for big data")
    for i, rec in enumerate(recommendations, 1):
        summary += f"{i}. {rec}\n"

    summary += f"\n---\nReport generated by DataBot Analytics Pro"
    return summary

def generate_detailed_analysis(df, include_charts=True, include_stats=True, include_correlations=True):
    israel_tz = pytz.timezone('Asia/Jerusalem')
    now_israel = datetime.now(israel_tz)
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    text_cols = df.select_dtypes(include=['object']).columns
    datetime_cols = df.select_dtypes(include=['datetime64']).columns

    analysis = f"""
# ğŸ“ˆ Detailed Data Analysis
*Created: {now_israel.strftime('%Y-%m-%d %H:%M')} (Israel time)*

## ğŸ“Š Dataset Overview

### Basic Characteristics
- **Total records**: {len(df):,}
- **Number of columns**: {len(df.columns)}
- **Memory size**: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB

### Data Types
- **Numeric columns**: {len(numeric_cols)} ({', '.join(numeric_cols[:5])}{'...' if len(numeric_cols) > 5 else ''})
- **Text columns**: {len(text_cols)} ({', '.join(text_cols[:5])}{'...' if len(text_cols) > 5 else ''})
- **Datetime columns**: {len(datetime_cols)}

## ğŸ” Data Quality

### Missing Value Analysis
"""
    missing_data = df.isnull().sum()
    missing_pct = (missing_data / len(df)) * 100
    if missing_data.sum() > 0:
        analysis += f"**Total missing values**: {missing_data.sum():,}\n\n"
        analysis += "**Columns with missing values**:\n"
        for col in missing_data[missing_data > 0].index:
            analysis += f"- {col}: {missing_data[col]} ({missing_pct[col]:.1f}%)\n"
    else:
        analysis += "âœ… No missing values detected\n"
    analysis += "\n"

    duplicates = df.duplicated().sum()
    if duplicates > 0:
        analysis += f"**Duplicates**: {duplicates} rows ({duplicates/len(df)*100:.1f}%)\n\n"
    else:
        analysis += "âœ… No duplicates detected\n\n"

    if include_stats and len(numeric_cols) > 0:
        analysis += "## ğŸ“Š Statistical Analysis\n\n"
        for col in numeric_cols:
            stats = df[col].describe()
            analysis += f"### {col}\n"
            analysis += f"- **Mean**: {stats['mean']:.3f}\n"
            analysis += f"- **Median**: {stats['50%']:.3f}\n"
            analysis += f"- **Standard deviation**: {stats['std']:.3f}\n"
            analysis += f"- **Minimum**: {stats['min']:.3f}\n"
            analysis += f"- **Maximum**: {stats['max']:.3f}\n"
            analysis += f"- **Coefficient of variation**: {(stats['std']/stats['mean']*100):.1f}%\n\n"

    if include_correlations and len(numeric_cols) >= 2:
        analysis += "## ğŸ”— Correlation Analysis\n\n"
        corr_matrix = df[numeric_cols].corr()
        strong_correlations = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_val = corr_matrix.iloc[i, j]
                if abs(corr_val) > 0.5:
                    strong_correlations.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))
        if strong_correlations:
            analysis += "### Significant correlations (|r| > 0.5):\n"
            strong_correlations.sort(key=lambda x: abs(x[2]), reverse=True)
            for var1, var2, corr in strong_correlations:
                strength = "very strong" if abs(corr) > 0.8 else "strong" if abs(corr) > 0.6 else "moderate"
                direction = "positive" if corr > 0 else "negative"
                analysis += f"- **{var1}** â†” **{var2}**: {corr:.3f} ({strength} {direction})\n"
        else:
            analysis += "No strong correlations detected.\n"
        analysis += "\n"

    if len(numeric_cols) > 0:
        analysis += "## ğŸ“ˆ Distribution Analysis\n\n"
        for col in numeric_cols[:3]:
            skewness = df[col].skew()
            kurtosis = df[col].kurtosis()
            analysis += f"### {col}\n"
            analysis += f"- **Skewness**: {skewness:.3f} "
            if abs(skewness) < 0.5:
                analysis += "(symmetric distribution)\n"
            elif skewness > 0:
                analysis += "(right-skewed)\n"
            else:
                analysis += "(left-skewed)\n"
            analysis += f"- **Kurtosis**: {kurtosis:.3f} "
            if kurtosis > 3:
                analysis += "(peaked distribution)\n"
            elif kurtosis < -1:
                analysis += "(flat distribution)\n"
            else:
                analysis += "(close to normal)\n"
            analysis += "\n"

    analysis += "## ğŸ’¡ Conclusions and Recommendations\n\n"
    conclusions = []
    missing_pct_total = (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100
    if missing_pct_total < 5:
        conclusions.append("âœ… High data quality - ready for analysis")
    elif missing_pct_total < 15:
        conclusions.append("âš ï¸ Data requires preprocessing")
    else:
        conclusions.append("âŒ Serious data cleaning needed")
    if len(df) > 100000:
        conclusions.append("ğŸ“Š Large dataset - suitable for machine learning")
    elif len(df) > 1000:
        conclusions.append("ğŸ“ˆ Medium dataset - sufficient for statistical analysis")
    else:
        conclusions.append("ğŸ“‰ Small dataset - limited analysis capabilities")
    if len(numeric_cols) >= 2:
        corr_matrix = df[numeric_cols].corr()
        max_corr = corr_matrix.abs().values[np.triu_indices_from(corr_matrix.values, 1)].max()
        if max_corr > 0.8:
            conclusions.append("ğŸ”— Strong correlations detected - possible multicollinearity")
        elif max_corr > 0.5:
            conclusions.append("ğŸ“Š Moderate correlations found between variables")
    outlier_cols = []
    for col in numeric_cols:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        outliers = df[(df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)]
        if len(outliers) > len(df) * 0.05:
            outlier_cols.append(col)
    if outlier_cols:
        conclusions.append(f"âš ï¸ Outliers detected in columns: {', '.join(outlier_cols)}")
    for i, conclusion in enumerate(conclusions, 1):
        analysis += f"{i}. {conclusion}\n"
    analysis += "\n### Next Steps:\n"
    next_steps = []
    if missing_pct_total > 5:
        next_steps.append("ğŸ”§ Handle missing values")
    if len(numeric_cols) >= 3:
        next_steps.append("ğŸ¤– Apply machine learning methods")
    if outlier_cols:
        next_steps.append("ğŸ¯ Analyze and handle outliers")
    if len(text_cols) > 0:
        next_steps.append("ğŸ“ Analyze categorical variables")
    if len(numeric_cols) >= 2:
        next_steps.append("ğŸ“Š Build predictive models")
    for i, step in enumerate(next_steps, 1):
        analysis += f"{i}. {step}\n"
    analysis += f"\n---\nDetailed analysis performed on {now_israel.strftime('%Y-%m-%d at %H:%M')} (Israel time)"
    return analysis

def generate_executive_summary(df):
    israel_tz = pytz.timezone('Asia/Jerusalem')
    now_israel = datetime.now(israel_tz)
    numeric_cols = df.select_dtypes(include=[np.number]).columns

    summary = f"""
# ğŸ“Š Data Brief Overview

## Key Metrics
- **Records**: {len(df):,}
- **Columns**: {len(df.columns)}
- **Numeric columns**: {len(numeric_cols)}
- **Data quality**: {calculate_data_quality(df):.1f}/10

## Key Statistics
"""
    if len(numeric_cols) > 0:
        for col in numeric_cols[:3]:
            mean_val = df[col].mean()
            summary += f"- **{col}**: average {mean_val:.2f}\n"
    summary += f"\n*Overview created: {now_israel.strftime('%Y-%m-%d %H:%M')} (Israel time)*"
    return summary

def generate_custom_report(df, include_charts=True, include_stats=True, include_correlations=True, include_outliers=False):
    israel_tz = pytz.timezone('Asia/Jerusalem')
    now_israel = datetime.now(israel_tz)
    numeric_cols = df.select_dtypes(include=[np.number]).columns

    report = f"""
# ğŸ¯ Custom Report
*Created: {now_israel.strftime('%Y-%m-%d %H:%M')} (Israel time)*

## ğŸ“‹ Report Configuration
- Charts: {'âœ…' if include_charts else 'âŒ'}
- Statistics: {'âœ…' if include_stats else 'âŒ'}
- Correlations: {'âœ…' if include_correlations else 'âŒ'}
- Outlier analysis: {'âœ…' if include_outliers else 'âŒ'}

## ğŸ“Š Data Overview
- Total records: {len(df):,}
- Total columns: {len(df.columns)}
"""
    if include_stats and len(numeric_cols) > 0:
        report += f"\n## ğŸ“ˆ Statistical Overview\n"
        for col in numeric_cols[:5]:
            stats = df[col].describe()
            report += f"**{col}**: min={stats['min']:.2f}, max={stats['max']:.2f}, mean={stats['mean']:.2f}\n"
    if include_correlations and len(numeric_cols) >= 2:
        report += f"\n## ğŸ”— Correlation Analysis\n"
        corr_matrix = df[numeric_cols].corr()
        max_corr = corr_matrix.abs().values[np.triu_indices_from(corr_matrix.values, 1)].max()
        report += f"Maximum correlation: {max_corr:.3f}\n"
    if include_outliers:
        report += f"\n## ğŸ¯ Outlier Analysis\n"
        for col in numeric_cols[:3]:
            outliers_info = detect_outliers_advanced(df[col])
            iqr_outliers = outliers_info['IQR']['count']
            report += f"**{col}**: {iqr_outliers} outliers by IQR method\n"
    return report

# Helper functions for creating demo data
def create_demo_data():
    """
    ×™×¦×™×¨×ª × ×ª×•× ×™ ×”×“×’××” ××§×™×¤×™× ×œ×¦×•×¨×›×™ × ×™×¡×•×™ ×•×œ×™××•×“
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - ×™×¦×™×¨×ª ××¡×“ × ×ª×•× ×™× ×¡×™× ×ª×˜×™ ×©××“××” × ×ª×•× ×™× ×¢×¡×§×™×™× ×××™×ª×™×™×
    - ×××¤×©×¨ ×œ××©×ª××©×™× ×œ× ×¡×•×ª ××ª ×”××¤×œ×™×§×¦×™×” ×œ×œ× ×¦×•×¨×š ×‘× ×ª×•× ×™× ×©×œ×”×
    - ×›×•×œ×œ ××’×•×•×Ÿ ×¨×—×‘ ×©×œ ×¡×•×’×™ × ×ª×•× ×™× ×•××‘× ×™×
    - ××›×™×œ ×§×©×¨×™× ×¨×™××œ×™×¡×˜×™×™× ×‘×™×Ÿ ×”××©×ª× ×™×
    
    ×”× ×ª×•× ×™× ×›×•×œ×œ×™×:
    - Date: ×ª××¨×™×›×™× ×¨×¦×•×¤×™× ×œ××•×¨×š ×©× ×”
    - Sales: × ×ª×•× ×™ ××›×™×¨×•×ª ×‘×”×ª×¤×œ×’×•×ª × ×•×¨××œ×™×ª
    - Customers: ××¡×¤×¨ ×œ×§×•×—×•×ª ×‘×”×ª×¤×œ×’×•×ª ×¤×•××¡×•×Ÿ
    - Revenue: ×”×›× ×¡×•×ª ×¢× ×©×•× ×•×ª ×¨×™××œ×™×¡×˜×™×ª
    - Region: ××–×•×¨×™× ×’×™××•×’×¨×¤×™×™× ×©×•× ×™×
    - Category: ×§×˜×’×•×¨×™×•×ª ××•×¦×¨×™×
    - Rating: ×“×™×¨×•×’×™× ×¢× ×¢×¨×›×™× ×—×¡×¨×™× ×œ×¨×™××œ×™×–×
    
    ××©×ª× ×™× ××—×•×©×‘×™×:
    - Conversion: ××—×•×– ×”××¨×” (×œ×§×•×—×•×ª/××›×™×¨×•×ª)
    - Average_Check: ×¡×›×•× ×××•×¦×¢ ×œ×œ×§×•×—
    
    ×”×—×–×¨×”:
        DataFrame: ××¡×’×¨×ª × ×ª×•× ×™× ×¢× 1000 ×¨×©×•××•×ª ×œ×”×“×’××”
    """
    np.random.seed(42)
    
    n_records = 1000
    
    data = {
        'Date': pd.date_range('2024-01-01', periods=n_records, freq='D'),
        'Sales': np.random.normal(1000, 200, n_records),
        'Customers': np.random.poisson(30, n_records),
        'Revenue': np.random.normal(5000, 1000, n_records),
        'Region': np.random.choice(['North', 'South', 'East', 'West'], n_records),
        'Category': np.random.choice(['A', 'B', 'C'], n_records),
        'Rating': np.random.uniform(1, 5, n_records)
    }
    
    df = pd.DataFrame(data)
    
    # Add some realistic dependencies
    df['Conversion'] = df['Customers'] / df['Sales'] * 100
    df['Average_Check'] = df['Revenue'] / df['Customers']
    
    # Add some missing values for realism
    df.loc[np.random.choice(df.index, 50), 'Rating'] = np.nan
    
    return df

def create_ecommerce_data():
    """
    ×™×¦×™×¨×ª × ×ª×•× ×™ ××¡×—×¨ ××œ×§×˜×¨×•× ×™ ×¨×™××œ×™×¡×˜×™×™×
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - ×™×¦×™×¨×ª ××¡×“ × ×ª×•× ×™× ×¡×™× ×ª×˜×™ ×”××“××” ×—× ×•×ª ××•× ×œ×™×™×Ÿ ×××™×ª×™×ª
    - ××ª××™× ×œ× ×™×ª×•×—×™× ×¢×¡×§×™×™× ×•××—×§×¨×™ ×©×•×§
    - ×›×•×œ×œ ××˜×¨×™×§×•×ª ××¤×ª×— ×‘×ª×—×•× ×”××¡×—×¨ ×”××œ×§×˜×¨×•× ×™
    - ×××¤×©×¨ × ×™×ª×•×— ×”×ª× ×”×’×•×ª ×œ×§×•×—×•×ª ×•×“×¤×•×¡×™ ×¨×›×™×©×”
    
    × ×ª×•× ×™ ×”××©×ª××©×™× ×•×”×¨×›×™×©×•×ª:
    - user_id: ××–×”×” ××©×ª××© ×™×™×—×•×“×™
    - Age: ×’×™×œ ×”××©×ª××© (18-65)
    - Gender: ××™×Ÿ (×–/× )
    - Purchase_Amount: ×¡×›×•× ×”×¨×›×™×©×” (×‘×”×ª×¤×œ×’×•×ª ××§×¡×¤×•× × ×¦×™××œ×™×ª)
    - Category: ×§×˜×’×•×¨×™×•×ª ××•×¦×¨×™× (××œ×§×˜×¨×•× ×™×§×”, ×‘×’×“×™×, ×¡×¤×¨×™×, ×‘×™×ª)
    - Satisfaction: ×“×™×¨×•×’ ×©×‘×™×¢×•×ª ×¨×¦×•×Ÿ (1-5)
    
    × ×ª×•× ×™ ×”×ª× ×”×’×•×ª ×‘××ª×¨:
    - Time_on_Site: ×–××Ÿ ×©×”×™×” ×‘××ª×¨ (×©× ×™×•×ª)
    - Page_Views: ××¡×¤×¨ ×¦×¤×™×•×ª ×‘×¢××•×“×™×
    - Device: ×¡×•×’ ××›×©×™×¨ (××—×©×‘, × ×™×™×“, ×˜××‘×œ×˜)
    - Source: ××§×•×¨ ×”×’×¢×” (×—×™×¤×•×©, ×¨×©×ª×•×ª ×—×‘×¨×ª×™×•×ª, ×“×•××¨, ×™×©×™×¨)
    
    ××œ×’×•×¨×™×ª××™× ××•×˜××¢×™×:
    - ×¦×¢×™×¨×™× × ×•×˜×™× ×™×•×ª×¨ ×œ×”×©×ª××© ×‘××›×©×™×¨×™× × ×™×™×“×™×
    - ×¨×›×™×©×•×ª ×‘×§×˜×’×•×¨×™×•×ª ××¡×•×™××•×ª ××©×¤×™×¢×•×ª ×¢×œ ×”×¡×›×•×
    - ×§×©×¨ ×‘×™×Ÿ ×–××Ÿ ×‘××ª×¨ ×œ×¡×›×•× ×”×¨×›×™×©×”
    
    ×”×—×–×¨×”:
        DataFrame: ××¡×’×¨×ª × ×ª×•× ×™× ×¢× 2000 ×¨×©×•××•×ª ×©×œ × ×ª×•× ×™ ××¡×—×¨ ××œ×§×˜×¨×•× ×™
    """
    np.random.seed(123)
    
    n_records = 2000
    
    data = {
        'user_id': range(1, n_records + 1),
        'Age': np.random.randint(18, 65, n_records),
        'Gender': np.random.choice(['M', 'F'], n_records),
        'Purchase_Amount': np.random.exponential(50, n_records),
        'Category': np.random.choice(['Electronics', 'Clothing', 'Books', 'Home'], n_records),
        'Satisfaction': np.random.randint(1, 6, n_records),
        'Time_on_Site': np.random.normal(300, 100, n_records),  # seconds
        'Page_Views': np.random.poisson(5, n_records),
        'Device': np.random.choice(['Desktop', 'Mobile', 'Tablet'], n_records),
        'Source': np.random.choice(['Search', 'Social Media', 'Email', 'Direct'], n_records)
    }
    
    df = pd.DataFrame(data)
    
    # Add dependencies
    # Young users more likely to use mobile
    mobile_prob = 1 / (1 + np.exp((df['Age'] - 35) / 10))
    df.loc[np.random.random(n_records) < mobile_prob, 'Device'] = 'Mobile'
    
    # More time on site = more purchases
    df['Purchase_Amount'] = df['Purchase_Amount'] * (1 + df['Time_on_Site'] / 1000)
    
    # Add missing values
    df.loc[np.random.choice(df.index, 100), 'Satisfaction'] = np.nan
    
    return df

def auto_analyze_data():
    """
    × ×™×ª×•×— ××•×˜×•××˜×™ ×©×œ ×”× ×ª×•× ×™× ×”×˜×¢×•× ×™×
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - ×‘×™×¦×•×¢ ×¡×“×¨×ª × ×™×ª×•×—×™× ××•×˜×•××˜×™×™× ×¢×œ ×”× ×ª×•× ×™× ×”×˜×¢×•× ×™×
    - ×™×¦×™×¨×ª ×ª×•×‘× ×•×ª ××™×™×“×™×•×ª ×•××¢×©×™×•×ª ×œ×œ× ×¦×•×¨×š ×‘×”×ª×“×—×œ×•×ª ×™×“× ×™×ª
    - ×”×¦×’×ª ×¡×™×›×•× ××§×™×£ ×©×œ ×××¤×™×™× ×™ ×•××™×›×•×ª ×”× ×ª×•× ×™×
    - ×—×™×¡×›×•×Ÿ ×–××Ÿ ×œ××©×ª××© ×‘×‘×™×¦×•×¢ × ×™×ª×•×—×™× ×‘×¡×™×¡×™×™×
    
    ×¨×›×™×‘×™ ×”× ×™×ª×•×— ×”××•×˜×•××˜×™:
    - ××“×“×™× ×‘×¡×™×¡×™×™×: ×’×•×“×œ ××¡×“, ××—×•×– ×¢×¨×›×™× ×—×¡×¨×™×, ×¦×™×•×Ÿ ××™×›×•×ª
    - ×ª×•×‘× ×•×ª ×•×¢×¦×•×ª ××§×¦×•×¢×™×•×ª ××‘×•×¡×¡×•×ª ×¢×œ ××‘× ×” ×”× ×ª×•× ×™×
    - ×¡×˜×˜×™×¡×˜×™×§×•×ª ××”×™×¨×•×ª: ×××•×¦×¢, ×©×•× ×•×ª, ××§×“× ×©×•× ×•×ª
    - ×”×“×’×©×ª ××©×ª× ×™× ×¨×œ×•×•× ×˜×™×™× ×‘×™×•×ª×¨ (×¢×“ 3 ×¨××©×•× ×™×)
    
    ××•×¤×Ÿ ×”×¤×¢×œ×”:
    ×”×¤×•× ×§×¦×™×” ×¢×•×‘×“×ª ×¨×§ ×× ×™×© × ×ª×•× ×™× ×˜×¢×•× ×™× ×‘××¦×‘ ×”×¡×©×Ÿ
    
    ×”×—×–×¨×”: ×œ×œ× - ×”×¤×•× ×§×¦×™×” ××¦×™×’×” ××ª ×ª×•×¦××•×ª ×”× ×™×ª×•×— ×™×©×™×¨×•×ª
    """
    if 'data' not in st.session_state:
        return
    
    df = st.session_state.data
    
    st.markdown("### ğŸ” Auto-Analysis Results")
    
    # Basic information
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("ğŸ“Š Data Size", f"{len(df)} Ã— {len(df.columns)}")
    
    with col2:
        missing_pct = (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100
        st.metric("âŒ Missing", f"{missing_pct:.1f}%")
    
    with col3:
        quality_score = calculate_data_quality(df)
        st.metric("â­ Quality", f"{quality_score:.1f}/10")
    
    # Show insights
    show_insights_and_advice(df)
    
    # Brief statistical overview
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        st.markdown("#### ğŸ“ˆ Quick Statistics")
        
        for col in numeric_cols[:3]:  # Show first 3 columns
            mean_val = df[col].mean()
            std_val = df[col].std()
            cv = (std_val / mean_val) * 100 if mean_val != 0 else 0
            
            st.write(f"**{col}**: mean = {mean_val:.2f}, variation = {cv:.1f}%")

def calculate_data_quality(df):
    """
    ×—×™×©×•×‘ ×¦×™×•×Ÿ ××™×›×•×ª × ×ª×•× ×™× ×›×•×œ×œ
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - ×”×¢×¨×›×ª ××§×™×¤×” ×•×›××•×ª×™×ª ×©×œ ××™×›×•×ª ××¡×“ ×”× ×ª×•× ×™×
    - ××ª×Ÿ ×¦×™×•×Ÿ ××™×›×•×ª ×§×œ ×œ×”×‘× ×” (0-10)
    - ×–×™×”×•×™ ×‘×¢×™×•×ª ××™×›×•×ª ×¢×™×§×¨×™×•×ª ×”××©×¤×™×¢×•×ª ×¢×œ ×××™× ×•×ª ×”× ×™×ª×•×—
    - ××ª×Ÿ ×‘×¡×™×¡ ×œ×”×—×œ×˜×•×ª ×¢×œ ×¦×¢×“×™ × ×™×§×•×™ ×•×¢×™×‘×•×“
    
    ×¤×¨××˜×¨×™×:
        df (DataFrame): ××¡×’×¨×ª ×”× ×ª×•× ×™× ×œ×”×¢×¨×›×”
    
    ××¨×›×™×‘×™ ×”×¦×™×•×Ÿ (×¤× ×œ×™×–×¦×™×•×ª):
    - ×¢×¨×›×™× ×—×¡×¨×™×: ×¢×“ -5 × ×§×•×“×•×ª (××—×•×– ×—×¡×¨×™× * 5)
    - ×¨×©×•××•×ª ×›×¤×•×œ×•×ª: ×¢×“ -3 × ×§×•×“×•×ª (××—×•×– ×›×¤×•×œ×™× * 3)
    - ××¢×˜ ××“×™ ×¢××•×“×•×ª: -1 × ×§×•×“×” (×× ×¤×—×•×ª ×-3 ×¢××•×“×•×ª)
    
    ×¡×•×œ× ×”×¦×™×•× ×™×:
    - 9-10: ××™×›×•×ª ××¦×•×™× ×ª
    - 7-8: ××™×›×•×ª ×˜×•×‘×”
    - 5-6: ××™×›×•×ª ×¡×‘×™×¨×”
    - 0-4: ××™×›×•×ª ×¨×¢×” - ×“×¨×•×© × ×™×§×•×™
    
    ×”×—×–×¨×”:
        float: ×¦×™×•×Ÿ ××™×›×•×ª ×‘×™×Ÿ 0 ×œ-10
    """
    
    score = 10.0
    
    # Penalty for missing values
    missing_pct = df.isnull().sum().sum() / (len(df) * len(df.columns))
    score -= missing_pct * 5
    
    # Penalty for duplicates
    duplicate_pct = df.duplicated().sum() / len(df)
    score -= duplicate_pct * 3
    
    # Penalty for low variation (constant columns)
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if df[col].std() == 0:  # Constant column
            score -= 1
    
    return max(0, min(10, score))

def fill_missing_values(df):
    """
    ××™×œ×•×™ ×¢×¨×›×™× ×—×¡×¨×™× ×‘×©×™×˜×•×ª ××ª××™××•×ª ×œ×¤×™ ×¡×•×’ ×”× ×ª×•× ×™×
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - ×˜×™×¤×•×œ ××•×˜×•××˜×™ ×‘×¢×¨×›×™× ×—×¡×¨×™× ×œ×¤×™ ×¡×•×’ ×”× ×ª×•× ×™× ×‘×›×œ ×¢××•×“×”
    - ×©×™×¤×•×¨ ××™×›×•×ª ×”× ×ª×•× ×™× ×œ×¦×•×¨×š × ×™×ª×•×—×™× × ×•×¡×¤×™×
    - ×”×›× ×ª ×”× ×ª×•× ×™× ×œ××œ×’×•×¨×™×ª××™ ×œ××™×“×ª ××›×•× ×”
    - ×©××™×¨×” ×¢×œ ×”××‘× ×” ×•×”×ª×•×›×Ÿ ×”××§×•×¨×™ ×©×œ ×”× ×ª×•× ×™× ×›×›×œ ×”× ×™×ª×Ÿ
    
    ×¤×¨××˜×¨×™×:
        df (DataFrame): ××¡×’×¨×ª ×”× ×ª×•× ×™× ×¢× ×¢×¨×›×™× ×—×¡×¨×™×
    
    ×©×™×˜×•×ª ×”××™×œ×•×™ ×œ×¤×™ ×¡×•×’ × ×ª×•× ×™×:
    - ×¢××•×“×•×ª × ×•××¨×™×•×ª (float64, int64): ××™×œ×•×™ ×‘×—×¦×™×•×Ÿ
      - ×‘×—×™×¨×” ×‘×—×¦×™×•×Ÿ ×¢×œ ×¤× ×™ ×××•×¦×¢ ×œ×”×¤×—×ª×ª ×”×©×¤×¢×ª ×¢×¨×›×™× ×—×¨×™×’×™×
      - ×©××™×¨×” ×¢×œ ×”×”×ª×¤×œ×’×•×ª ×”××§×•×¨×™×ª ×©×œ ×”× ×ª×•× ×™×
    - ×¢××•×“×•×ª ×˜×§×¡×˜/×§×˜×’×•×¨×™××œ×™×•×ª (object): ××™×œ×•×™ ×‘××¦×‘ (Mode)
      - ×‘×—×™×¨×” ×‘×¢×¨×š ×”×©×›×™×— ×‘×™×•×ª×¨ ×œ×›×œ ×¢××•×“×”
      - ×”×¤×—×ª×ª ×”×©×¤×¢×” ×¢×œ ×”×”×ª×¤×œ×’×•×ª ×”×§×˜×’×•×¨×™××œ×™×ª
    - ×¢××•×“×•×ª ××—×¨×•×ª: ××™×œ×•×™ ×§×“××™ (Forward Fill)
      - ×©×™××•×© ×‘×¢×¨×š ×”×§×•×“× ×”×–××™×Ÿ
      - ××ª××™× ×œ×¡×“×¨×•×ª ×–××Ÿ ×•×œ× ×ª×•× ×™× ×¡×“×¨×ª×™×™×
    
    ×™×ª×¨×•× ×•×ª ×”×©×™×˜×”:
    - ×’×™×©×” ××ª×•×—×›××ª ×•××•×ª×××ª ×œ×¡×•×’ ×”× ×ª×•× ×™×
    - ×©××™×¨×” ×¢×œ ×”×ª×¤×œ×’×•×™×•×ª ××§×•×¨×™×•×ª
    - ×¢××™×“×•×ª ×‘×¤× ×™ ×¢×¨×›×™× ×—×¨×™×’×™×
    - ×ª×•×¦××•×ª ××”×™×× ×•×ª ×œ× ×™×ª×•×—×™× ×¡×˜×˜×™×¡×˜×™×™×
    
    ×”×—×–×¨×”:
        DataFrame: ××¡×’×¨×ª × ×ª×•× ×™× ××¢×•×“×›× ×ª ×œ×œ× ×¢×¨×›×™× ×—×¡×¨×™×
    """
    
    df_filled = df.copy()
    
    for col in df_filled.columns:
        if df_filled[col].dtype in ['float64', 'int64']:
            # Numeric - with median
            df_filled[col] = df_filled[col].fillna(df_filled[col].median())
        elif df_filled[col].dtype == 'object':
            # Categorical - with mode or 'Unknown'
            mode_val = df_filled[col].mode()
            if len(mode_val) > 0:
                df_filled[col] = df_filled[col].fillna(mode_val[0])
            else:
                df_filled[col] = df_filled[col].fillna('Unknown')
        elif df_filled[col].dtype == 'datetime64[ns]':
            # Dates - with median
            df_filled[col] = df_filled[col].fillna(df_filled[col].median())
    
    return df_filled

def show_quick_summary():
    """
    ×”×¦×’×ª ×¡×™×›×•× ××”×™×¨ ×©×œ ×”× ×ª×•× ×™× ×‘×¤×¢×•×œ×” ××”×™×¨×”
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - ××ª×Ÿ ×¡×§×™×¨×” ××”×™×¨×” ×•××•×§×“×ª ×©×œ ×××¤×™×™× ×™ ×”× ×ª×•× ×™× ×”×‘×¡×™×¡×™×™×
    - ×—×™×¡×›×•×Ÿ ×‘×–××Ÿ ×œ××©×ª××© ×œ×œ× ×¦×•×¨×š ×‘× ×™×•×•×˜ ××•×¨×›×‘
    - ×¢×–×¨×” ×‘×”×‘× ×ª ×”××‘× ×” ×•×”××™×›×•×ª ×©×œ ×”× ×ª×•× ×™×
    - ××ª×Ÿ ×‘×¡×™×¡ ×œ×”×—×œ×˜×” ×¢×œ × ×™×ª×•×—×™× × ×•×¡×¤×™×
    
    ×ª×•×›×Ÿ ×”×¡×™×›×•×:
    - ××˜×¨×™×§×•×ª ×‘×¡×™×¡×™×•×ª: ××¡×¤×¨ ×¨×©×•××•×ª, ×¢××•×“×•×ª, ×¢××•×“×•×ª × ×•××¨×™×•×ª
    - ××—×•×– ×¢×¨×›×™× ×—×¡×¨×™× ×›×•×œ×œ
    - ×¡×˜×˜×™×¡×˜×™×§×•×ª ××”×™×¨×•×ª ×œ-3 ×”×¢××•×“×•×ª ×”× ×•××¨×™×•×ª ×”×¨××©×•× ×•×ª
    - ×××•×¦×¢ (Î¼) ×•×¡×˜×™×™×ª ×ª×§×Ÿ (Ïƒ) ×œ×›×œ ×¢××•×“×”
    
    ××•×¤×Ÿ ×”×¤×¢×œ×”:
    ×”×¤×•× ×§×¦×™×” ×¢×•×‘×“×ª ×¨×§ ×× ×™×© × ×ª×•× ×™× ×˜×¢×•× ×™× ×‘××¦×‘ ×”×¡×©×Ÿ
    
    ×”×—×–×¨×”: ×œ×œ× - ×”×¤×•× ×§×¦×™×” ××¦×™×’×” ××ª ×”×¡×™×›×•× ×™×©×™×¨×•×ª
    """
    if 'data' not in st.session_state:
        return
    
    df = st.session_state.data
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    st.markdown("### ğŸ“Š Quick Data Summary")
    
    # Basic stats
    col1, col2 = st.columns(2)
    with col1:
        st.metric("Total Records", f"{len(df):,}")
        st.metric("Columns", len(df.columns))
    with col2:
        st.metric("Numeric Cols", len(numeric_cols))
        missing_pct = (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100
        st.metric("Missing %", f"{missing_pct:.1f}%")
    
    # Quick insights
    if len(numeric_cols) > 0:
        st.write("**Top Numeric Columns:**")
        for col in numeric_cols[:3]:
            mean_val = df[col].mean()
            std_val = df[col].std()
            st.write(f"â€¢ {col}: Î¼={mean_val:.2f}, Ïƒ={std_val:.2f}")

def show_quick_correlation():
    """
    ×”×¦×’×ª × ×™×ª×•×— ××ª×××™× ××”×™×¨
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - ×–×™×”×•×™ ××”×™×¨ ×©×œ ×”×§×©×¨×™× ×”×—×–×§×™× ×‘×™×Ÿ ×”××©×ª× ×™× ×”× ×•××¨×™×™×
    - ××ª×Ÿ ×¦×•×¨×ª ×‘×™×Ÿ ×œ×§×©×¨×™× ×”××¢× ×™×™× ×™× ×‘×™×•×ª×¨ ×‘× ×ª×•× ×™×
    - ×–×™×”×•×™ ×“×¤×•×¡×™× ×•×ª×œ×•×™×•×ª ×¤×•×˜× ×¦×™××œ×™×•×ª
    - ×¢×–×¨×” ×‘×‘×—×™×¨×ª ××©×ª× ×™× ×œ× ×™×ª×•×—×™× × ×•×¡×¤×™×
    
    ×“×¨×™×©×•×ª ×”×¤×¢×œ×”:
    - ×œ×¤×—×•×ª 2 ×¢××•×“×•×ª × ×•××¨×™×•×ª ×‘××¡×“ ×”× ×ª×•× ×™×
    - × ×ª×•× ×™× ×˜×¢×•× ×™× ×‘××¦×‘ ×”×¡×©×Ÿ
    
    ×ª×•×›×Ÿ ×”× ×™×ª×•×—:
    - ×—×™×©×•×‘ ××˜×¨×™×¦×ª ××ª×××™ ×¤×™×¨×¡×•×Ÿ ×œ×›×œ ×”××©×ª× ×™× ×”× ×•××¨×™×™×
    - ×–×™×”×•×™ ×•×”×¦×’×ª ×”××ª×××™× ×”×—×–×§×™× ×‘×™×•×ª×¨ (|×¨| > 0.5)
    - ××™×™×Ÿ ×”×§×©×¨: ×—×™×•×‘×™ (×™×©×™×¨) ××• ×©×œ×™×œ×™ (×”×¤×•×š)
    - ×¢×•×¦××ª ×”×§×©×¨ ×‘×¢×¨×›×™× × ×•××¨×™×™× (-1 ×¢×“ +1)
    
    ×¤×¨×©× ×•×ª ×”×ª×•×¦××•×ª:
    - ××ª×× > 0.7: ×§×©×¨ ×—×–×§ ×××•×“
    - ××ª×× 0.5-0.7: ×§×©×¨ ×—×–×§
    - ××ª×× 0.3-0.5: ×§×©×¨ ×‘×™× ×•× ×™
    - ××ª×× < 0.3: ×§×©×¨ ×—×œ×©
    
    ×”×—×–×¨×”: ×œ×œ× - ×”×¤×•× ×§×¦×™×” ××¦×™×’×” ××ª ×”× ×™×ª×•×— ×™×©×™×¨×•×ª
    """
    if 'data' not in st.session_state:
        return
    
    df = st.session_state.data
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    if len(numeric_cols) >= 2:
        st.markdown("### ğŸ”— Quick Correlation Analysis")
        
        corr_matrix = df[numeric_cols].corr()
        
        # Find highest correlations
        correlations = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_val = corr_matrix.iloc[i, j]
                correlations.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))
        
        # Sort by absolute correlation
        correlations.sort(key=lambda x: abs(x[2]), reverse=True)
        
        st.write("**Strongest Correlations:**")
        for i, (var1, var2, corr) in enumerate(correlations[:5]):
            strength = "Strong" if abs(corr) > 0.7 else "Moderate" if abs(corr) > 0.5 else "Weak"
            st.write(f"{i+1}. {var1} â†” {var2}: {corr:.3f} ({strength})")

def show_quick_3d():
    """
    ×”×¦×’×ª ×•×™×–×•××œ×™×–×¦×™×” ×ª×œ×ª-××™××“×™×ª ××”×™×¨×”
    
    ××˜×¨×ª ×”×¤×•× ×§×¦×™×”:
    - ×™×¦×™×¨×ª ×ª×¨×©×™× ×¤×™×–×•×¨ ×ª×œ×ª-××™××“×™ ××™× ×˜×¨××§×˜×™×‘×™ ×©×œ ×”× ×ª×•× ×™×
    - ××ª×Ÿ ××¤×©×¨×•×ª ×œ×—×§×¨ ×§×©×¨×™× ××•×¨×›×‘×™× ×‘×™×Ÿ ×©×œ×•×©×” ××©×ª× ×™× × ×•××¨×™×™×
    - ×”×¦×’×ª ×ª×•×‘× ×•×ª ×•×™×–×•××œ×™×•×ª ×©×§×©×” ×œ×§×‘×œ ×‘×ª×¨×©×™××™× ×“×•-××™××“×™×™×
    - ×¢×–×¨×” ×‘×–×™×”×•×™ ×§×‘×•×¦×•×ª, ×“×¤×•×¡×™× ×•×—×¨×™×’×•×ª ×‘××¨×—×‘ ×”×ª×œ×ª-××™××“×™
    
    ×“×¨×™×©×•×ª ×”×¤×¢×œ×”:
    - ×œ×¤×—×•×ª 3 ×¢××•×“×•×ª × ×•××¨×™×•×ª ×‘××¡×“ ×”× ×ª×•× ×™×
    - × ×ª×•× ×™× ×˜×¢×•× ×™× ×‘××¦×‘ ×”×¡×©×Ÿ
    
    ×××¤×™×™× ×™ ×”×ª×¨×©×™×:
    - ×¦×™×¨×™ X, Y, Z: ×©×œ×•×©×ª ×”××©×ª× ×™× ×”× ×•××¨×™×™× ×”×¨××©×•× ×™×
    - × ×§×•×“×•×ª ××™× ×˜×¨××§×˜×™×‘×™×•×ª ×”× ×™×ª× ×•×ª ×œ×–×•× ×•×¡×™×‘×•×‘
    - ×¦×‘×¢×™× ×•×¨×“×™×¤× ×¦×™××œ×™×™× ×œ×§×‘×•×¦×•×ª ×× ×§×™×™××™× ××©×ª× ×™× ×§×˜×’×•×¨×™××œ×™×™×
    - ×›×œ×™× ×œ× ×™×•×•×˜ ×ª×œ×ª-××™××“×™: ×–×•×, ×¡×™×‘×•×‘, ×”×–×–×”
    
    ×™×ª×¨×•× ×•×ª ×”×•×™×–×•××œ×™×–×¦×™×” ×”×ª×œ×ª-××™××“×™×ª:
    - ×—×©×™×¤×ª ×§×©×¨×™× × ×¡×ª×¨×™× ×©×œ× × ×¨××™× ×‘×ª×¨×©×™××™× ×“×•-××™××“×™×™×
    - ×–×™×”×•×™ ×§×‘×•×¦×•×ª (clusters) ×‘××¨×—×‘ ×ª×œ×ª-××™××“×™
    - ×”×‘× ×ª ×”×ª×¤×œ×’×•×ª ×”× ×ª×•× ×™× ×‘××¨×—×‘ ×¨×‘-××™××“×™
    - ×–×™×”×•×™ ×¢×¨×›×™× ×—×¨×™×’×™× ×‘××¨×—×‘ ×”×ª×œ×ª-××™××“×™
    
    ×¤×œ×˜×¤×•×¨××ª ×”×ª×¨×©×™×:
    - ×©×™××•×© ×‘×¡×¤×¨×™×™×ª Plotly ×œ××™× ×˜×¨××§×˜×™×‘×™×•×ª ××œ××”
    - ×ª××™×›×” ×‘×›×œ×™ × ×™×•×•×˜ ××ª×§×“××™×
    - ××¤×©×¨×•×™×•×ª ×™×™×¦×•× ×•×”×“×¤×¡×” ×©×œ ×”×ª×¨×©×™×
    
    ×”×—×–×¨×”: ×œ×œ× - ×”×¤×•× ×§×¦×™×” ××¦×™×’×” ××ª ×”×ª×¨×©×™× ×™×©×™×¨×•×ª
    """
    if 'data' not in st.session_state:
        return
    
    df = st.session_state.data
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    if len(numeric_cols) >= 3:
        st.markdown("### ğŸŒ Quick 3D Visualization")
        
        # Auto-select first 3 numeric columns
        x_col, y_col, z_col = numeric_cols[:3]
        
        fig = go.Figure(data=[go.Scatter3d(
            x=df[x_col],
            y=df[y_col],
            z=df[z_col],
            mode='markers',
            marker=dict(
                size=5,
                color=df[z_col],
                colorscale='Viridis',
                opacity=0.8
            )
        )])
        
        fig.update_layout(
            title=f"3D Plot: {x_col} vs {y_col} vs {z_col}",
            scene=dict(
                xaxis_title=x_col,
                yaxis_title=y_col,
                zaxis_title=z_col
            ),
            height=400
        )
        
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.warning("Need at least 3 numeric columns for 3D plot")

# Run application
if __name__ == "__main__":
    main()
    # ××™×Ÿ ×œ×”×—×–×™×¨ ×“×‘×¨ ×‘×¡×•×£ ×”×¡×§×¨×™×¤×˜
